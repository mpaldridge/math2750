% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={MATH2750 Introduction to Markov Processes},
  pdfauthor={Matthew Aldridge},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}

\usepackage{titlesec, environ}
\newif\ifcomm\commtrue
\NewEnviron{myanswers}{\ifcomm\BODY\fi}

\newcommand{\sectionbreak}{\clearpage}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{MATH2750 Introduction to Markov Processes}
\author{\href{mailto:m.aldridge@leeds.ac.uk}{Matthew Aldridge}}
\date{University of Leeds, 2020--2021}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{home}{%
\section*{Schedule}\label{home}}
\addcontentsline{toc}{section}{Schedule}

\textbf{Week 7} (8--12 March):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S13-poisson-poisson}{\textbf{Section 13}: Poisson process with Poisson increments}
\item
  \protect\hyperlink{S14-poisson-exponential}{\textbf{Section 14}: Poisson process with exponential holding times}
\item
  \protect\hyperlink{P07}{\textbf{Problem Sheet 7}}
\item
  \protect\hyperlink{computing}{\textbf{Computational Worksheet 2 / Assessment 2}}: computational drop-in sessions this week, due Thursday 18 March (next week)
\item
  \protect\hyperlink{A3}{\textbf{Assessment 3}}: due Thursday 25 March
\item
  \textbf{Lecture}: Tuesday at 1400 (Zoom)
\item
  \textbf{Workshops} on Problem Sheet 6: Monday or Tuesday (Zoom)
\item
  \textbf{Computational drop-in sessions}: Monday, Tuesday or Wedenesday (Teams)
\item
  \textbf{Drop-in sessions}: Tuesday or Wednesday (Teams)
\end{itemize}

\textbf{Week 6} (1--5 March):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S11-long-term-chains}{\textbf{Section 11}: Long-term behaviour of Markov chians}
\item
  \protect\hyperlink{S12-revision-i}{\textbf{Section 12}: End of of Part I: Discrete time Markov chains}
\item
  \protect\hyperlink{P06}{\textbf{Problem Sheet 6}}
\item
  \protect\hyperlink{computing}{\textbf{Computational Worksheet 2 / Assessment 2}}: computational drop-in sessions next week, due Thursday 18 March
\item
  \protect\hyperlink{A3}{\textbf{Assessment 3}}: due 25 March
\end{itemize}

\textbf{Week 5} (22--26 February):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S09-recurrence-transience}{\textbf{Section 9}: Recurrence and transience}
\item
  \protect\hyperlink{S10-stationary-distributions}{\textbf{Section 10}: Stationary distributions}
\item
  \protect\hyperlink{P05}{\textbf{Problem Sheet 5}}
\end{itemize}

\textbf{Week 4} (15--19 February):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S07-classes}{\textbf{Section 7}: Class Structure}
\item
  \protect\hyperlink{S08-hitting-times}{\textbf{Section 8}: Hitting times}
\item
  \protect\hyperlink{P04}{\textbf{Problem Sheet 4}}
\item
  \protect\hyperlink{computing}{\textbf{Computational Worksheet 1}}, with computational drop-in sessions
\end{itemize}

\textbf{Week 3} (8--12 February):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S05-markov-chains}{\textbf{Section 5}: Discrete time Markov chains}
\item
  \protect\hyperlink{S06-examples}{\textbf{Section 6}: Examples from actuarial science}
\item
  \protect\hyperlink{P03}{\textbf{Problem Sheet 3}}
\item
  \protect\hyperlink{A1}{\textbf{Assessment 1}} due Thursday 11 February (this week)
\item
  \protect\hyperlink{computing}{\textbf{Computational Worksheet 1}}: computational drop-in sessions next week
\end{itemize}

\textbf{Week 2} (1--5 February):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S03-gamblers-ruin}{\textbf{Section 3}: Gambler's ruin}
\item
  \protect\hyperlink{S04-ldes}{\textbf{Section 4}: Linear difference equations}
\item
  \protect\hyperlink{P02}{\textbf{Problem Sheet 2}}
\item
  \protect\hyperlink{A1}{\textbf{Assessment 1}} due Thursday 11 February (next week)
\end{itemize}

\textbf{Week 1} (25--29 January):

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{S00-about}{About the module}
\item
  \protect\hyperlink{S01-stochastic-processes}{\textbf{Section 1}: Stochastic processes and the Markov property}
\item
  \protect\hyperlink{S02-random-walk}{\textbf{Section 2}: Random walks}
\item
  \protect\hyperlink{P01}{\textbf{Problem Sheet 1}}
\end{itemize}

\hypertarget{S00-about}{%
\section*{About MATH2750}\label{S00-about}}
\addcontentsline{toc}{section}{About MATH2750}

This module is \textbf{MATH2750 Introduction to Markov Processes}. The module manager and lecturer is Dr Matthew Aldridge, and my email address is \href{mailto:m.aldridge@leeds.ac.uk}{\nolinkurl{m.aldridge@leeds.ac.uk}}.

\hypertarget{about-module}{%
\subsection*{Organisation of MATH2750}\label{about-module}}
\addcontentsline{toc}{subsection}{Organisation of MATH2750}

This module lasts for 11 weeks. The first nine weeks run from 25 January to 26 March, then we break for Easter, and then the final two weeks run from 26 April to 7 May.

\hypertarget{notes}{%
\subsubsection*{Notes and videos}\label{notes}}
\addcontentsline{toc}{subsubsection}{Notes and videos}

The main way I expect you to learn the material for this course is by reading these notes and by watching the accompanying videos. I will set two sections of notes each week, for a total of 22 sections.

Reading mathematics is a slow process. Each section roughly corresponds to one lecture last year, which would have been 50 minutes. If you find yourself regularly getting through sections in much less than an hour, you're probably not reading carefully enough through each sentence of explanation and each line of mathematics, including understanding the motivation as well as checking the accuracy.

It is possible (but not recommended) to learn the material by only reading the notes and not watching the videos. It is not possible to learn the material by only watching the videos and not reading the notes.

You are probably reading the web version of the notes. If you want a PDF copy (to read offline or to print out), then click the PDF button in the top ribbon of the page. (Warning: I have not made as much effort to make the PDF neat and tidy as I have the web version.)

Since we will all be relying heavily on these notes, I'm even more keen than usual to hear about errors mathematical, typographical or otherwise. Please, please \href{mailto:m.aldridge@leeds.ac.uk}{email me} if think you may have found any.

\hypertarget{problem-sheets}{%
\subsubsection*{Problem sheets}\label{problem-sheets}}
\addcontentsline{toc}{subsubsection}{Problem sheets}

There will be 10 problem sheets; Problem Sheet \(n\) covers the material from the two sections from week \(n\) (Sections \(2n -1\) and \(2n\)), and will be discussed in your workshop in week \(n+1\).

\hypertarget{lectures}{%
\subsubsection*{Lectures}\label{lectures}}
\addcontentsline{toc}{subsubsection}{Lectures}

There will be one online synchronous ``lecture'' session each week, on Tuesdays at 1400, with me, run through Zoom.

This will not be a ``lecture'' in the traditional sense of the term, but will be an opportunity to re-emphasise material you have already learned from notes and videos, to give extra examples, and to answer common student questions, with some degree of interactivity.

I will assume you have completed all the work for the previous week by the time of the lecture, but I will not assume you've started the work for that week itself.

I am very keen to hear about things you'd like to go through in the lectures; please \href{mailto:m.aldridge@leeds.ac.uk}{email me} with your suggestions.

\hypertarget{workshops}{%
\subsubsection*{Workshops}\label{workshops}}
\addcontentsline{toc}{subsubsection}{Workshops}

There will be 10 workshops, starting in the second week. The main goal of the workshops will be to go over your answers to the problems sheets in smaller classes. You will have been assigned to one of three workshop groups, meeting on Mondays or Tuesdays, led by \href{https://eps.leeds.ac.uk/maths/pgr/8790/jason-klebes}{Jason Klebes}, \href{http://www1.maths.leeds.ac.uk/~voss/}{Dr Jochen Voss}, or me. Your workshop will be run through Zoom or Microsoft Teams; your workshop leader will contact you before the end of this week with arrangements.

My recommended approach to problem sheets and workshops is the following:

\begin{itemize}
\tightlist
\item
  Work through the problem sheet before the workshop, spending plenty of time on it, and making multiple efforts at questions you get stuck on. I recommend spending \emph{at least three hours} on each problem sheet, in more than one block. Collaboration is encouraged when working through the problems, but I recommend writing up your work on your own.
\item
  Take advantage of the smaller group setting of the workshop to ask for help or clarification on questions you weren't able to complete.
\item
  After the workshop, attempt again the questions you were previously stuck on.
\item
  If you're still unable to complete a question after this second round of attempts, \emph{then} consult the solutions.
\end{itemize}

\hypertarget{assessments}{%
\subsubsection*{Assessments}\label{assessments}}
\addcontentsline{toc}{subsubsection}{Assessments}

There will be four pieces of assessed coursework, making up a total of 15\% of your mark for the module. Assessments 1, 3 and 4 will involve writing up answers to a few problems, in a similar style to the problem sheets, and are worth 4\% each. (In response to previous student feedback, there are fewer questions per assessment.) Assessment 2 will be a report on some computational work (\protect\hyperlink{about--computing}{see below}) and is worth 3\%.

Copying, plagiarism and other types of cheating are not allowed and will be dealt with in accordance with University procedures.

The assessments deadlines are:

\begin{itemize}
\tightlist
\item
  Assessment 1: Thursday 11 February 1400 (week 3)
\item
  Assessment 2 (Computational Worksheet 2): Thursday 18 March 1400 (week 8)
\item
  Assessment 3: Thursday 25 March 1400 (week 9)
\item
  Assessment 4: Thursday 6 May 1400 (week 11)
\end{itemize}

Work will be submitted via Gradescope.

Your markers are Jason Klebes, Macauley Locke, and Muyang Zhang -- but you should \href{mailto:m.aldridge@leeds.ac.uk}{contact the module leader} if you have marking queries, not the markers directly.

\hypertarget{about-computing}{%
\subsubsection*{Computing worksheets}\label{about-computing}}
\addcontentsline{toc}{subsubsection}{Computing worksheets}

There will be two computing worksheets, which will look at the material in the course through simulations in R. This material is examinable. You should be able to work through the worksheets in your own time, but if you need help, there will be optional online drop-in sessions in the weeks 4 and 7 with \href{https://eps.leeds.ac.uk/maths/pgr/6422/muyang-zhang}{Muyang Zhang} through Microsoft Teams. (Your computing drop-in session may be listed as ``Practical'' on your timetable.)

The first computing worksheet will be a practice run, while a report on the second computing worksheet will be the second assessed piece of work.

\hypertarget{dropin}{%
\subsubsection*{Drop-in sessions}\label{dropin}}
\addcontentsline{toc}{subsubsection}{Drop-in sessions}

If you there is something in the course you wish to discuss in detail, the place for the is the optional weekly drop-in session. The drop-in sessions are an optional opportunity for you to ask questions you have to a member of staff -- nothing will happen unless you being your questions.

You will have been assigned to one of three groups on Tuesdays or Wednesdays with \href{https://eps.leeds.ac.uk/maths/pgr/4992/nikita-merkulov}{Nikita Merkulov} or me. The drop-in sessions will be run the Microsoft Teams. Your drop-in session would be an excellent place to go if you are having trouble understanding something in the written notes, or if you're still struggling on a problem sheet question after your workshop.

\hypertarget{team}{%
\subsubsection*{Microsoft Team}\label{team}}
\addcontentsline{toc}{subsubsection}{Microsoft Team}

I have set up \href{https://teams.microsoft.com/l/channel/19\%3a8cb8008c95204bbeaefa8ee7d48c1a13\%40thread.tacv2/General?groupId=1c138eac-0c54-43b0-9d20-d4cf3d65c40a\&tenantId=bdeaeda8-c81d-45ce-863e-5232a535b7cb}{a Microsoft Team} for the course. I propose to use the ``Q and A'' channel there as a discussion board. This is a good place to post questions about material from the course, and -- even better! -- to help answer you colleagues' questions. The idea is that you all as a group should help each other out. I will visit a couple of times a week to clarify if everybody is stumped by a question, or if there is disagreement.

\hypertarget{time}{%
\subsubsection*{Time management}\label{time}}
\addcontentsline{toc}{subsubsection}{Time management}

It is, of course, up to you how you choose to spend your time on this module. But, if you're interested, my recommendations would be something like this:

\begin{itemize}
\tightlist
\item
  \textbf{Every week:} 7.5 hours per week

  \begin{itemize}
  \tightlist
  \item
    \textbf{Notes and videos:} 2 sections, 1 hour each
  \item
    \textbf{Problem sheet:} 3.5 hours per week
  \item
    \textbf{Lecture:} 1 hour per week
  \item
    \textbf{Workshop:} 1 hour per week
  \end{itemize}
\item
  \textbf{When required:}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Assessments 1, 2 and 4:} 2 hours each
  \item
    \textbf{Computer worksheets:} 2 hours each
  \item
    \textbf{Revision}: 12 hours
  \end{itemize}
\item
  \textbf{Total:} 100 hours
\end{itemize}

\hypertarget{exam}{%
\subsubsection*{Exam}\label{exam}}
\addcontentsline{toc}{subsubsection}{Exam}

There will be an exam -- or, rather, a final ``online time-limited assessment'' -- after the end of the module, making up the remaining 85\% of your mark. The exam will consist of four questions, and you are expected to answer all of them. You will have 48 hours to complete the exam, although the exam itself should represent half a day to a day's work. Further details to follow nearer the time.

\hypertarget{ask}{%
\subsubsection*{Who should I ask about\ldots?}\label{ask}}
\addcontentsline{toc}{subsubsection}{Who should I ask about\ldots?}

\begin{itemize}
\tightlist
\item
  \emph{I don't understand something in the notes or on a problem sheet}: Go to your weekly drop-in session, or post a question on \href{https://teams.microsoft.com/l/channel/19\%3a5fcd058b7074426ca1f7d1cf2052d3b4\%40thread.tacv2/Q\%2520and\%2520A?groupId=1c138eac-0c54-43b0-9d20-d4cf3d65c40a\&tenantId=bdeaeda8-c81d-45ce-863e-5232a535b7cb}{the Teams Q and A board}. (If you email me, I am likely to respond, ``That would be an excellent question for your drop-in session or the Q and A board.'')
\item
  \emph{I don't understand something in on a computational worksheet:} Go to your computing drop-in session in weeks 4 or 7.
\item
  \emph{I have an admin question about general arrangements for the module:} \href{mailto:m.aldridge@leeds.ac.uk}{Email me}.
\item
  \emph{I have an admin question about arrangements for my workshop:} Email your workshop leader.
\item
  \emph{I have suggestion for something to cover in the lectures:} \href{mailto:m.aldridge@leeds.ac.uk}{Email me}.
\item
  \emph{I need an extension on or exemption from an assessment:} \href{mailto:Maths.Taught.Students@leeds.ac.uk}{Email the Maths Taught Students Office}.
\end{itemize}

\hypertarget{about-content}{%
\subsection*{Content of MATH2750}\label{about-content}}
\addcontentsline{toc}{subsection}{Content of MATH2750}

\hypertarget{prereqs}{%
\subsubsection*{Prerequisites}\label{prereqs}}
\addcontentsline{toc}{subsubsection}{Prerequisites}

Some students have asked what background you'll be expected to know for this course.

It's essential that you're very comfortable with the basics of probability theory: events, probability, discrete and continuous random variables, expectation, variance, approximations with the normal distribution, etc. Conditional probability and independence are particularly important concepts in this course. This course will use the binomial, geometric, Poisson, normal and exponential distributions, although the notes will usually remind you about them first, in case you've forgotten.

Many students on the module will have studied these topics in MATH1710 Probability and Statistics 1; others will have covered these in different modules.

\hypertarget{syllabus}{%
\subsubsection*{Syllabus}\label{syllabus}}
\addcontentsline{toc}{subsubsection}{Syllabus}

The course has two major parts: the first part will cover processes in discrete time and the second part processes in continuous time.

An outline plan of the topics covered is the following. (Remember that one week's work is two sections of notes.)

\begin{itemize}
\tightlist
\item
  \textbf{Discrete time Markov chains} {[}12 sections{]}

  \begin{itemize}
  \tightlist
  \item
    Introduction to stochastic processes {[}1 section{]}
  \item
    Important examples: Random walk, gambler's ruin, linear difference equations, examples from actuarial science {[}4 sections{]}
  \item
    General theory: transition probabilities, \(n\)-step transition probabilities, class structure, periodicity, hitting times, recurrence and transience, stationary distributions, long-term behaviour {[}6 sections{]}
  \item
    Revision {[}1 section{]}
  \end{itemize}
\item
  \textbf{Continuous time Markov jump processes} {[}10 sections{]}

  \begin{itemize}
  \tightlist
  \item
    Important examples: Poisson process, counting processes, queues {[}5 sections{]}
  \item
    General theory: holding times and jump chains, forward and backward equations, class structure, hitting times, stationary distributions, long-term behaviour {[}4 sections{]}
  \item
    Revision {[}1 section{]}
  \end{itemize}
\end{itemize}

\hypertarget{books}{%
\subsubsection*{Books}\label{books}}
\addcontentsline{toc}{subsubsection}{Books}

You can do well on this module by reading the notes and watching the videos, attending the lectures and workshops, and working on the problem sheets, assignments and practicals, without any further reading. However, students can benefit from optional extra background reading or an alternative view on the material.

My favourite book on Markov chains, which I used a lot while planning this course and writing these notes, is:

\begin{itemize}
\tightlist
\item
  J.R. Norris, \emph{Markov Chains}, Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press, 1997. Chapters 1-3.
\end{itemize}

This a whole book just on Markov processes, including some more detailed material that goes beyond this module. Its coverage of of both discrete and continuous time Markov processes is very thorough. \href{http://www.statslab.cam.ac.uk/~james/Markov/}{Chapter 1 on discrete time Markov chains is available online.}

Other good books with sections on Markov processes that I have used include:

\begin{itemize}
\tightlist
\item
  G.R. Grimmett and D.R. Stirzaker, \emph{Probability and Random Processes}, 4th edition, Oxford University Press, 2020. Chapter 6.
\item
  G. Grimmet and D. Walsh, \emph{Probability: an introduction}, 2nd edition, Oxford University Press, 2014. Chapter 12.
\item
  D.R. Stirzaker, \href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991013131349705181}{\emph{Elementary Probability}}, 2nd edition, Cambridge University Press, 2003. Chapter 9.
\end{itemize}

Grimmett and Stirzaker is an excellent handbook that covers most of undergraduate probability -- I bought a copy when I was a second-year undergraduate and still keep it next to my desk.
\href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991013131349705181}{The whole of Stirzaker is available online.}

A gentler introduction with plenty of examples is provided by:

\begin{itemize}
\tightlist
\item
  P.W. Jones and P. Smith, \href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991002938739705181}{\emph{Stochastic Processes: an introduction}}, 3nd edition, Texts in Statistical Science, CRC Press, 2018. Chapters 2-7.
\end{itemize}

although it doesn't cover everything in this module. \href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991002938739705181}{The whole book is available online.}

(I've listed the newest editions of these books, but older editions will usually be fine too.)

\hypertarget{finally}{%
\subsubsection*{And finally\ldots{}}\label{finally}}
\addcontentsline{toc}{subsubsection}{And finally\ldots{}}

These notes were mostly written by Matthew Aldridge in 2018--19, and have received updates (mostly in Sections 9--11) and reformatting this year. Some of the material (especially Section 1, Section 6, and numerous diagrams) follows closely previous notes by Dr Graham Murphy, and I also benefited from reading earlier notes by Dr Robert Aykroyd and Prof Alexander Veretennikov. Dr Murphy's general help and advice was also very valuable. Many thanks to students in previous runnings of the module for spotting errors and suggesting improvements.

\hypertarget{part-part-i-discrete-time-markov-chains}{%
\part*{Part I: Discrete time Markov chains}\label{part-part-i-discrete-time-markov-chains}}
\addcontentsline{toc}{part}{Part I: Discrete time Markov chains}

\hypertarget{S01-stochastic-processes}{%
\section{Stochastic processes and the Markov property}\label{S01-stochastic-processes}}

\begin{itemize}
\tightlist
\item
  Stochastic processes with discrete or continuous state space and discrete or continuous time
\item
  The Markov ``memoryless'' property
\end{itemize}

\hypertarget{models}{%
\subsection{Deterministic and random models}\label{models}}

A \textbf{model} is an imitation of a real-world system. For example, you might want to have a model to imitate the world's population, the level of water in a reservoir, cashflows of a pension scheme, or the price of a stock. Models allow us to try to understand and predict what might happen in the real world in a low risk, cost effective and fast way.

To design a model requires a set of assumptions about how it will work and suitable parameters need to be determined, perhaps based on past collected data.

An important distinction is between \textbf{deterministic} models and \textbf{random} models. Another word for a random model is a \textbf{stochastic} (``\emph{sto}-\textsc{\emph{kass}}-\emph{tik}'') model. Deterministic models do not contain any random components, so the output is completely determined by the inputs and any parameters. Random models have variable outcomes to account for uncertainty and unpredictability, so they can be run many times to give a sense of the range of possible outcomes.

Consider models for:

\begin{itemize}
\tightlist
\item
  the future position of the Moon as it orbits the Earth,
\item
  the future price of shares in Apple.
\end{itemize}

For the moon, the random components -- for example, the effect of small meteorites striking the Moon's surface -- are not very significant and a deterministic model based on physical laws is good enough for most purposes. For Apple shares, the price changes from day to day are highly uncertain, so a random model can account for the variability and unpredictability in a useful way.

In this module we will see many examples of stochastic models. Lots of the applications we will consider come from financial mathematics and actuarial science where the use of models that take into account uncertainty is very important, but the principles apply in many areas.

\hypertarget{stochastic-processes}{%
\subsection{Stochastic processes}\label{stochastic-processes}}

If we want to model, for example, the total number of claims to an insurance company in the whole of 2020, we can use a random variable \(X\) to model this -- perhaps a Poisson distribution with an appropriate mean. However, if we want to track how the number of claims changes over the course of the year 2021, we will need to use a \textbf{stochastic process} (or ``random process'').

A stochastic process, which we will usually write as \((X_n)\), is an indexed sequence of random variables that are (usually) dependent on each other.

Each random variable \(X_n\) takes a value in a \textbf{state space} \(\mathcal S\) which is the set of possible values for the process. As with usual random variables, the state space \(\mathcal S\) can be \textbf{discrete} or \textbf{continuous}. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \(\mathcal S = \{\text{Heads},\text{Tails}\}\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \(\mathcal S = \mathbb Z_+ = \{0,1,2,\dots\}\). A continuous state space denotes an uncountably infinite continuum of gradually varying outcomes. For example, the nonnegative real line \(\mathcal S = \mathbb R_+ = \{x \in \mathbb R : x \geq 0\}\) is the state space for the amount of rainfall on a given day, while some bounded subset of \(\mathbb R^3\) would be the state space for the position of a gas particle in a box.

Further, the process has an \textbf{index set} that puts the random variables that make up the process in order. The index set is usually interpreted as a \textbf{time} variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at distinct points, often denoted by \(n = 0,1,2,\dots\), while continuous time denotes a process monitored constantly over time, often denoted by \(t \in \mathbb R_+ = \{x \in \mathbb R : x \geq 0\}\). In the insurance example, we might count up the number of claims each day -- then the discrete index set will be the days of the year, which we could denote \(\{1,2,\dots,365\}\). Alternatively, we might want to keep a constant tally that we update after every claim, requiring a continuous time index \(t\) representing time across the whole year. In discrete time, we can write down the first few steps of the process as \((X_0, X_1, X_2, \dots)\).

This gives us four possibilities in total:

\begin{itemize}
\tightlist
\item
  \textbf{Discrete time, discrete space}

  \begin{itemize}
  \tightlist
  \item
    Example: Number of students attending each lecture of maths module.
  \item
    \textbf{Markov chains} -- discrete time, discrete space stochastic processes with a certain ``Markov property'' -- are the main topic of the first half of this module.
  \end{itemize}
\item
  \textbf{Discrete time, continuous space}

  \begin{itemize}
  \tightlist
  \item
    Example: Daily maximum temperature in Leeds.
  \item
    We will briefly mention continuous space Markov chains in the first half of the course, but these are not as important.
  \end{itemize}
\item
  \textbf{Continuous time, discrete space}

  \begin{itemize}
  \tightlist
  \item
    Example: Number of visitors to a webpage over time.
  \item
    \textbf{Markov jump processes} -- continuous time, discrete space stochastic processes with the ``Markov property'' -- are the main topic of the second half of this module.
  \end{itemize}
\item
  \textbf{Continuous time, continuous space}

  \begin{itemize}
  \tightlist
  \item
    Example: Level of the FTSE 100 share index over time.
  \item
    Such processes, especially the famous Brownian motion -- another process with the Markov property -- are very important, but outside the scope of this course. See MATH3734 Stochastic Calculus for Finance next year, for example.
  \end{itemize}
\end{itemize}

\hypertarget{markov-property}{%
\subsection{Markov property}\label{markov-property}}

Because stochastic processes consist of a large number -- even infinitely many; even uncountably infinitely many -- random variables that could all be dependent on each other, they can get extremely complicated. The Markov property is a crucial property that restricts the type of dependencies in a process, to make the process easier to study, yet still leaves most of the useful and interesting examples intact. (Although particular examples of Markov processes go back further, the first general study was by the Russian mathematician \href{https://mathshistory.st-andrews.ac.uk/Biographies/Markov/}{Andrey Andreyevich Markov}, published in 1906.)

Think of a simple board game where we roll a dice and move that many squares forward on the board. Suppose we are currently on the square \(X_n\). Then what can we say about which square \(X_{n+1}\) we move to on our next turn?

\begin{itemize}
\tightlist
\item
  \(X_{n+1}\) is random, since it depends on the roll of the dice.
\item
  \(X_{n+1}\) depends on where we are now \(X_n\), since the score of dice will be added onto the number our current square,
\item
  Given the square \(X_n\) we are now, \(X_{n+1}\) doesn't depend any further on which sequence of squares \(X_0, X_1, \dots, X_{n-1}\) we used to get here.
\end{itemize}

It is this third point that is the crucial property of the stochastic processes we will study in this course, and it is called the \textbf{Markov property} or \textbf{memoryless property}. We say ``memoryless'', because it's as if the process forgot how it got here -- we only need to remember what square we've reached, not which squares we used to get here. The stochastic process before this moment has no bearing on the future, given where we are now. A mathematical way to say this is that ``the past and the future are conditionally independent given the present.''

To write this down formally, we need to recall \textbf{conditional probability}: the conditional probability of an event \(A\) given another event \(B\) is written \(\mathbb P(A \mid B)\), and is the probability that \(A\) occurs \emph{given} that \(B\) definitely occurs. You may remember the definition
\[ \mathbb P(A \mid B) = \frac{\mathbb P(A \cap B)}{\mathbb P(B)} , \]
although is often more useful to reason directly about conditional probabilities than use this formula.

(You may also remember that the definition of conditional probability requires that \(\mathbb P(B) > 0\). Whenever we write down a conditional probability, we implicitly assume the conditioning event has strictly positive probability without explicitly saying so.)

\begin{definition}
\protect\hypertarget{def:def-markov-property}{}\label{def:def-markov-property}

Let \((X_n) = (X_0, X_1, X_2, \dots)\) be a stochastic process in discrete time \(n = 0,1,2,\dots\) and discrete space \(\mathcal S\). Then we say that \((X_n)\) has the \textbf{Markov property} if, for all times \(n\) and all states \(x_0, x_1, \dots,x_n, x_{n+1} \in \mathcal S\) we have
\[  \mathbb P(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}, X_{n-1} = x_{n-1}, \dots,X_0=x_0) = \mathbb P(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}) . \]

\end{definition}

Here, the left hand side is the probability we go to state \(x_{n+1}\) next conditioned on the entire history of the process, while the right hand side is the probability we go to state \(x_{n+1}\) next conditioned only on where we are now \(x_n\). So the Markov property tells us that it only matters where we are now and not how we got here.

(There's also a similar definition for continuous time processes, which we'll come to later in the course.)

Stochastic processes that have the Markov property are much easier to study than general processes, as we only have to keep track of where we are now and we can forget about the entire history that came before.

\textbf{In the next section}, we'll see the first, and most important, example of a discrete time discrete space Markov chain: the ``random walk''.

\hypertarget{S02-random-walk}{%
\section{Random walk}\label{S02-random-walk}}

\newcommand{\Var}{\operatorname{Var}}

\begin{itemize}
\tightlist
\item
  Definition of the simple random walk and the exact binomial distribution
\item
  Expectation and variance of general random walks
\end{itemize}

\hypertarget{simple-random-walk}{%
\subsection{Simple random walk}\label{simple-random-walk}}

Consider the following \textbf{simple random walk} on the integers \(\mathbb Z\): We start at \(0\), then at each time step, we go up by one with probability \(p\) and down by one with probability \(q = 1-p\). When \(p = q = \frac12\), we're equally as likely to go up as down, and we call this the \textbf{simple symmetric random walk}.

The simple random walk is a simple but very useful model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In many modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the ``drunkard's walk'', suggesting it could model a drunk person trying to stagger home.

\begin{figure}
\includegraphics[width=0.5\linewidth]{math2750_files/figure-latex/rw-pics-1} \includegraphics[width=0.5\linewidth]{math2750_files/figure-latex/rw-pics-2} \caption{Two simulations of random walks.}\label{fig:rw-pics}
\end{figure}

We can write this as a stochastic process \((X_n)\) with discrete time \(n = \{0,1,2,\dots\} = \mathbb Z_+\) and discrete state space \(\mathcal S = \mathbb Z\), where \(X_0 = 0\) and, for \(n \geq 0\), we have
\[ X_{n+1} = \begin{cases} X_n + 1 & \text{with probability $p$,} \\
                             X_n - 1 & \text{with probability $q$.} \end{cases} \]

It's clear from this definition that \(X_{n+1}\) (the future) depends on \(X_n\) (the present), but, given \(X_n\), does not depend on \(X_{n-1}, \dots, X_1, X_0\) (the past). Thus the Markov property holds, and the simple random walk is a \textbf{discrete time Markov process} or \textbf{Markov chain}.

\begin{example}
\protect\hypertarget{exm:rw1}{}\label{exm:rw1}

\emph{What is the probability that after two steps a simple random walk has reached \(X_2 = 2\)?}

To achieve this, the walk must go upwards in both time steps, so \(\mathbb P(X_2 = 2) = pp = p^2\).

\end{example}

\begin{example}
\protect\hypertarget{exm:rw2}{}\label{exm:rw2}

\emph{What is the probability that after three steps a simple random walk has reached \(X_3 = -1\)?}

There are three ways to reach \(-1\) after three steps: up--down--down, down--up--down, or down--down--up. So
\[ \mathbb P(X_3 = -1) = pqq+qpq+qqp = 3pq^2 . \]

\end{example}

\hypertarget{general-random-walks}{%
\subsection{General random walks}\label{general-random-walks}}

An alternative way to write the simple random walk is to put
\begin{equation}
    X_n = X_0 + \sum_{i=1}^n Z_i ,  \label{eq:rw}
  \end{equation}
where the starting point is \(X_0 = 0\) and the \textbf{increments} \(Z_1, Z_2, \dots\) are independent and identically distributed (IID) random variables with distribution given by \(\mathbb P(Z_i = 1) = p\) and \(\mathbb P(Z_i = -1) = q\). You can check that \eqref{eq:rw} means that \(X_{n+1} = X_n + Z_{n+1}\), and that this property defines the simple random walk.

Any stochastic process with the form \eqref{eq:rw} for some \(X_0\) and some distribution for the IID \(Z_i\)s is called a \textbf{random walk} (without the word ``simple'').

Random walks often have state space \(\mathcal S = \mathbb Z\), like the simple random walk, but they could be defined on other state spaces. We could look at higher dimensional simple random walks: in \(\mathbb Z^2\), for example, we could step up, down, left or right each with probability \(\frac14\). We could even have a continuous state space like \(\mathbb R\), if, for example, the \(Z_i\)s had a normal distribution.

We can use this structure to calculate the expectation or variance of any random walk (including the simple random walk).

Let's start with the expectation. For a random walk \((X_n)\) we have
\[ \mathbb E X_n = \mathbb E \left(X_0 + \sum_{i=1}^n Z_i\right) = \mathbb E X_0 + \sum_{i=1}^n \mathbb E Z_i = \mathbb EX_0 + n \mathbb E Z_1 , \]
where we've used the linearity of expectation, and that the \(Z_i\)s are identically distributed.

In the case of the simple random walk, we have \(\mathbb E X_0 = 0\), since we start from \(0\) with certainty, and
\[ \mathbb E Z_1 = \sum_{z \in \mathbb Z} z \mathbb P(Z_1 = z) = 1\times p + (-1)\times q = p-q .\]
Hence, for the simple random walk, \(\mathbb EX_n = n(p-q)\).

If \(p > \frac12\), then \(p > q\), so \(\mathbb E X_n\) grows ever bigger over time, while if \(p < \frac12\), then \(\mathbb E X_n\) grows ever smaller (that is, negative with larger absolute value) over time. If \(p = \frac12 = q\), which is the case of the simple symmetric random walk, then then the expectation \(\mathbb E X_n = 0\) is zero for all time.

Now the variance of a random walk. We have
\[ \operatorname{Var}(X_n) = \operatorname{Var}\left(X_0 + \sum_{i=1}^n Z_i\right) = \operatorname{Var}X_0 + \sum_{i=1}^n \operatorname{Var}Z_i = \operatorname{Var}X_0 + n \operatorname{Var}Z_1 , \]
where it was crucial that \(X_0\) and all the \(Z_i\)s were independent (so we had no covariance terms).

For the simple random walk we have \(\operatorname{Var}X_0 = 0\), since we always start from \(0\) with certainty. To calculate the variance of the increments, we write
\begin{align*}
  \operatorname{Var}(Z_1) &= \mathbb E (Z_1 - \mathbb EZ_1)^2 \\
            &= p\big(1 - (p-q)\big)^2 + q \big( {-1} - (p-q)\big)^2\\
            &= p(2q)^2 + q(-2p)^2\\
            &= 4pq^2 + 4p^2q \\
            &= 4pq(p+q) \\
            &= 4pq .
  \end{align*}
Here we've used that \(1-p = q\), \(1-q=p\), and \(p+q = 1\); you should take a few moments to check you've followed the algebra here. Hence the variance of the simple random walk is \(4pqn\). We see that (unless \(p\) is \(0\) or \(1\)) the variance grows over time, so it becomes harder and harder to predict where the random walk will be.

The variance of the simple symmetric random walk is \(4 \frac12 \frac12 n = n\).

For large \(n\), we can use a normal approximation for a random walk. Suppose the increments process \((Z_n)\) has mean \(\mu\) and variance \(\sigma^2\), and that the walk starts from \(X_0 = 0\). Then we have \(\mathbb E X_n = \mu n\) and \(\operatorname{Var}(X_n) = \sigma^2 n\), so for large \(n\) we can use the normal approximation \(X_n \approx \mathrm{N}(\mu n, \sigma^2 n)\). (Remember, of course, that the \(X_n\) are not independent.) To be more formal, the central limit theorem tells us that, as \(n \to \infty\), we have
\[ \frac{X_n - n\mu}{\sigma \sqrt{n}} \to \mathrm{N}(0,1) . \]

\hypertarget{exact-distribution}{%
\subsection{Exact distribution of the simple random walk}\label{exact-distribution}}

We have calculated the expectation and variance of any random walk. But for the simple random walk, we can in fact give the exact distribution, by writing down an exact formula for \(\mathbb P(X_n = i)\) for any time \(n\) and any state \(i\).

Recall that, at each of the first \(n\) times, we independently take an upward step with probability \(p\), and otherwise take a downward step. So if we let \(Y_n\) be the number of upward steps over the first \(n\) time periods, we see that \(Y_n\) has a binomial distribution \(Y_n \sim \text{Bin}(n,p)\).

Recall that the binomial distribution has probability
\[  \mathbb P(Y_n = k)  = \binom nk p^k (1-p)^{n-k} = \binom nk p^k q^{n-k} , \]
for \(k = 0,1,\dots, n\), where \(\binom{n}{k}\) is the binomial coefficient ``\(n\) choose \(k\)''.

If \(Y_n = k\), that means we've taken \(k\) upward steps and \(n-k\) downward steps, leaving us at position \(k - (n-k) = 2k - n\). Thus we have that
\begin{equation}
  \mathbb P(X_n = 2k - n) = \mathbb P(Y_n = k) = \binom nk p^k q^{n-k} .  \label{eq:bin}
  \end{equation}

Note that after an odd number of time steps \(n\) we're always at an odd-numbered state, since \(2k - \text{odd} = \text{odd}\), while after an even number of time steps \(n\) we're always at an even-numbered state, since \(2k - \text{even} = \text{even}\).

Writing \(i = 2k - n\) gives \(k = (n+i)/2\) and \(n-k = (n-i)/2\). So we can rearrange \eqref{eq:bin} to see that the distribution for the simple random walk is
\[ \mathbb P(X_n = i) =  \binom{n}{(n+i)/2} p^{(n+i)/2} q^{(n-i)/2} , \]
when \(n\) and \(i\) have the same parity with \(-n \leq i \leq n\), and is \(0\) otherwise.

In the special case of the simple symmetric random walk, we have
\[ \mathbb P(X_n = i) = \binom{n}{(n+i)/2} \left(\frac12\right)^{(n+i)/2} \left(\frac12\right)^{(n-i)/2} = \binom{n}{(n+i)/2} 2^{-n} . \]

\textbf{In the next section}, we look at a gambling problem based on the simple random walk.

\hypertarget{P01}{%
\section*{Problem Sheet 1}\label{P01}}
\addcontentsline{toc}{section}{Problem Sheet 1}

\commtrue

You should attempt all these questions and write up your solutions in advance of your workshop in week 2 (Monday 1 or Tuesday 2 February) where the answers will be discussed.

\textbf{1.} When designing a model for a quantity that changes over time, one has many decisions to make:

\begin{itemize}
\tightlist
\item
  Discrete or continuous state space?
\item
  Discrete or continuous index set for time?
\item
  Deterministic or stochastic model?
\item
  If a stochastic model is chosen, is it reasonable to assume that the Markov property holds?
\end{itemize}

What would you decide for the following scenarios:

\textbf{(a)} The percentage of UK voters with a positive opinion of Boris Johnson in a weekly tracking poll.

\textbf{(b)} The number of points won by a football league club throughout the season.

\textbf{(c)} The temperature of a bowl of water placed in an oven.

\textbf{(d)} The number of people inside the University of Leeds library.

\begin{myanswers}
\emph{Suggestions.} This question is meant to inspire discussion, so there are not necessarily right and wrong answers. If I were designing the models, however, my choices might be these:

\textbf{(a)} Discrete space if percentages are given to nearest 1\%, otherwise continuous; discrete time (weekly); stochastic; the Markov property might be appropriate, perhaps using some sort of random walk.

\textbf{(b)} Discrete space (number of points); discrete time (update after each game); stochastic; the Markov property might be appropriate, depending on if you think teams can have non-Markovian ``winning streaks'' (or losing streaks) that don't reflect underlying performance.

\textbf{(c)} Continuous space (temperature); continuous time; if the oven is reliable and the experiment carried out carefully, a deterministic model might be sufficient.

\textbf{(d)} Discrete space (number of people); continuous time; stochastic; Markov property might not be appropriate due to `bursts' of people leaving during fire drills or entering when it starts raining.

\end{myanswers}

\textbf{2.} A fair six-sided dice is rolled twice, resulting in the
values \(X_1, X_2 \in \{1, 2, \ldots, 6\}\). Let \(Y = X_1 + X_2\) be the total score.
Calculate:

\textbf{(a)} the probability \(\mathbb P(Y = 10)\);

\begin{myanswers}
\emph{Solution.} The following table illustrates the possible outcomes \(Y\) of the experiment. Each cell of the table is an equally probable outcome.

\begin{center}\includegraphics{math2750_files/figure-latex/dice-1} \end{center}

There are 3 possible ways to get \(Y=10\) (the grey cells in the table) out of the \(36\) possible outcomes, so we have \(\mathbb P(Y = 10) = 3/36 = 1/12\).

\end{myanswers}

\textbf{(b)} the conditional probability \(\mathbb P(Y=10 \mid X_1=x)\) for \(x=1, 2, \ldots, 6\);

\begin{myanswers}
\emph{Solution.} Conditioning on \(X_1 = x\) means restricting our attention only to column \(x\) of the table. Each column has \(6\) equally probably cells. For \(x=1,2,3\), none of the entries equal \(10\), so \(\mathbb P(Y=10 \mid X_1=x) = 0/6 = 0\). For each of \(x=4,5,6\), one of the entries equals \(10\), so \(\mathbb P(Y=10 \mid X_1=x) = 1/6\).

\end{myanswers}

\textbf{(c)} the conditional probability \(\mathbb P(X_1=x \mid Y=10)\) for \(x=1, 2, \ldots, 6\).

\begin{myanswers}
\emph{Solution.} Conditioning on \(Y =10\) means restricting our attention only to the \(3\) shaded cells, which are each equally likely. For \(x=1,2,3\), none of the shaded cells are in column \(x\), so \(\mathbb P(X_1=x \mid Y=10) = 0/3 = 0\). For each of \(x=4,5,6\), one of the shaded cells is in column \(x\), so \(\mathbb P(X_1=x \mid Y=10) = 1/3\).

\end{myanswers}

\textbf{3.} Let \((X_n)\) be a simple random walk starting from \(X_0 = 0\) and that at each step goes up one with probability \(p\) or down one with probability \(q = 1-p\). What are:

\textbf{(a)} \(\mathbb P(X_5 = 3)\),

\begin{myanswers}
\emph{Solution.} To get \(X_5 = 3\), we must take \(4\) steps up and \(1\) step down. The down step can be at any of the \(5\) time steps. Therefore we have \(\mathbb P(X_5 = 3) = 5p^4q\).

\end{myanswers}

\textbf{(b)} \(\mathbb P(X_5 = 3 \mid X_2 = 2)\),

\begin{myanswers}
\emph{Solution.} Once we're at \(X_2 = 2\), we must take \(2\) steps up and \(1\) step down over the next \(3\) time steps. So \(\mathbb P(X_5 = 3 \mid X_2 = 2) = 3p^2q\).

\end{myanswers}

\textbf{(c)} \(\mathbb P(X_n = n-2)\),

\begin{myanswers}
\emph{Solution.} This requires \(n-1\) steps up and \(1\) step down, and the down step can be at any of the \(n\) time steps. So \(\mathbb P(X_n = n-2) = np^{n-1}q\).

\end{myanswers}

\textbf{(d)} \(\mathbb E X_4\),

\begin{myanswers}
\emph{Solution.} The increments \(Z_n = X_n - X_{n-1}\) have expectation \(1p + (-1)q = p - q\), so \(\mathbb E X_4 = 4(p-q)\).

\end{myanswers}

\textbf{(e)} \(\mathbb E(X_6 \mid X_4 = 2)\),

\begin{myanswers}
\emph{Solution.} We are already at 2, then another two increments will take us up \(2(p-q)\) on average. Therefore \(\mathbb E(X_6 \mid X_4 = 2) = 2 + 2(p-q)\).

\end{myanswers}

\textbf{4.} The price \(X_n\) of a stock at the close of day \(n\) is modelled as a Gaussian random walk, where the increments \((Z_n)\) have a normal distribution \(Z_n \sim \text{N}(\mu, \sigma^2)\). The model assumes a drift of \(\mu = 0.7\) and a volatility of \(\sigma = 2.2\). The initial price is \(X_0 = 42.3\).

\textbf{(a)} Calculate the mean and variance of the price of the stock at the close of day \(5\).

\begin{myanswers}
\emph{Solution.} The mean and variance are
\begin{gather*}
  \mathbb EX_5 = \mathbb E X_0 + n \mathbb E Z_1 = 42.3 + 5 \cdot 0.7 = 45.8 , \\
  \operatorname{Var}X_5 = \operatorname{Var}X_0 + n \operatorname{Var}Z_1 = 0 + 5 (2.2)^2 = 24.2 .
  \end{gather*}

\end{myanswers}

\textbf{(b)} Give a 95\% prediction interval for the price at the close of day 5. (You might find it useful to recall that, if \(W \sim \text{N}(0,1)\) is a standard normal random variable, then \(\mathbb P(W \leq 1.96) = 0.975\).)

\begin{myanswers}
\emph{Solution.} Note that \(X_5\) itself is normally distributed, so \(X_5 \sim \text{N}(45.8,24.2)\). The 95\% prediction interval for a normal distribution \(\text{N}(\mu, \sigma^2)\) is \((\mu - 1.96\sigma, \mu + 1.96\sigma)\), so the prediction interval for \(X_5\) is
\[ \big(45.8 - 1.96\sqrt{24.2},  45.8 + 1.96\sqrt{24.2}\big) = (36.16, 55.44) . \]

\end{myanswers}

\textbf{(c)} After day 4, the prices at the end of each of the first four days have been recorded as \(X_1 = 44.4, X_2 = 44.0, X_3 = 47.1, X_4 = 47.8\). Update your prediction interval for the price at the close of day 5, and comment on how it differs from the earlier prediction interval.

\begin{myanswers}
\emph{Solution.} By the Markov property, \(X_5\) depends on \(X_4\), but given \(X_4\) does not depend on the other values, which we can therefore ignore. Since \(X_5 = X_4 + Z_5\), we have
\begin{gather*}
  \mathbb E(X_5 \mid X_4) = X_4 + \mathbb E Z_5 = 47.8 + 0.7 = 48.5 \\
  \operatorname{Var}(X_5 \mid X_4) = 0 + \operatorname{Var}Z_5 = 0 + (2.2)^2 = 4.84.
  \end{gather*}
The desired prediction interval is
\[ \big(48.5 - 1.96\sqrt{4.84},  48.5+ 1.96\sqrt{4.84}\big) = (44.19, 52.81) . \]
Compared to before, the centre of the prediction interval is slightly higher, because the stock has outperformed expectations so far, and the interval is much narrower, because as we get closer to day 5 we become less uncertain.

\end{myanswers}

\textbf{5.} A gambler decides to model her total winnings as a simple random walk starting from \(X_0 = 0\) that at each time goes up one with probability \(p\) and down one with probability \(1-p\), but where \(p\) is unknown. The first \(10\) recordings, \(X_1\) to \(X_{10}\), are
\[ (1, 2, 1, 2, 3, 4, 5, 6, 5, 6) . \]

\textbf{(a)} What would you guess for the value of \(p\), given this data?

\begin{myanswers}
\emph{Solution.} In \(10\) time steps, the process went up \(k = 8\) times and down \(n - k = 2\) times. So it seems reasonable to guess that \(p\) has the value \(\hat p = \frac{8}{10} = 0.8\).

\end{myanswers}

\textbf{(b)} More generally, how would you estimate \(p\) from the data \(X_0 = 0, X_1 = x_1, X_2 = x_2, \dots, X_n = x_n\)?

\begin{myanswers}
\emph{Solution.} We will estimate \(\hat p = k/n\), where \(k\) is the number of upward steps. We saw in lectures that \(k = (n + x_n)/2\), so our estimate is
\[ \hat p = \frac{n + x_n}{2n} = \frac12\ + \frac{x_n}{2n} . \]

\end{myanswers}

\textbf{(c)} Show that your estimate is in fact the maximum likelihood estimate of \(p\).

\begin{myanswers}
\emph{Solution.} The concept of ``maximum likelihood estimation'' will be known to those who have done MATH2715; this might be new for those who didn't take that course.

Let \(k = (n + x_n)/2\) be the number of upward steps. Then the ``likelihood'' is the probability mass function
\[ f(\mathbf x; p) = p^{k}(1-p)^{n-k} , \]
since we take \(k\) steps up and \(n-k\) steps down.
Given \(\mathbf x\) (or equivalently \(k\)) the ``maximum likelihood estimate'' is the value of \(p\) that maximises this likelihood.

As is often the case, it's equivalent but actually more convenient to maximise the log-likelihood
\[ \ell(\mathbf x; p) = \ln f(\mathbf x; p) = k \ln p + (n-k)\ln(1-p) .\]
We can perform the maximisation by differentiating and setting equal to \(0\). The derivative is
\[ \frac{\text{d}}{\text{d}p} \ell(\mathbf x; p) = \frac kp - \frac{n-k}{1-p} ,\]
so the maximum likelihood estimate \(\hat p\) satisfies
\[ 0 = \frac k{\hat p} - \frac{n-k}{1-\hat p} . \]

Solving this by clearing denominators we get
\[ 0 = (k - k\hat p) - (n\hat p - k\hat p) = k - n \hat p , \]
and rearranging gives \(\hat p = k/n\) as desired.

\end{myanswers}

\hypertarget{S03-gamblers-ruin}{%
\section{Gambler's ruin}\label{S03-gamblers-ruin}}

\begin{itemize}
\tightlist
\item
  The gambler's ruin Markov chain
\item
  Equations for probability of ruin and expected duration of the game by conditioning on the first step
\end{itemize}

\hypertarget{ruin-chain}{%
\subsection{Gambler's ruin Markov chain}\label{ruin-chain}}

Consider the following gambling problem. Alice is gambling against Bob. Alice starts with £\(a\) and Bob starts with £\(b\). It will be convenient to write \(m = a + b\) for the total amount of money, so Bob starts with £\((m-a)\). At each step of the game, both players bet £1; Alice wins £1 off Bob with probability \(p\), or Bob wins £1 off Alice with probability \(q\). The game continues until one player is out of money (or is ``ruined'').

Let \(X_n\) denote how much money Alice has after \(n\) steps of the game. We can write this as a stochastic process with discrete time \(n \in \{0,1,2,\dots\} = \mathbb Z_+\) and discrete state space \(\mathcal S = \{0,1,\dots,m\}\). Then \(X_0 = a\), and, for \(n \geq 0\), we have
\[ X_{n+1} = \begin{cases} X_n + 1 & \text{with probability $p$ if $1\leq X_n \leq m-1$,} \\
                           X_n - 1 & \text{with probability $q$ if $1\leq X_n \leq m-1$,} \\
                           0       & \text{if $X_n = 0$,} \\
                           m       & \text{if $X_n = m$.} \end{cases} \]
So Alice's money goes up one with probability \(p\) or down one with probability \(q\), unless the game is already over with \(X_n = 0\) (Alice is ruined) or \(X_n = m\) (Alice has won all Bob's money, so Bob in ruined).

We see that the gambler's ruin process \((X_n)\) clearly satisfies the Markov property: the next step \(X_{n+1}\) depends on where we are now \(X_n\), but, given that, does not depend on how we got here.

The gambler's ruin process is exactly like a simple random walk started from \(X_0 = a\) except that we have \textbf{absorbing barriers} and \(0\) and \(m\), where the random walk stops because one of the players has ruined. (One could also consider random walks with \textbf{reflecting barriers}, that bounce the random walk back into the state space, or \textbf{mixed barriers} that are absorbing or reflecting at random.)

There are two questions about the gambler's ruin that we'll try to answer in this section:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the probability that the game ends by Alice ruining?
\item
  How long does the game last on average?
\end{enumerate}

\hypertarget{ruin-probability}{%
\subsection{Probability of ruin}\label{ruin-probability}}

The gambling game continues until either Alice is ruined (\(X_n = 0\)) or Bob is ruined (\(X_n = m\)). A natural question to ask is: What is the probability that the game ends in Alice's ruin?

Let us write \(r_i\) for the probability Alice ends up ruined if she currently has £\(i\). Then the probability of ruin for the whole game is \(r_a\), since Alice initially starts with £\(a\). The probability Bob will end up ruined is \(1 - r_a\), since one of the players must lose.

What can we say about \(r_i\)? Clearly we have \(r_0 = 1\), since \(X_n = 0\) means that Alice has run out of money and is ruined, and \(r_m = 0\), since \(X_n = m\) means that Alice has won all the money and Bob is ruined. What about when \(1 \leq i \leq m-1\)?

The key is to \emph{condition on the first step}. That is, we can write
\begin{align*}
\mathbb P(\text{ruin}) &= \mathbb P(\text{win first round}) \, \mathbb P(\text{ruin} \mid \text{win first round}) \\
&\qquad{}\quad {}+ \mathbb P(\text{lose first round}) \, \mathbb P(\text{ruin} \mid \text{lose first round}) \\
&= p\,\mathbb P(\text{ruin} \mid \text{win first round}) + q \,\mathbb P(\text{ruin} \mid \text{lose first round}) .
\end{align*}
Here we have conditioned on whether Alice wins or loses the first round. More formally, we have used the \textbf{law of total probability}, which says that if the events \(B_1, \dots, B_k\) are disjoint and cover the whole sample space, then
\[ \mathbb P(A) = \sum_{i=1}^k \mathbb P(B_i) \, \mathbb P(A \mid B_i) . \]
Here, \(\{\text{Alice wins the first round}\}\) and \(\{\text{Alice loses the first round}\}\) are indeed disjoint events that cover the whole sample space. This idea of ``conditioning on the first step'' will be the most crucial tool throughout this whole module.

If Alice wins the first round from having £\(i\), she now has £\((i+1)\). Her probability of ruin is now \(r_{i+1}\), because, by the Markov property, it's as if the game were starting again with Alice having £\((i+1)\) to start with. The Markov property tells us that it doesn't matter \emph{how} Alice got to having £\((i+1)\), it only matters how much she has now. Similarly, if Alice loses the first round, she now has £\((i-1)\), and the ruin probability is \(r_{i-1}\). Hence we have
\[ r_i = pr_{i+1} + qr_{i-1}. \]

Rearranging, and including the ``boundary conditions'', we see that the equation we want to solve is
\[ pr_{i+1} - r_i + qr_{i-1} = 0 \qquad \text{subject to} \qquad r_0 = 1,\ r_m = 0. \]
This is a \textbf{linear difference equation} -- and, because the left-hand side is \(0\), we call it a \textbf{homogeneous} linear difference equation.

We will see how to solve this equation in the next lecture. We will see that, if we set \(\rho = q/p\), then the ruin probability is given by
\[ r_a = \begin{cases} \displaystyle\frac{\rho^a - \rho^m}{1 - \rho^m} & \text{if $\rho \neq 1$,} \\[0.35cm]
           1 - \displaystyle\frac{a}{m} & \text{if $\rho = 1$.} \end{cases} \]
Note that \(\rho = 1\) is the same as the symmetric condition \(p = q = \frac12\).

Imagine Alice is not playing against a similar opponent Bob, but rather is up against a large casino. In this case, the casino's capital £\((m-a)\) is typically much bigger than Alice's £\(a\). We can model this by keeping \(a\) fixed taking a limit \(m \to \infty\). Typically, the casino has an ``edge'', meaning they have a better than \(50:50\) chance of winning; this means that \(q > p\), so \(\rho > 1\). In this case, we see that the ruin probability is
\[ \lim_{m \to \infty} r_a = \lim_{m \to \infty} \frac{\rho^a - \rho^m}{1 - \rho^m} = \lim_{m \to \infty} \frac{\rho^a/\rho^m - 1}{1/\rho^m - 1} = \frac{0-1}{0-1} = 1, \]
so Alice will be ruined with certainty.

Even with a generous casino that offered an exactly fair game with \(p = q = \frac12\), so \(\rho = 1\), we would have
\[ \lim_{m \to \infty} r_a = \lim_{m \to \infty}\left( 1 - \frac{a}{m} \right) = 1-0 = 1 , \]
so, even with this fair game, Alice would still be ruined with certainty.

(The official advice of the University of Leeds module MATH2750 is that you shouldn't gamble against a casino if you can't afford to lose.)

\hypertarget{expected-duration}{%
\subsection{Expected duration of the game}\label{expected-duration}}

We could also ask for how long we expect the game to last.

We approach this like before. Let \(d_i\) be the expected duration of the game from a point when Alice has £\(i\). Our boundary conditions are \(d_0 = d_m = 0\), because \(X_n = 0\) or \(X_n = m\) means that the game is over with Alice or Bob ruined. Again, we proceed by conditioning on the first step, so
\begin{align*}
\mathbb E(\text{duration}) &= \mathbb P(\text{win first round}) \, \mathbb E(\text{duration} \mid \text{win first round}) \\
&\qquad{}+ \mathbb P(\text{lose first round}) \, \mathbb E(\text{duration} \mid \text{lose first round}) \\
&= p\,\mathbb E(\text{duration} \mid \text{win first round}) + q \,\mathbb E(\text{duration} \mid \text{lose first round}) .
\end{align*}
More formally, we've used another version of the law of total probability,
\[ \mathbb E(X) = \sum_{i=1}^k \mathbb P(B_i) \, \mathbb E(X \mid B_i) , \]
or, alternatively, the \textbf{tower law} for expectations
\[ \mathbb E(X) = \mathbb E_Y \mathbb E (X \mid Y) = \sum_{y} \mathbb P(Y= y)\, E(X \mid Y = y), \]
where, in our case, \(Y\) was the outcome of the first round.

Now, the expected duration given we win the first round is \(1 + d_{i+1}\). This is because the round itself takes \(1\) time step, and then, by the Markov property, it's as if we are starting again from \(i+1\). Similarly, the expected duration given we lose the first round is \(1 + d_{i-1}\). Thus we have
\[ d_i = p(1 + d_{i+1}) + q (1 + d_{i-1}) = 1 + pd_{i+1} + qd_{i-1} . \]
Don't forget the 1 that counts the current round!

Rearranging, and including the boundary conditions, we have another linear difference equation:
\[ pd_{i+1} - d_i + qd_{i-1} = -1 \qquad \text{subject to} \qquad d_0 = 0,\ d_m = 0. \]
Because the right-hand side, \(-1\), is nonzero, we call this an \textbf{inhomogeneous} linear difference equation.

Again, we'll see how to solve this in the next lecture, and will find that the solution is given by
\[ d_a = \begin{cases} {\displaystyle \frac{1}{q-p} \left(a - m\frac{1-\rho^a}{1- \rho^m} \right)} & \text{if $\rho \neq 1$,} \\
\displaystyle a(m-a) & \text{if $\rho = 1$.} \end{cases} \]

Thinking again of playing against the casino, with \(q > p\), \(\rho > 1\), and \(m \to \infty\), we see that the expected duration is
\[ \lim_{m\to\infty} d_a = \lim_{m\to\infty} \frac{1}{q-p} \left(a - m\frac{1-\rho^a}{1 - \rho^m} \right)  = \frac{1}{q-p} \left(a - 0 \right) = \frac{a}{q-p} , \]
since \(\rho^m\) grows much quicker than \(m\). So Alice ruins with certainty, and it will take time \(a/(q-p)\), on average.

In the case of the generous casino, though, with \(q = p\), so \(\rho = 1\), we have
\[ \lim_{m\to\infty} d_a =  \lim_{m\to\infty} a(m-a) = \infty .  \]
So here, Alice will ruin with certainty, but it may take a very long time until the ruin occurs, since the expected duration is infinite.

\textbf{In the next section}, we see how to solve linear difference equations, in order to find the ruin probability and expected duration of the gambler's ruin.

\hypertarget{S04-ldes}{%
\section{Linear difference equations}\label{S04-ldes}}

\begin{itemize}
\tightlist
\item
  How to solve homogeneous and inhomogeneous linear difference equations
\item
  Solving for probability of ruin and expected duration of the gambler's ruin
\end{itemize}

In \protect\hyperlink{S03-gamblers-ruin}{the previous section}, we looked at the probability of ruin and expected duration of the gambler's ruin process. We set up linear difference equations to find these. In this section, we'll learn how to solve these equations.

A \textbf{linear difference equation} is an equation that looks like
\begin{equation}
a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = f(n) \label{eq:lde} 
\end{equation}
for \(n = 0,1,\dots\), where the \(a_i\) are given constants, \(f(n)\) is a given function, and we want to solve for the sequence \((x_n)\). The equation normally comes with some extra conditions, such as the value of the first few \(x_n\)s.

When the right-hand side of \eqref{eq:lde} is zero, so \(f(n) = 0\), we say the equation is \textbf{homogeneous}; when the right-hand side is nonzero, it is \textbf{inhomogeneous}. The number \(k\), where there are \(k+1\) terms on the left-hand side, is called the \textbf{degree} of the equation; we are mostly interested in second-degree linear difference equations, which have three terms on the left-hand side.

\hypertarget{hom-ldes}{%
\subsection{Homogeneous linear difference equations}\label{hom-ldes}}

We start with the homogeneous case, which is simpler.

Consider a homogeneous linear difference equation. We shall use the second-degree example
\[ x_{n+2} - 5x_{n+1} + 6x_{n} = 0 \qquad \text{subject to } x_0 = 4, x_1 = 9 .  \]
Here, the conditions on \(x_0\) and \(x_1\) are \textbf{initial conditions}, because they tell us how the sequence \((x_n)\) starts.

For the moment, we shall put the initial conditions to the side and just worry about the equation
\[ x_{n+2} - 5x_{n+1} + 6x_{n} = 0 . \]
We start by guessing there might be a solution of the form \(x_n = \lambda^n\) for some constant \(\lambda\). We can find out if there is such a solution by substituting in \(x_n = \lambda^n\), and seeing if there's a \(\lambda\) that solves the equation. For our example, we get
\[ \lambda^{n+2} - 5 \lambda^{n+1} + 6\lambda^n = 0 . \]
After cancelling off a common factor of \(\lambda^n\), we get
\[ \lambda^2 - 5 \lambda + 6 = 0 . \]
This is called the \textbf{characteristic equation}. For a general homogeneous linear difference equation \eqref{eq:lde}, the characteristic equation is
\begin{equation}
  a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 .  \label{eq:cheq} 
\end{equation}
When writing out answers to questions, you can jump straight to the characteristic equation.

We can now solve the characteristic equation for \(\lambda\). In our example, we can factor the left-hand side to get \((\lambda - 3)(\lambda - 2) = 0\), which has solutions \(\lambda = 2\) and \(\lambda = 3\). Thus \(x_n = 2^n\) and \(x_n = 3^n\) both solve our equation. In fact, since the right-hand side of the equation is \(0\), any linear combination of these two solutions is a solution also, thus we get the \textbf{general solution}
\[ x_n = A 2^n + B 3^n , \]
which is a solution for any values of the constants \(A\) and \(B\).

For a general characteristic equation with distinct roots \(\lambda_1, \lambda_2, \dots, \lambda_k\), the general solution is
\[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_k \lambda_k^n . \]
If we have a repeated root -- say, \(\lambda_1 = \lambda_2 = \cdots = \lambda_r\) is repeated \(r\) times -- than you can check that a solution is given by
\[ x_n = (D_0 + D_1 n + \cdots + D_{r-1} n^{r-1}) \lambda_1^n , \]
which should take its place in the general solution.

Once we have the general solution, we can use the extra conditions to find the values of the constants. In our example, we can use the initial conditions to find out the values of \(A\) and \(B\). We see that
\begin{gather*}
x_0 = A2^0 + B3^0 = A + B = 4 , \\
x_1 = A2^1 + B3^1 = 2A + 3B = 9 .
\end{gather*}
We can now solve this pair of simultaneous equations to solve for \(A\) and \(B\). By subtracting twice the first equation from the second we get \(B = 1\), and substituting this into the first equation we get \(A = 3\). Thus the solution is
\[ x_n = 3\cdot 2^n + 3^n . \]

In conclusion, the process here was:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the general solution by writing down and solving the characteristic equation.
\item
  Use the extra conditions to find the values of the constants in the general solution.
\end{enumerate}

Here are two more examples. These also give an idea of how I would expect you to set out your own answers to similar problems.

\begin{example}
\protect\hypertarget{exm:lde1}{}\label{exm:lde1}

\emph{Solve the homogeneous linear difference equation}
\[ x_{n+2} - x_{n+1} - 6x_n = 0 \qquad \text{subject to} \quad x_0 = 3,\quad x_1 = 4 . \]

\emph{Step 1.} The characteristic equation is
\[ \lambda^2 - \lambda - 6 = 0 . \]
We can solve this by factorising it as \((\lambda - 3) (\lambda + 2) = 0\),
to find the solutions \(\lambda_1 = -2\) and \(\lambda_2 = 3\). Thus the general solution is
\[ x_n = A(-2)^n + B3^n . \]

\emph{Step 2.} Substituting the initial conditions into the general solution, we have
\begin{align*}
x_0 &= A(-2)^0 + B3^0 = A + B = 3 \\
x_1 &= A(-2)^1 + B3^1 = -2A + 3B = 4 .
\end{align*}
We can add twice the first equation to the second to get \(5B = 10\), so \(B=2\). We can substitute this into the first equation to get \(A = 1\).

The solution is therefore
\[ x_n = 1\cdot(-2)^n + 2 \cdot 3^n = (-2)^n + 2 \cdot 3^n . \]

\end{example}

\begin{example}
\protect\hypertarget{exm:lde2}{}\label{exm:lde2}

\emph{Solve the homogeneous linear difference equation}
\[ x_{n+2} + 4x_{n+1} +4x_n = 0 \qquad \text{subject to} \quad x_0 = 2,\quad x_1 = -6 . \]

\emph{Step 1.} The characteristic equation is
\[ \lambda^2 + 4\lambda + 4 = 0 . \]
We can solve this by factorising it as \((\lambda + 2)^2 = 0\), to find a repeated root \(\lambda_1 = \lambda_2 = -2\). Thus the general solution is
\[ x_n = (A + Bn) (-2)^n . \]

\emph{Step 2.} Substituting the initial conditions into the general solution, we have
\begin{align*}
x_0 &= (A + B0)(-2)^0 = A = 2 \\
x_1 &= (A + B1)(-2)^1 = -2A - 2B = -6 .
\end{align*}
The first immediately gives \(A = 2\), and substituting this into the second equation gives \(B = 1\).

The solution is therefore
\[ x_n = (2 + n)(-2)^n . \]

\end{example}

\hypertarget{ruin-probability-solve}{%
\subsection{Probability of ruin for the gambler's ruin}\label{ruin-probability-solve}}

In the last lecture we saw that probability of ruin for the gambler's ruin process is the solution to
\[ pr_{i+1} - r_i + qr_{i-1} = 0 \qquad \text{subject to} \qquad r_0 = 1,\ r_m = 0 , \]
where the extra conditions here are \textbf{boundary conditions}, because they tell us what happens at the boundaries of the state space.

The characteristic equation is
\[ p\lambda^2 - \lambda + q = 0 .\]
We can solve the characteristic equation by factorising it as \((p \lambda - q)(\lambda - 1) = 0\). (It might take a moment to check this really is a factorisation of the characteristic equation. Hint: we've used that \(p+q=1\).) So the characteristic equation has roots \(\lambda = q/p\), which we called \(\rho\) last time, and \(\lambda = 1\). Now, if \(\rho = 1\) (so \(p = q = \frac12\)) we have a repeated root, while if \(\rho \neq 1\) we have distinct roots, so we'll need to deal with the two cases separately.

First, the case \(\rho \neq 1\). Since the two roots are distinct, we have the general solution
\[ r_i = A\rho^i + B1^i = A\rho^i + B . \]

We can now use the boundary conditions to find \(A\) and \(B\). We have
\begin{gather*} r_0 = A \rho^0 + B = A+B = 1, \\
                r_m = A \rho^m + B = 0 . \end{gather*}
From the first we get \(B = 1-A\), which we substitute into the second to get
\[ A\rho^m + 1 - A = 0 \quad \Rightarrow \quad A = \frac{1}{1-\rho^m} , \]
and hence
\[ B = 1 - A = 1 - \frac{1}{1-\rho^m} = - \frac{\rho^m}{1 - \rho^m} . \]
Thus the solution is
\[ r_i = \frac{1}{1-\rho^m} \rho^i -  \frac{\rho^m}{1 - \rho^m} = \frac{\rho^i - \rho^m}{1 - \rho^m}  , \]
as we claimed last time.

Second, the case \(\rho = 1\). Now we have a repeated root \(\lambda = 1\), so the general solution is
\[ r_i = (A + Bi) 1^i = A+Bi . \]

Again, we use the boundary conditions, to get
\begin{gather*} r_0 = A + B\cdot 0 = A = 1, \\
r_m = A + Bm = 0 , \end{gather*}
and we immediately see that \(A = 1\) and \(B = -1/m\). Thus the solution is
\[ r_i = 1 - \frac{1}{m}i = 1 - \frac{i}{m} , \]
as claimed last time.

\hypertarget{inhom-ldes}{%
\subsection{Inhomogeneous linear difference equations}\label{inhom-ldes}}

Solving inhomogeneous linear difference equations requires three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the general solution to the \emph{homogeneous} equation by writing down and solving the characteristic equation.
\item
  By making an ``educated guess'', find a solution (a ``particular solution'') to the inhomogeneous equation. The general solution to the inhomogeneous equation is a particular solution plus the general solution to the homogeneous equation.
\item
  Use the extra conditions to find the values of the constants in the general solution.
\end{enumerate}

This idea works because, once you have a particular solution, adding a solution to the \emph{homogeneous} equation to the left-hand side adds zero to the right-hand side, so maintains a solution to the inhomogeneous equation.

Let's work through the example
\[ x_{n+2} - 5x_{n+1} + 6x_{n} = 2 \qquad \text{subject to } x_0 = 4, x_1 = 9 . \]

We already know from earlier that the general solution to the homogeneous equation \(x_{n+2} - 5x_{n+1} + 6x_{n} = 0\) (with a zero on the right-hand side) is
\[ x_n = A2^n + B3^n . \]

We now need to find a \textbf{particular solution} -- that is, any solution -- to our new inhomogeneous equation. The usual process here is to guess a solution with the same ``shape'' as the right-hand side. For example, if the right-hand side is a constant, try a constant for the particular solution. Here our right-hand side is the constant \(2\), so we should try a constant \(x_n = C\). Substituting this into the inhomogeneous equation gives us \(C - 5C + 6C = 2\), thus \(2C = 2\) and \(C = 1\), giving a particular solution \(x_n = 1\). The general solution to the inhomogeneous equation is therefore
\[ x_n = 1 + A2^n + B3^n , \]
the sum of the particular solution \(x_n = 1\) and the general solution to the homogeneous equation.

Because the right-hand side was a constant, we guessed a constant -- this is the main case we will deal with. Other cases you could come across include:

\begin{itemize}
\tightlist
\item
  If the right-hand side is a polynomial of degree \(d\), try a polynomial of degree \(d\).
\item
  If the right-hand side is \(\alpha^n\) for some \(\alpha\), try \(C\alpha^n\).
\item
  If the right-hand side is a constant, but a constant \(C\) doesn't work, try \(Cn\). If that still doesn't work, try \(Cn^2\), and so on. A general rule is that is 1 is a root of the characteristic equation with multiplicity \(m\), you need to try \(Cn^m\). We discuss this case further in the next subsection.
\end{itemize}

Continuing with the example, we use the initial conditions to get the constants \(A\) and \(B\). We have
\begin{gather*}
x_0 = 1 + A2^0 + B3^0 = 1+ A + B = 4 , \\
x_1 = 1 + A2^1 + B3^1 = 1+ 2A + 3B = 9 .
\end{gather*}
The second equation minus twice the first gives \(-1 + B = 1\), so \(B=2\), and substituting that back into the first gives \(A = 1\). Thus the solution is
\[ x_n = 1 + 1\cdot 2^n + 2 \cdot 3^n = 1 + 2^n + 2 \cdot 3^n . \]

Here's another example.

\begin{example}
\protect\hypertarget{exm:lde3}{}\label{exm:lde3}

\emph{Solve the inhomogeneous linear difference equation}
\[ 10 x_{n+2} - 7x_{n+1} + x_n = 8 \qquad \text{subject to} \quad x_0 = 0,\quad x_1 = \tfrac{13}{10} . \]

\emph{Step 1.} The characteristic equation is
\[ 10\lambda^2 - 7\lambda + 1 = 0 . \]
We can solve this by factorising it as
\[ (2\lambda - 1) (5\lambda - 1) = 0 , \]
to find the solutions \(\lambda_1 = \frac12\) and \(\lambda_2 = \frac15\). Thus the general solution of the homogeneous equation is
\[ x_n = A\left(\frac12\right)^n + B\left(\frac15\right)^n . \]

\emph{Step 2.} Since the right hand side of the inhomogeneous equation is a constant, we guess a constant particular solution with shape \(x_n = C\). Substituting in this guess, we get
\[ 10C - 7C + C = 4C = 8 \]
with solution \(C=2\). Thus a particular solution is \(x_n = 2\), and the general solution to the inhomogeneous equation is
\[ x_n = 2 + A\left(\frac12\right)^n + B\left(\frac15\right)^n . \]

\emph{Step 3.} Substituting the initial conditions into the general solution, we have
\begin{align*}
x_0 = 2 + A\left(\frac12\right)^0 + B\left(\frac15\right)^0 = 2 + A + B = 0 \quad &\Rightarrow \quad A + B = -2 \\
x_1 = 2 + A\left(\frac12\right)^1 + B\left(\frac15\right)^1 = 2 + A\frac12 + B\frac15 = \frac{13}{10} \quad &\Rightarrow \quad 5A + 2B = -7.
\end{align*}
We can take twice the first equation abd subtract the second to get \(-3A = 3\), so \(A = -1\). We can substitute this into the second equation to get \(B = -1\).

The solution is therefore
\[ x_n = 2 - \left(\frac12\right)^n - \left(\frac15\right)^n . \]

\end{example}

\hypertarget{duration-solve}{%
\subsection{Expected duration for the gambler's ruin}\label{duration-solve}}

From last time, the expected duration of the gambler's ruin game solves
\[ pd_{i+1} - d_i + qd_{i-1} = -1 \qquad \text{subject to} \qquad d_0 = 0,\ d_m = 0. \]
As before, we divide cases based on whether or not \(\rho = 1\).

First, the case \(\rho \neq 1\). We already know that the general solution to the homogeneous equation is
\[ d_i =  A \rho^i + B . \]

Now we need a particular solution. It's tempting to guess a constant \(C\) for a particular solution, but we know that constants solve the homogeneous equation, since \(d_i = B\) is a solution, so a constant will give right-hand side \(0\), not \(-1\). (We could try out \(x_i = C\) if we wanted; we would get \((p - 1 + q)C = -1\), but \(p-1+q=0\), and \(0 \times C = -1\) has no solution.) The next best try is to go one degree up: let's guess \(x_i = Ci\) instead. This gives
\begin{align*}
  -1 &= pC(i+1) - Ci + qC(i-1)\\
     &= C(pi + p - i + qi - q) \\
     &= C\big((p+q-1)i + (p-q)\big) \\
     &= C(p-q) ,
  \end{align*}
since \(p + q - 1 = 1 - 1 = 0\). This \(C = -1/(p-q) = 1/(q-p)\). Finding a solution for \(C\) shows that our guess worked. The general solution to the inhomogeneous equation is
\[ d_i = \frac{i}{q-p} + A \rho^i + B .  \]

Then to find the constants, we have
\begin{gather*} d_0 = \frac{0}{q-p} + A \rho^0 + B = A+B = 0, \\
                  d_m = \frac{m}{q-p} + A \rho^m + B = 0 , \end{gather*}
which you can check gives
\[ A = -B = \frac{m}{q-p} \cdot \frac{1}{1 - \rho^m} . \]
Hence, the solution is
\[ d_i = \frac{i}{q-p} + \frac{m}{q-p} \frac{1}{1 - \rho^m} \rho^i - \frac{m}{q-p} \frac{1}{1 - \rho^m} =  \frac{1}{q-p} \left(i - m\frac{1-\rho^i}{1- \rho^m} \right) . \]

Second, the case \(\rho = 1\), so \(p = q = \frac12\). We already know that the general solution to the homogeneous equation is
\[ d_i =  A + Bi . \]

We need a particular solution. Since 1 is a double root of the characteristic equation, both constants \(x_i = A\) and linear \(x_i = Bi\) terms solve the homogeneous equation. (You can check that guessing \(x_i = C\) or \(x_i = Ci\) doesn't work, if you like.) So we'll have to go up another degree and try \(x_i = Ci^2\). This gives
\begin{align*}
    -1 &= \tfrac12 C(i+1)^2 - Ci^2 + \tfrac 12 C(i-1)^2 \\
       &= \tfrac12 C(i^2 + 2i + 1 - 2i^2 + i^2 - 2i + 1) \\
       &= \tfrac12 C\big((1-2+1)i^2 + (2-2)i + (1+1)\big) \\
       &=C ,
\end{align*}
so the general solution to the inhomogeneous equation is
\[ d_i = -i^2 + A + Bi .  \]

Then to find the constants, we have
\begin{gather*} d_0 = -0^2 + A + B\cdot0 = A = 0, \\
                  d_m = -m^2 + A + Bm = 0 , \end{gather*}
giving \(A = 0, B = m\). The solution is
\[ d_i = -i^2 + 0 + mi = i(m-i) .\]

\textbf{In the next section}, we move on from the specific cases we've looked at so far to the general theory of discrete time Markov chains.

\hypertarget{P02}{%
\section*{Problem sheet 2}\label{P02}}
\addcontentsline{toc}{section}{Problem sheet 2}

\commtrue

You should attempt all these questions and write up your solutions in advance of your workshop in week 3 (Monday 8 or Tuesday 9 February) where the answers will be discussed.

\textbf{1.} Solve the following linear difference equations:

\textbf{(a)} \(x_{n+2} - 4x_{n+1} + 3x_{n} = 0\), subject to \(x_0 = 0\), \(x_1 = 2\).

\begin{myanswers}
\emph{Solution.} The characteristic equation is \(\lambda^2 - 4\lambda + 3 = 0\), which factorises as \((\lambda - 3)(\lambda - 1) = 0\), with solutions \(\lambda = 1,3\), so the general solution is \(x_n = A1^n + B3^n = A + B3^n\). The initial conditions give \(A+B = 0\) and \(A + 3B = 2\), meaning \(B = 1\) and \(A = -1\). Hence the solution is \(x_n = 3^n - 1\).

\end{myanswers}

\textbf{(b)} \(4x_{n+1} = 4x_n - x_{n-1}\), subject to \(x_0 = 1\), \(x_1 = 0\).

\begin{myanswers}
\emph{Solution.} First, we rearrange to \(4x_{n+1} - 4x_n + x_{n-1} = 0\). The characteristic equation is \(4\lambda^2 - 4\lambda + 1 = 0\), which factorises as \((2\lambda - 1)^2 = 0\), which has a repeated root \(\lambda = \frac12\), so the general solution is \(x_n = (A + Bn)(\frac12)^n\). The initial conditions give \(A=1\) and \((A + B)/2 = 0\), meaning \(B = -1\). Hence the solution is \(x_n = (1 - n)(\frac12)^n\).

\end{myanswers}

\textbf{(c)} \(x_n-5x_{n-1} + 6x_{n-2} = 1\), subject to \(x_0 = 1\), \(x_1 = 2\).

\begin{myanswers}
\emph{Solution.} The characteristic equation is \(\lambda^2 - 5\lambda + 6 = 0\), which factorises as \((\lambda - 2)(\lambda - 3) = 0\), with solutions \(\lambda = 2,3\), so the general solution to the homogeneous equation is \(A2^n + B3^n\). For a particular solution, we guess a solution of the form \(x_n = C\); substituting this into the inhomogeneous equation gives \(C - 5C + 6C = 1\), so \(C = \frac12\). So the general solution to the inhomogeneous equation is \(x_n = A2^n + B3^n + \frac12\). The initial conditions give \(A+B \frac12= 1\) and \(2A + 3B + \frac12= 2\), which is solved by \(B = \frac12\) and \(A = 0\). Hence the solution is \(x_n = \frac12 3^n + \frac12 = (3^n + 1)/2\).

\end{myanswers}

\textbf{(d)} \(x_{n+2} - 2x_{n+1} + x_n = -1\), subject to \(x_0 = 0\), \(x_1 = 2\).

\begin{myanswers}
\emph{Solution.} The characteristic equation is \(\lambda^2 - 2\lambda + 1 = 0\), which factorises as \((\lambda - 1)^2 = 0\), with a repeated root \(\lambda = 1\), so the general solution to the homogeneous equation is \((A + Bn)1^n = A + Bn\). For a particular solution, since constant and linear terms will equal \(0\), not \(-1\), we guess a solution of the form \(x_n = Cn^2\); substituting this into the inhomogeneous equation gives
\[ C(n+2)^2 - 2C(n+1)^2 + Cn^2 = 2C = -1  \]
so \(C = -\frac12\). So the general solution to the inhomogeneous equation is \(x_n = A + Bn - \frac12 n^2\). The initial conditions give \(A = 0\) and \(A + B - \frac12= 2\), so \(B = \frac52\). Hence the solution is \(x_n = \frac52n - \frac12n^2 = \frac n2(5-n)\).

\end{myanswers}

\textbf{2.} Consider a simple symmetric random walk on the state space \(\mathcal{S} = \{0,1,\ldots ,m\}\) with an absorbing barrier at \(0\) and a reflecting barrier at \(m\). In other words,
\[ \mathbb P(X_{n+1} = 0 \mid X_n = 0) = 1 \quad \text{and} \quad  \mathbb P(X_{n+1} = m-1 \mid X_n = m) = 1 . \]
Let \(\eta_i\) be the expected time until the the walk hits \(0\) when starting from \(i \in \mathcal S\).

\textbf{(a)} Explain why \((\eta_i)\) satisfies
\[ \eta_i = 1 + \tfrac12 \eta_{i+1} +\tfrac12 \eta_{i-1} \]
for \(i \in \{1,2,\dots,m-1\}\).

\begin{myanswers}
\emph{Solution.} We condition on the first step. The first step itself takes time 1. After that, with probability \(\frac12\) we are at state \(i+1\), with expected time remaining \(\eta_{i+1}\), while with probability \(\frac12\) we are at state \(i-1\), with expected time remaining \(\eta_{i-1}\).

\end{myanswers}

\textbf{(b)} Give a similar equation for \(\eta_m\), and state the value of \(\eta_0\).

\begin{myanswers}
\emph{Solution.} From \(m\), we move to \(m-1\) with certainty, so conditioning on the first step gives \(\eta_m = 1 + \eta_{m-1}\).

Clearly \(\eta_0 = 0\), as we stop immediately.

\end{myanswers}

\textbf{(c)} Hence, find the value of \(\eta_i\) for all \(i \in \mathcal S\).

\begin{myanswers}
\emph{Solution.} We rewrite the equation as \(\eta_{i+1} - 2 \eta_i + \eta_{i-1} = -2\). This has characteristic equation \(\lambda^2 - 2\lambda + 1 = 0\), which factorises as \((\lambda-1)^2\), with a repeated root of \(1\), so the general solution to the homogeneous equation is \(A + Bi\). By the same logic as before, we attempt a particular solution of the form \(\eta_i = Ci^2\), which gives
\[ C(i+1)^2 - 2Ci^2 + C(i-1)^2 = 2C = -2 ,   \]
so \(C = -1\). The general solution to the inhomogeneous equation is therefore \(\eta_i = A + Bi - i^2\). From the boundary condition \(k_0 = 0\) we have \(A = 0\). From the boundary condition \(k_m = 1 + k_{m-1}\) we have
\[ Bm - m^2 = 1 + B(m-1) - (m-1)^2 = Bm - B - m^2 +2m  , \]
giving \(B = 2m\). Therefore the solution is \(\eta_i = 2mi - i^2 = i(2m - i)\).

\end{myanswers}

\textbf{(d)} You should notice that your answer is the same as the expected duration of the gambler's ruin for \(p = \frac12\), except with \(m\) replaced by \(2m\). Can you explain why this might be?

\begin{myanswers}
\emph{Solution.} This is an example of the \textbf{reflection principle}. Let \((Y_n)\) be a gambler's ruin (simple random walk with two absorbing barriers) on \(\{0,1,\dots, 2m\}\). Then consider placing a mirror at \(m\), and viewing the Markov chain so that it remains in the first half \(\{0,1,\dots,m\}\); more formally, we consider \((X_n)\) where
\[ X_n = \begin{cases} Y_n & \text{if $Y_n \leq m$} \\
                      2m - Y_n & \text{if $Y_n > m$.} \end{cases}   \]
Then \((X_n)\) is the half-reflecting random walk we consider in this question. Further, \((X_n)\) is absorbed at \(0\) when \((Y_n)\) is absorbed at either \(0\) or \(2m\), which has the given expected time \(i(2m-i)\).

\end{myanswers}

\textbf{3.} Consider the gambler's ruin problem with draws: at each step, Alice wins £1 with probability \(p\), loses £1 with probability \(q\), and neither wins nor loses any money with probability \(s\), where \(p + q +s = 1\), and \(0 < p,q,s<1\). Alice starts with £\(a\) and Bob with £\((m-a)\).

\textbf{(a)} Let \(r_i\) be Alice's probability of ruin given that she has £\(i\).

\textbf{(i)} Write down a linear difference equation for \((r_i)\), remembering to include appropriate boundary conditions.

\begin{myanswers}
\emph{Solution.} By conditioning on the first step, we have
\[ r_i = pr_{i+1} + sr_i + qr_{i-1} , \]
which can be rearranged to
\[ pr_{i+1} - (1-s)r_i + qr_{i-1} = 0 . \]
The boundary conditions are \(r_0 = 1\) and \(r_m = 0\).

\end{myanswers}

\textbf{(ii)} Solve the linear difference equation, to find \(r_a\), Alice's probability of ruin. You may assume that \(p \neq q\).

\begin{myanswers}
\emph{Solution.} The characteristic equation is \(p\lambda^2 - (1-s)\lambda + q = 0\), which factorises as \((p\lambda - q)(\lambda - 1) = 0\), since \(p + q = 1-s\). The solutions are \(\lambda = q/p = \rho\) and \(\lambda = 1\). Since we assume \(p \neq q\), we have that \(\rho \neq 1\), so we have unique roots, and general solution
\(r_i = A + B \rho^i\). The boundary conditions give \(A + B = 1\) and \(A + B\rho^m = 0\), meaning that \(B = 1/(1-\rho^m)\) and \(A = -\rho^m/(1-\rho^m)\), so the solution is
\[ r_i = -\frac{\rho^m}{1-\rho^m} + \frac{1}{1-\rho^m}\rho^i = \frac{\rho^i - \rho^m}{1-\rho^m}.   \]

\end{myanswers}

\textbf{(b)} Let \(d_i\) be be the expected duration of the game from the point that Alice has £\(i\).

\textbf{(i)} Write down a linear difference equation for \((d_i)\), remembering to include appropriate boundary conditions.

\begin{myanswers}
\emph{Solution.} By conditioning on the first step, we have
\[ d_i = p(1 + d_{i+1}) + s(1 + d_i) + q(1 + d_{i-1}) ,  \]
which after rearranging gives
\[ pd_{i+1} - (1-s)d_i + qd_{i-1} = -1.  \]
The boundary conditions are \(d_0 = 0\) and \(d_m = 0\).

\end{myanswers}

\textbf{(ii)} Solve the linear difference equation, to find \(d_a\), the expected duration of the game. You may assume that \(p \neq q\).

\begin{myanswers}
\emph{Solution.} As before, the solution to the homogeneous equation is \(A + B\rho^i\). We try a particular solution of the for \(d_i = Ci\), and find that
\[ pC(i+1) -(1-s)Ci + qC(i-1) = C(p-q) = -1 ,\]
so \(C= 1/(q-p)\), and the general solution to the inhomogeneous equation is
\[ d_i = A + B\rho^i + \frac{i}{q-p} .  \]
The boundary conditions give \(A + B = 0\) and \(A + B \rho^m + m/(q-p) = 0\), meaning that
\[ B = -A = \frac{m}{q-p} \frac{1}{1-\rho^m} . \]
Hence the solution is
\[ d_i = \frac{1}{q-p} \left(i - m\frac{1-\rho^i}{1-\rho^m} \right) .   \]

\end{myanswers}

\textbf{(c)} Alice starts playing against Bob in a standard gambler's ruin game with probabilities \(p \neq q\) and \(s = 0\). A draw probability \(s > 0\) is then introduced in such a way that the ratio \(\rho = q/p\) remains constant. Comment on how this changes Alice's ruin probability and the expected duration of the game.

\begin{myanswers}
\emph{Solution.} The ruin probability does not change, as we see immediately. This is not surprising, as the win and lose probabilities for a round conditional on the round not being a draw have stayed the same.

The expected duration of the game increases. If \(\rho = q/p\) stays the same while introducing a draw probability \(s\), then the ``new'' \(q\) and \(p\) are \((1-s)q\) and \((1-s)p\), so \(q-p\) becomes\((1-s)q - (1-s)p = (1-s)(q-p)\). Hence expected duration goes up by a factor of \(1/(1-s)\). This makes sense, since number of rounds until a non-draw result is a geometric distribution with expectation \(1/(1-s)\), so each step takes \(1/(1-s)\) times as long on average.

\end{myanswers}

\textbf{4.} The Fibonacci numbers are 1, 1, 2, 3, 5, 8, 13, 21, 34, \ldots, where each number in the sequence is the sum of the two previous numbers. Show that the ratio of consecutive Fibonacci numbers tends to the ``golden ratio'' \(\phi = (1 + \sqrt{5})/2\).

\begin{myanswers}
\emph{Solution.} The Fibonacci numbers \((F_n)\) satisfy \(F_{n+1} = F_n + F_{n-1}\), which rearranges to \(F_{n+1} -F_n - F_{n-1} = 0\). The is a linear difference equation with characteristic equation \(\lambda^2 - \lambda - 1 = 0\). This has two solutions, which can be found using the quadratic formula. The solution with larger absolute value is \(\lambda_1 = (1+\sqrt{5})/2 = \phi\), the golden ratio, and the solution with smaller absolute value is \(\lambda_2 = (1-\sqrt{5})/2\). Hence, the general solution to the equation is \(F_n = A\phi^n + B\lambda_2^n\). We could use the initial conditions \(F_1 = 1\) and \(F_2 = 1\) to find \(A\) and \(B\), but there's no need to here.

The ratio of consecutive Fibonacci numbers is
\[ \frac{F_{n+1}}{F_n} = \frac{A\phi^{n+1} + B\lambda_2^{n+1}}{A\phi^n + B\lambda_2^n} = \frac{\phi + B\lambda_2^{n+1}/A\phi^n}{1 + B\lambda_2^n/A\phi^n} \to \frac{\phi + 0}{1 + 0} = \phi \]
as \(n \to \infty\), since \(|\lambda_2/\phi| < 1\) means that \(\lambda_2^n / \phi^n \to 0\).

\end{myanswers}

\hypertarget{A1}{%
\section*{Assessment 1}\label{A1}}
\addcontentsline{toc}{section}{Assessment 1}

A solutions sheet for this assessment is available on Minerva. Marks and feedback are available on Minerva and Gradescope respectively.

\textbf{1.} Let \((X_n)\) be a simple random walk that starts from \(X_0 = 0\) and on each step goes up one with probability \(p\) and down one with probability \(q = 1-p\).

Calculate:

\textbf{(a)} \(\mathbb P(X_6 = 0)\), {{[}1 mark{]}}

\textbf{(b)} \(\mathbb EX_6\), {{[}1{]}}

\textbf{(c)} \(\text{Var}(X_6)\), {{[}1{]}}

\textbf{(d)} \(\mathbb E(X_{10} \mid X_4 = 4)\), {{[}1{]}}

\textbf{(e)} \(\mathbb P(X_{10} = 0 \mid X_6 = 2)\), {{[}1{]}}

\textbf{(f)} \(\mathbb P(X_4 = 2 \mid X_{10} = 6)\). {{[}1{]}}

Consider the case \(p = 0.6\), so \(q = 0.4\).

\textbf{(g)} What are \(\mathbb E X_{100}\) and \(\text{Var}(X_{100})\)? {{[}1{]}}

\textbf{(h)} Using a normal approximation, estimate \(\mathbb P(16 \leq X_{100} \leq 26)\). You should use an appropriate ``continuity correction'', and explain why you chose it. (Bear in mind the possible values \(X_{100}\) can take.) {{[}3{]}}

\textbf{2.} Consider the gambler's ruin with draws: Alice starts with £\(a\) and Bob with £\((m-a)\), and at each time step Alice wins £1 off Bob with probability \(p\), loses £1 to Bob with probability \(q\), and no money is exchanged with probability \(s\), where \(p+q+s =1\). We consider the case where Bob and Alice are equally matched, so \(p = q\) and \(s = 1-2p\). (We assume \(0 < p < 1/2\).)

Let \(r_i\) be Alice's ruin probability from the point she has £\(i\).

\textbf{(a)} By conditioning on the first step, explain why \(pr_{i+1} - (1-s)r_i + pr_{i-1} = 0\), and give appropriate boundary conditions. {{[}2{]}}

\textbf{(b)} Solve this linear difference equation to find an expression for \(r_i\). {{[}2{]}}

Let \(d_i\) be the expected duration of the game from the point Alice has £\(i\).

\textbf{(c)} Explain why \(pd_{i+1} - (1-s)d_i + pd_{i-1} = -1\), and give appropriate boundary conditions. {{[}2{]}}

\textbf{(d)} Solve this linear difference equation to find an expression for \(d_i\). {{[}2{]}}

\textbf{(e)} Compare your answer to parts (b) and (d) with those for the standard gambler's ruin problem with \(p = 1/2\), and give reasons for the similarities or differences. {{[}2{]}}

\hypertarget{S05-markov-chains}{%
\section{Discrete time Markov chains}\label{S05-markov-chains}}

\begin{itemize}
\tightlist
\item
  Definition of time homogeneous discrete time Markov chains
\item
  Calculating \(n\)-step transition properties
\item
  The Chapman--Kolomogorov equations
\end{itemize}

\hypertarget{thmc}{%
\subsection{Time homogeneous discrete time Markov chains}\label{thmc}}

So far we've seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.

To define a so-called ``Markov chain'', we first need to say where we start from, and second what the probabilities of transitions from one state to another are.

In our examples of the simple random walk and gambler's ruin, we specified the start point \(X_0 = i\) exactly, but we could pick the start point at random according to some distribution \(\lambda_i = \mathbb P(X_0 = i)\).

After that, we want to know the \textbf{transition probabilities} \(\mathbb P(X_{n+1} = j \mid X_n = i)\) for \(i,j \in \mathcal S\). Here, because of the Markov property, the transition probability only needs to condition on the state we're in now \(X_n = i\), and not on the whole history of the process.

In the case of the simple random walk, for example, we had initial distribution
\[ \lambda_i = \mathbb P(X_0 = i) = \begin{cases} 1 & \text{if $i = 0$} \\ 0 & \text{otherwise} \end{cases} \]
and transition probabilities
\[ \mathbb P(X_{n+1} = j \mid X_n = i) = \begin{cases} p & \text{if $j = i+1$} \\ q & \text{if $j = i-1$} \\ 0 & \text{otherwise.} \end{cases} \]

For the random walk (and also the gambler's ruin), the transition probabilities \(\mathbb P(X_{n+1} = j \mid X_n = i)\) don't depend on \(n\); in other words, the transition probabilities stay the same over time. A Markov process with this property is called \textbf{time homogeneous}. We will always consider time homogeneous processes from now on (unless we say otherwise).

Let's write \(p_{ij} = \mathbb P(X_{n+1} = j \mid X_n = i)\) for the transition probabilities, which are independent of \(n\).
We must have \(p_{ij} \geq 0\), since it is a probability, and we must also have \(\sum_j p_{ij} = 1\) for all states \(i\), as this is the sum of the probabilities of all the places you can move to from state i.

\begin{definition}
\protect\hypertarget{def:def-thmc}{}\label{def:def-thmc}

Let \((\lambda_i)\) be a probability distribution on a sample space \(\mathcal S\). Let \(p_{ij}\), where \(i,j \in \mathcal S\), be such that \(p_{ij} \geq 0\) for all \(i,j\), and \(\sum_j p_{ij} = 1\) for all \(i\). Let the time index be \(n = 0,1,2,\dots\). Then the \textbf{time homogeneous discrete time Markov process} or \textbf{Markov chain} \((X_n)\) with initial distribution \((\lambda_i)\) and transition probabilities \((p_{ij})\) is defined by
\begin{gather*}
    \mathbb P(X_0 = i) = \lambda_i ,\\
    \mathbb P(X_{n+1} = j \mid X_n = i, X_{n-1} = x_{n-1}, \dots, X_0 = x_0) = \mathbb P(X_{n+1} = j \mid X_n = i) =  p_{ij}  . \end{gather*}

\end{definition}

When the state space is finite (and even sometimes when it's not), it's convenient to write the transition probabilities \((p_{ij})\) as a matrix \(\mathsf P\), called the \textbf{transition matrix}, whose \((i,j)\)th entry is \(p_{ij}\). Then the condition that \(\sum_j p_{ij} = 1\) is the condition that each of the rows of \(\mathsf P\) add up to \(1\).

\begin{example}
\protect\hypertarget{exm:mcex}{}\label{exm:mcex}

\emph{In this notation, what is \(\mathbb P(X_0 = i \text{ and } X_1 = j)\)?}

First we must start from \(i\), and then we must move from \(i\) to \(j\), so
\[ \mathbb P(X_0 = i \text{ and } X_1 = j) = \mathbb P(X_0 = i)\mathbb P(X_1 = j \mid X_0 = i) = \lambda_i p_{ij} . \]

\emph{In this notation, what is \(\mathbb P(X_{n+2} = j \text{ and } X_{n+1} = k \mid X_n = i)\)?}

First we must move from \(i\) to \(k\), then we must move from \(k\) to \(j\), so
\begin{align*}
\mathbb P(X_{n+2} = j \text{ and } X_{n+1} = k \mid X_n = i)
&= \mathbb P(X_{n+1} = k \mid X_n = i)\mathbb P(X_{n+2} = j \mid X_{n+1} = k) \\
&= p_{ik}p_{kj} .
\end{align*}
Note that the term \(\mathbb P(X_{n+2} = j \mid X_{n+1} = k)\) did not have to depend on \(X_n\), thanks to the Markov property.

\end{example}

\hypertarget{S05-example}{%
\subsection{A two-state example}\label{S05-example}}

Consider a simple two-state Markov chain with state space \(\mathcal S = \{0,1\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} p_{00} & p_{01} \\ p_{10} & p_{11} \end{pmatrix} = \begin{pmatrix} 1-\alpha & \alpha \\ \beta & 1-\beta \end{pmatrix}  \]
for some \(0 < \alpha, \beta < 1\). Note that the rows of \(\mathsf P\) add up to \(1\), as they must.

We can illustrate \(\mathsf P\) by a \textbf{transition diagram}, where the blobs are the states and the arrows give the transition probabilities. (We don't draw the arrow if \(p_{ij} = 0\).) In this case, our transition diagram looks like this:

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/twostate-1} 

}

\caption{Transition diagram for the two-state Markov chain}\label{fig:twostate}
\end{figure}

We can use this as a simple model of a broken printer, for example. If the printer is broken (state 0) on one day, then with probability \(\alpha\) it will be fixed (state 1) by the next day; while if it is working (state 1), then with probability \(\beta\) it will have broken down (state 0) by the next day.

\begin{example}
\protect\hypertarget{exm:printer}{}\label{exm:printer}

\emph{If the printer is working on Monday, what's the probability that it also is working on Wednesday?}

If we call Monday day \(n\), then Wednesday is day \(n+2\), and we want to find the two-step transition probability.
\[ p_{11}(2) = \mathbb P (X_{n+2} = 1 \mid X_n = 1) . \]
The key to calculating this is to \emph{condition on the first step} again -- that is, on whether the printer is working on Tuesday. We have
\begin{align*}
  p_{11}(2) &= \mathbb P (X_{n+1} = 0 \mid X_n = 1)\,\mathbb P (X_{n+2} = 1 \mid X_{n+1} = 0, X_n = 1) \\
  &\qquad{} + \mathbb P (X_{n+1} = 1 \mid X_n = 1)\,\mathbb P (X_{n+2} = 1 \mid X_{n+1} = 1, X_n = 1) \\
  &= \mathbb P (X_{n+1} = 0 \mid X_n = 1)\,\mathbb P (X_{n+2} = 1 \mid X_{n+1} = 0) \\
  &\qquad{} + \mathbb P (X_{n+1} = 1 \mid X_n = 1)\,\mathbb P (X_{n+2} = 1 \mid X_{n+1} = 1) \\
  &= p_{10}p_{01} + p_{11}p_{11} \\
  &= \beta\alpha + (1-\beta)^2 .
\end{align*}
In the second equality, we used the Markov property to mean conditional probabilities like \(\mathbb P(X_{n+2} = 1 \mid X_{n+1} = k)\) did not have to depend on \(X_n\).

Another way to think of this as we summing the probabilities of all length-2 paths from 1 to 1, which are \(1\to 0\to 1\) with probability \(\beta\alpha\) and \(1 \to 1 \to 1\) with probability \((1-\beta)^2\)

\end{example}

\hypertarget{n-step}{%
\subsection{\texorpdfstring{\emph{n}-step transition probabilities}{n-step transition probabilities}}\label{n-step}}

In the above example, we calculated a two-step transition probability \(p_{ij}(2) = \mathbb P (X_{n+2} = j \mid X_n = i)\) by conditioning on the first step. That is, by considering all the possible intermediate steps \(k\), we have
\[ p_{ij}(2) = \sum_{k\in\mathcal S} \mathbb P (X_{n+1} = k \mid X_n = i)\mathbb P (X_{n+2} = j \mid X_{n+1} = k) = \sum_{k\in\mathcal S} p_{ik}p_{kj} . \]

But this is exactly the formula for multiplying the matrix \(\mathsf P\) with itself! In other words, \(p_{ij}(2) = \sum_{k} p_{ik}p_{kj}\) is the \((i,j)\)th entry of the matrix square \(\mathsf P^2 = \mathsf{PP}\). If we write \(\mathsf P(2) = (p_{ij}(2))\) for the matrix of two-step transition probabilities, we have \(\mathsf P(2) = \mathsf P^2\).

More generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths \(i\to k_1 \to k_2 \to \cdots \to k_{n-1} \to j\) of length \(n\) from \(i\) to \(j\).

\begin{theorem}
\protect\hypertarget{thm:thm-n-step}{}\label{thm:thm-n-step}

Let \((X_n)\) be a Markov chain with state space \(\mathcal S\) and transition matrix \(\mathsf P = (p_{ij})\). For \(i,j \in \mathcal S\), write
\[ p_{ij}(n) = \mathbb P(X_n = j \mid X_0 = i) \]
for the \(n\)-step transition probability. Then
\[ p_{ij}(n) = \sum_{k_1, k_2, \dots, k_{n-1} \in \mathcal S} p_{ik_1} p_{k_1k_2} \cdots p_{k_{n-2}k_{n-1}} p_{k_{n-1}j} . \]
In particular, \(p_{ij}(n)\) is the \((i,j)\)th element of the matrix power \(\mathsf P^n\), and the matrix of \(n\)-step transition probabilities is given by \(\mathsf P(n) = \mathsf P^n\).

\end{theorem}

The so-called \textbf{Chapman--Kolmogorov equations} follow immediately from this.

\begin{theorem}[Chapman–Kolmogorov equations]
\protect\hypertarget{thm:c-k}{}\label{thm:c-k}

Let \((X_n)\) be a Markov chain with state space \(\mathcal S\) and transition matrix \(\mathsf P = (p_{ij})\). Then, for non-negative integers \(n,m\), we have
\[ p_{ij}(n+m) = \sum_{k \in \mathcal S} p_{ik}(n)p_{kj}(m) , \]
or, in matrix notation, \(\mathsf P(n+m) = \mathsf P(n)\mathsf P(m)\).

\end{theorem}

In other words, a trip of length \(n + m\) from \(i\) to \(j\) is a trip of length \(n\) from \(i\) to some other state \(k\), then a trip of length \(m\) from \(k\) back to \(j\), and this intermediate stop \(k\) can be any state, so we have to sum the probabilities.

Of course, once we know that \(\mathsf P(n) = \mathsf P^n\) is given by the matrix power, it's clear to see that \(\mathsf P(n+m) = \mathsf P^{n+m} = \mathsf P^n \mathsf P^m = \mathsf P(n)\mathsf P(m)\).

Sidney Chapman (1888--1970) was a British applied mathematician and physicist, who studied applications of Markov processes. Andrey Nikolaevich Kolmogorov (1903--1987) was a Russian mathematician who did very important work in many different areas of mathematics, is considered the ``father of modern probability theory'', and studied the theory of Markov processes. (Kolmogorov is also \href{https://genealogy.math.ndsu.nodak.edu/id.php?id=53569}{my academic great-great-grandfather}.)

\begin{example}
\protect\hypertarget{exm:printer2}{}\label{exm:printer2}

In our two-state broken printer example above, the matrix of two-state transition probabilities is given by
\begin{align*}
\mathsf P(2) = \mathsf P^2 &=  \begin{pmatrix} 1-\alpha & \alpha \\ \beta & 1-\beta \end{pmatrix}  \begin{pmatrix} 1-\alpha & \alpha \\ \beta & 1-\beta \end{pmatrix} \\
&=  \begin{pmatrix} (1-\alpha)^2 + \alpha\beta & (1-\alpha)\alpha + \alpha(1-\beta) \\ \beta(1-\alpha) + (1-\beta)\beta & \beta\alpha + (1-\beta)^2 \end{pmatrix} ,
\end{align*}
where the bottom right entry \(p_{11}(2)\) is what we calculated earlier.

\end{example}

One final comment. It's also convenient to consider the initial distribution \(\boldsymbol\lambda = (\lambda_i)\) as a \emph{row} vector. The first-step distribution is given by
\[ \mathbb P(X_1 = j) = \sum_{i \in \mathcal S} \lambda_i p_{ij} , \]
by conditioning on the start point.
This is exactly the \(j\)th element of the vector--matrix multiplication \(\boldsymbol\lambda \mathsf P\). More generally, the row vector of of probabilities after \(n\) steps is given by \(\boldsymbol\lambda \mathsf P^n\).

In the next section, we look at how to model some actuarial problems using Markov chains.

\hypertarget{S06-examples}{%
\section{Examples from actuarial science}\label{S06-examples}}

\begin{itemize}
\tightlist
\item
  Three Markov chain models for insurance problems
\end{itemize}

In this lecture we'll set up three simple models for an insurance company that can be analysed using ideas about Markov chains. The first example has a direct Markov chain model. For the second and third examples, we will have to be clever to find a Markov chain associated to the situation.

\hypertarget{S06-example1}{%
\subsection{A simple no-claims discount model}\label{S06-example1}}

A motor insurance company puts policy holders into three categories:

\begin{itemize}
\tightlist
\item
  no discount on premiums (state 1)
\item
  25\% discount on premiums (state 2)
\item
  50\% discount on premiums (state 3)
\end{itemize}

New policy holders start with no discount (state 1). Following a year with no insurance claims, policy holders move up one level of discount. If they start the year in state 3 and make no claim, they remain in state 3. Following a year with at least one claim, they move down one level of discount. If they start the year in state 1 and make at least one claim, they remain in state 1. The insurance company believes that probability that a motorist has a claim free year is \(\frac34\).

We can model this directly as a Markov chain:

\begin{itemize}
\tightlist
\item
  the state space \(\mathcal S = \{1,2,3\}\) is discrete;
\item
  the time index is discrete, as we have one discount level each year;
\item
  the probability of being in a certain state at a future time is completely determined by the present state (the Markov property);
\item
  the one-step transition probabilities are not time dependent (time homogeneous).
\end{itemize}

The transition probability and transition diagram of the Markov chain are:
\[ \mathsf P = \begin{pmatrix} \frac14 & \frac34 & 0 \\ \frac14 & 0 & \frac34 \\ 0 & \frac14 & \frac34 \end{pmatrix} . \]

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/example1-1} 

}

\caption{Transition diagram for the simple no-claims discount model}\label{fig:example1}
\end{figure}

\begin{example}
\protect\hypertarget{exm:act1}{}\label{exm:act1}

\emph{What is the probability of having a 50\% reduction to your premium three years from now, given that you currently have no reduction on the premium?}

We want to find the three-step transition probability
\[
p_{13}(3) = \mathbb P(X_{3} = 3 \mid X_0=1) .
\]
We can find this by summing over all paths \(1 \to k_1 \to k_2 \to 3\). There are two such paths, \(1 \to 1 \to 2 \to 3\) and \(1 \to 2 \to 3 \to 3\). Thus
\[ p_{13}(3) = p_{11}p_{12}p_{23} + p_{12}p_{23}p_{33} = \frac14 \cdot\frac34 \cdot\frac34 + \frac34 \cdot\frac34 \cdot\frac34 = \frac{36}{64} = \frac{9}{16} . \]

Alternatively, we could directly calculate all the three-step transition probabilities by the matrix method, to get
\[ \mathsf P(3) = \mathsf P^3 = \mathsf{PPP} = \frac{1}{64} \begin{pmatrix} 7 & 21 & 36 \\ 7 & 12 & 45 \\ 4 & 15 & 45 \end{pmatrix} .\]
(You can check this yourself, if you want.) The desired \(p_{13}(3)\) is the top right entry \(36/64 = 9/16\).

\end{example}

\hypertarget{S06-example2}{%
\subsection{An accident model with memory}\label{S06-example2}}

Sometimes, we are presented with a situation where the ``obvious'' stochastic process is not a Markov chain. But sometimes we can find a related process that \emph{is} a Markov chain, and study that instead. As an example of this, we consider at a different accident model.

According to a different model, a motorist's \(n\)th year of driving is either accident free, or has exactly one accident. (The model does not allow for more than one accident in a year.) Let \(Y_n\) be a random variable so that,
\[
Y_n=\begin{cases}
0&\text{ if the motorist has no accident in year $n$,}\\
1&\text{ if the motorist has one accident in year $n$.}
\end{cases}
\]
This defines a stochastic process \((Y_n)\) with finite state space \(\mathcal{S}=\{0,1\}\) and discrete time \(n = 1,2,3,\dots\).

The probability of an accident in year \(n+1\) is modelled as a function of the total number of previous accidents over a function of the number of years in the policy; that is,
\[
\mathbb P(Y_{n+1}= 1 \mid Y_n=y_{n},\dots ,Y_2=y_{2},Y_1=y_{1} )=\frac{f(y_1+y_2+\cdots +y_n)}{g(n)},
\]
and \(Y_{n+1} = 0\) otherwise,
where \(f\) and \(g\) are non-negative increasing functions with \(0\leq f(m)\leq g(m)\) for all \(m\). (We'll come back to these conditions in a moment.)

Unfortunately \((Y_n)\) is \emph{not} a Markov chain -- it's clear that \(Y_{n+1}\) depends not only on \(Y_n\), the number accidents this year, but the entire history \(Y_1, Y_2, \dots, Y_n\).

However, we have a cunning work-around. Define \(X_n=\sum_{i=1}^n Y_i\) to be the total number of accidents up to year \(n\). Then \((X_n)\) \emph{is} a Markov chain. In fact, we have
\begin{align*}
    \mathbb P(X_{n+1}={}&{}x_{n}+1\mid X_n=x_n, \dots, X_2=x_2, X_1=x_1)\\
    &=\mathbb P(Y_{n+1}=1\mid Y_n=x_n - x_{n-1}, \dots Y_2=x_2-x_1, Y_1=x_1)\\
    &=\frac{f\big((x_n-x_{n-1}) +\cdots +(x_2-x_1) + x_1\big)}{g(n)}\\
    &=\frac{f(x_n)}{g(n)},
\end{align*}
and \(X_{n+1} = x_n\) otherwise. This clearly depends only on \(x_n\). Thus we can use Markov chain techniques on \((X_n)\) to lean about the non-Markov process \((Y_n)\).

Note that the probability that \(X_{n+1} = x_n\) or \(x_n+ 1\) depends not only on \(x_n\) but also on the time \(n\). So this is a rare example of a time \emph{inhomogeneous} Markov process, where the transition probabilities do depend on the time \(n\).

Before we move on, let's think about the conditions we placed on this model. First, the condition that \(f\) is increasing means that between drivers who have been driving the same number of years, we think the more accident-prone in the past is more likely to have an accident in the future. Second, the condition that \(g\) is increasing means that between drivers who have had the same number of accidents, we think the one who has spread those accidents over a longer period of time is less likely to have accidents in the future. Third, the transition probabilities should lie in the range \([0,1]\); but since \(\sum_{i=1}^m y_i\leq m\), our condition \(0\leq f(m)\leq g(m)\) guaranteed that this is the case.

\hypertarget{S06-example3}{%
\subsection{A no-claims discount model with memory}\label{S06-example3}}

Sometimes, we are presented with a stochastic process which is not a Markov chain, but where by altering the state space \(\mathcal{S}\) we \emph{can} end up with a process which \emph{is} a Markov chain. As such, when making a model, it is important to think carefully about choice of state space. To see this we will return to the no-claims discount example.

Suppose now we have an model with four levels of discount:

\begin{itemize}
\tightlist
\item
  no discount (state 1)
\item
  20\% discount (state 2)
\item
  40\% discount (state 3)
\item
  60\% discount (state 4)
\end{itemize}

If a year is accident free, then the discount increases one level, to a maximum of 60\%. This time, if the year has an accident, then the discount decreases by one level if the year previous to that was accident free, but decreases by \emph{two} levels if the previous year had an accident as well, both to a minimum of no discount.

As before, the insurance company believes that probability that a motorist has a claim-free year is \(\frac34 = 0.75\).

We might consider the most natural choice of a state space, where the states are discount levels; say, \(\mathcal{S}=\{1,2,3,4\}\). But this is not a Markov chain, since if a policy holder has an accident, we may need to know about the past in order to determine probabilities for future states, which violates the Markov property. In particular, if a motorist is in state 3 (40\% discount) and has an accident, they will either move down to level 2 (if they had not crashed the previous year, so had previously been in state 2) or to level 1 (if they had crashed the previous year, so had previously been in state 4) -- but that depends on their previous level too, which the Markov property doesn't allow. (You can check this is the only violation of the Markov property.)

However, we can be clever again, this time in the choice of our state space. Instead, we can split the 40\% level into two different states: state ``3a'' if there was no accident the previous year, and state ``3b'' if there was an accident the previous year. Our states are now:

\begin{itemize}
\tightlist
\item
  no discount (state 1)
\item
  20\% discount (state 2)
\item
  40\% discount, no claim in previous year (state 3a)
\item
  40\% discount, claim in previous year (state 3b)
\item
  60\% discount (state 4)
\end{itemize}

Now this \emph{is} a Markov chain, because the new states 3s carry with them the memory of the previous year, to ensure the Markov property is preserved. Under the assumption of 25\% of drivers having an accident each year, the transition matrix is
\[
\mathsf P=\begin{pmatrix}
0.25 & 0.75 & 0 & 0 & 0\\
0.25 & 0 & 0.75 & 0 & 0\\
0 & 0.25 & 0 & 0 & 0.75\\
0.25 & 0 & 0 & 0 & 0.75\\
0 & 0 & 0 & 0.25 & 0.75\end{pmatrix}.
\]
The transition diagram is shown below. (Recall that we don't draw arrows with probability 0.)

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/example3-1} 

}

\caption{Transition diagram for the no-claims discount model with memory}\label{fig:example3}
\end{figure}

Note that when we move up from state 2, we go to 3a (no accident in the previous year); but when we move down from state 4, we go to 3b (accident in the previous year).

\textbf{In the next section}, we look at how to study big Markov chains by splitting them into smaller pieces called ``classes''.

\hypertarget{P03}{%
\section*{Problem sheet 3}\label{P03}}
\addcontentsline{toc}{section}{Problem sheet 3}

\commtrue

You should attempt all these questions and write up your solutions in advance of your workshop in week 4 (Monday 15 or Tuesday 16 February) where the answers will be discussed.

\textbf{1.} Consider a Markov chain with state space \(\mathcal S = \{1,2,3\}\), and transition matrix partially given by
\[ \mathsf P = \begin{pmatrix} ? & 0.3 & 0.3 \\ 0.2 & 0.4 & ? \\ ? & ? & 1 \end{pmatrix} . \]

\textbf{(a)} Replace the four question marks by the appropriate transition probabilities.

\begin{myanswers}
\emph{Solution.} Rows must add up to 1 and every entry must be non-negative, so the transition matrix is
\[ \mathsf P = \begin{pmatrix} 0.4 & 0.3 & 0.3 \\ 0.2 & 0.4 & 0.4 \\ 0 & 0 & 1 \end{pmatrix} . \]

\end{myanswers}

\textbf{(b)} Draw a transition diagram for this Markov chain.

\begin{myanswers}

\emph{Solution.}

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q31-1} 

}

\caption{Transition diagram for Question 1.}\label{fig:Q31}
\end{figure}

\end{myanswers}

\textbf{(c)} Find the matrix \(\mathsf P(2)\) of two-step transition probabilities.

\begin{myanswers}
\emph{Solution.} \({\displaystyle \mathsf P(2) = \mathsf P^2 = \begin{pmatrix} 0.22 & 0.24 & 0.54 \\ 0.16 & 0.22 & 0.62 \\ 0 & 0 & 1 \end{pmatrix}}\)

\end{myanswers}

\textbf{(d)} By summing the probabilities of all relevant paths, find the three-step transition probability \(p_{13}(3)\).

\begin{myanswers}
\emph{Solution.} There are seven relevant paths: \(1 \to 1 \to 1 \to 3\), \(1 \to 1 \to 2 \to 3\), \(1 \to 1 \to 3 \to 3\), \(1 \to 2 \to 1 \to 3\), \(1 \to 2 \to 2 \to 3\), \(1 \to 2 \to 3 \to 3\), and \(1 \to 3 \to 3 \to 3\). So
\begin{align*}
p_{13}(3) &= p_{11}p_{11}p_{11}p_{13} + p_{11}p_{12}p_{23} + p_{11}p_{13}p_{33}  + p_{12} p_{21} p_{13}\\
& \qquad{}+ p_{12}p_{22}p_{23} + p_{12}p_{23}p_{33} + p_{13}p_{33}p_{33}\\
& = 0.4 \cdot 0.4 \cdot 0.3 + 0.4\cdot 0.3\cdot 0.4 + 0.4\cdot 0.3 \cdot 1 + 0.3 \cdot 0.2 \cdot 0.3 \\
& \qquad{}+ 0.3\cdot 0.4 \cdot 0.4 + 0.3 \cdot 0.4 \cdot 1 + 0.3 \cdot 1 \cdot 1\\
&= 0.702
\end{align*}

\end{myanswers}

\textbf{2.} Consider a Markov chain \((X_n)\) which moves between the vertices of
a tetrahedron.

At each time step, the process randomly chooses one of the edges connected to the current vertex and follows it to a new vertex. The edge to follow is selected randomly with all options having equal probability and each selection is independent of the past movements. Let \(X_n\) be the vertex the process is in after step \(n\).

\textbf{(a)} Write down the transition matrix \(\mathsf P\) of this Markov chain.

\begin{myanswers}
The chain can move from a state to any of the other \(3\) states, each with probability \(1/3\). So
\[ \mathsf P = \begin{pmatrix} 0 & \frac13 & \frac13 & \frac13 \\
                               \frac13 & 0 & \frac13 & \frac13 \\
                               \frac13 & \frac13 & 0 & \frac13 \\
                               \frac13 & \frac13 & \frac13 & 0 \end{pmatrix} . \]

\end{myanswers}

\textbf{(b)} By summing over all relevant paths of length two, calculate the two-step transition probabilities \(p_{11}(2)\) and \(p_{12}(2)\). Hence, write down the two-step transition matrix \(\mathsf P(2)\).

\begin{myanswers}
The length-2 paths from 1 to 1 are \(1 \to k \to 1\) for \(k = 2,3,4\), so
\[ p_{11}(2) = p_{12}p_{21} + p_{13}p_{31} + p_{14}p_{41} = \tfrac13 \tfrac13 +  \tfrac13 \tfrac13 + \tfrac13 \tfrac13 = \tfrac13  .   \]
The length-2 paths from 1 to 2 are \(1 \to 3 \to 2\) and \(1 \to 4 \to 2\), so
\[ p_{12}(2) = p_{13}p_{32} + p_{14}p_{42} = \tfrac13 \tfrac13 + \tfrac13 \tfrac13 = \tfrac29 . \]

By symmetry, \(p_{ii}(2) = p_{11}(2)\) for all \(i\), and \(p_{ij}(2) = p_{12}(2)\) for all \(i \neq j\). Therefore
\[ \mathsf P(2) = \begin{pmatrix} \frac13 & \frac29 & \frac29 & \frac29 \\
                               \frac29 & \frac13 & \frac29 & \frac29 \\
                               \frac29 & \frac29 & \frac13 & \frac29 \\
                               \frac29 & \frac29 & \frac29 & \frac13 \end{pmatrix} . \]

\end{myanswers}

\textbf{(c)} Check your answer by calculating the matrix square \(\mathsf P^2\).

\begin{myanswers}
We can verify that
\[ \mathsf P^2 = \begin{pmatrix} 0 & \frac13 & \frac13 & \frac13 \\
\frac13 & 0 & \frac13 & \frac13 \\
\frac13 & \frac13 & 0 & \frac13 \\
\frac13 & \frac13 & \frac13 & 0 \end{pmatrix} \begin{pmatrix} 0 & \frac13 & \frac13 & \frac13 \\
\frac13 & 0 & \frac13 & \frac13 \\
\frac13 & \frac13 & 0 & \frac13 \\
\frac13 & \frac13 & \frac13 & 0 \end{pmatrix}=
\begin{pmatrix} \frac13 & \frac29 & \frac29 & \frac29 \\
\frac29 & \frac13 & \frac29 & \frac29 \\
\frac29 & \frac29 & \frac13 & \frac29 \\
\frac29 & \frac29 & \frac29 & \frac13 \end{pmatrix} , \]
as above.

\end{myanswers}

\begin{figure}

{\centering \includegraphics[width=850em]{math2750_files/figure-latex/testing-1} 

}

\caption{A tetrahedron}\label{fig:testing}
\end{figure}

\textbf{3.} Consider the two-state ``broken printer'' Markov chain, with state space \(\mathcal S = \{0,1\}\), transition matrix
\[ \mathsf P = \begin{pmatrix} 1-\alpha & \alpha \\
                 \beta & 1-\beta \end{pmatrix} \]
with \(0 < \alpha, \beta < 1\), and initial distribution \(\boldsymbol\lambda = (\lambda_0, \lambda_1)\). Write \(\mu_n =\mathbb P(X_n = 0)\).

\textbf{(a)} By writing \(\mu_{n+1}\) in terms of \(\mu_n\), show that we have
\[ \mu_{n+1} - \big(1-(\alpha+\beta)\big)\mu_n = \beta . \]

\begin{myanswers}
Using the law of total probability, we have
\begin{multline*}
\mathbb P(X_{n+1} = 0) = \mathbb P(X_n = 0)\,\mathbb P(X_{n+1} = 0 \mid X_n = 0) \\
+ \mathbb P(X_n = 1)\,\mathbb P(X_{n+1} = 0 \mid X_n = 1) ,
\end{multline*}
which in terms of \((\mu_n)\) is
\[ \mu_{n+1} = \mu_n (1-\alpha) + (1 - \mu_n)\beta . \]
We used here that \(\mathbb P(X_n = 1) = 1-\mu_n\).
Rearranging this gives the answer.

\end{myanswers}

\textbf{(b)} By solving this linear difference equation using the initial condition \(\mu_0 = \lambda_0\), or otherwise, show that
\[ \mu_n = \frac{\beta}{\alpha+\beta} + \left(\lambda_0 - \frac{\beta}{\alpha+\beta}\right)\big(1-(\alpha+\beta)\big)^n   . \]

\begin{myanswers}
The characteristic equation is \(\lambda - (1-(\alpha+\beta)) = 0\) with a single root at \(\lambda = 1 - (\alpha+\beta)\). The general solution to the homogeneous equation is, therefore, \(A(1-(\alpha+\beta))^n\).

For a particular solution, we guess a solution \(\mu_n = C\), and \(C - (1-(\alpha+\beta))C = \beta\) gives \(C = \beta/(\alpha+\beta)\). Thus the general solution to the inhomogeneous equation is
\[ \mu_n = \frac{\beta}{\alpha+\beta} + A\big(1-(\alpha+\beta)\big)^n .\]

From the initial condition, we get \(\lambda_0 = \beta/(\alpha+\beta) + A\), and therefore \(A = \lambda_0 - \beta/(\alpha+\beta)\). The solution is therefore as given.

\end{myanswers}

\textbf{(c)} What, therefore, are \(\lim_{n\to\infty} \mathbb P(X_n = 0)\) and \(\lim_{n\to\infty} \mathbb P(X_n = 1)\)?

\begin{myanswers}
Note that \(-1 < 1 - (\alpha + \beta) < 1\), so \((1-(\alpha+\beta))^n \to 0\). Therefore we have
\begin{align*}
\lim_{n\to\infty} \mathbb P(X_n = 0) &= \lim_{n\to\infty} \mu_n \\
&= \lim_{n\to\infty} \left( \frac{\beta}{\alpha+\beta} + \left(\lambda_0 - \frac{\beta}{\alpha+\beta}\right)\big(1-(\alpha+\beta)\big)^n \right) \\
&= \frac{\beta}{\alpha+\beta} .
\end{align*}
Since \(\mathbb P(X_n = 1) = 1- \mathbb P(X_n = 0)\), we have
\[ \mathbb P(X_n = 1) \to 1 - \frac{\beta}{\alpha+\beta} = \frac{\alpha}{\alpha+\beta} . \]

\end{myanswers}

\textbf{(d)} Explain what happens if the Markov chain is started in the distribution
\[ \lambda_0 = \frac{\beta}{\alpha+\beta} , \qquad \lambda_1 = \frac{\alpha}{\alpha+\beta}  . \]

\begin{myanswers}
Substituting in the value of \(\lambda_0\) into the equation for \(\mu_n\), the second term cancel, and we have that \(\mathbb P(X_n = 0) = \mu_n = \beta/(\alpha+\beta)\) for all times \(n\), and therefor \(\mathbb P(X_n = 1) = \alpha/(\alpha+\beta)\) too. This means that the Markov chain remains in the same ``stationary distribution'' forever.

\end{myanswers}

\textbf{4.} Let \((X_n)\) be a Markov chain. Show that, for any \(m \geq 1\), we have
\[ \mathbb P(X_{n+m} = x_{n+m} \mid X_n = x_n, X_{n-1} = x_{n-1}, \dots, X_0 = x_0)  =  \mathbb P(X_{n+m} = x_{n+m} \mid X_n = x_n) . \]

\begin{myanswers}
Note that we have a sequence of statements here, for \(m = 1, 2, \dots\). Note also that the case \(m = 1\) is the standard Markov property. When we have a sequence of statements and we can easily prove the first one, this is a good sign that a proof by induction is the way to go.

Before starting, for reasons of space, we adopt notation where we suppress the capital \(X\)s, so we want to show that
\[ \mathbb P(x_{n+m} \mid x_n, x_{n-1}, \dots, x_0 ) = \mathbb P(x_{n+m} \mid x_n) . \]

We work by induction on \(m\). The base case \(m = 1\) is the standard Markov property.

Assume the inductive hypothesis: that result holds for \(m\). We now need to prove the inductive step: that the result holds for \(m+1\). For \(m+1\) we have, by conditioning on the first step \(x_{n+1}\),
\begin{multline*} \mathbb P(x_{n+m+1} \mid x_n, x_{n-1}, \dots, x_0 ) \\
 = \sum_{x_{n+1}} \mathbb P(x_{n+1} \mid x_n, x_{n-1}, \dots, x_0 )\,\mathbb P(x_{n+m+1} \mid x_{n+1}, x_n, x_{n-1}, \dots, x_0 )     \end{multline*}
By the standard Markov property the first term simplifies to \(\mathbb P(x_{n+1} \mid x_n)\), and by the result for \(m\) the second term simplifies to \(\mathbb P(x_{n+m+1} \mid x_{n+1})\). So we have
\[ \mathbb P(x_{n+m+1} \mid x_n, x_{n-1}, \dots, x_0 ) = \sum_{x_{n+1}} \mathbb P(x_{n+1} \mid x_n) \mathbb P(x_{n+m+1} \mid x_{n+1}) . \]
But the right-hand side here is \(\mathbb P(x_{n+m+1} \mid x_n)\) written using conditioning on the first step and using the result for \(m\). By induction, we are done.

\end{myanswers}

\textbf{5.} A car insurance company operates a no-claims discount system for existing policy holders. The possible discounts on premiums are \(\{0\%,25\%,40\%,50\%\}\). Following a claim-free year, a policyholder's discount level increases by one level (or remains at 50\% discount). If the policyholder makes one or more claims in a year, the discount level decreases by one level (or remains at 0\% discount).

The insurer believes that the probability of of making at least one claim in a year is \(0.1\) if the previous year was claim-free and \(0.25\) if the previous year was not claim-free.

\textbf{(a)} Explain why we cannot use \(\{0\%,25\%,40\%,50\%\}\) as the state space of a Markov chain to model discount levels for policyholders.

\begin{myanswers}
The Markov property does not hold for the time-homogeneous process described since the probability of moving to a given state at the next time step is not simply dependent on the current state if \(\mathcal S=\{0\%,25\%,40\%,50\%\}\). For example,
\[
    \mathbb P(X_{n+1}=25\% \mid X_n= 40\% )=\begin{cases} 
    0.25 & \text{if $X_{n-1}=50\%$}\\
    0.1 & \text{if $X_{n-1}=25\%$.} \end{cases} \]

\end{myanswers}

\textbf{(b)} By considering additional states, show that a Markov chain can be used to model the discount level.

\begin{myanswers}
The problem is that the process has a memory of the previous year.
If we currently have a discount of 0\%, we know a claim was made in the year before, so no changes are required. Similarly, at 50\% discount, we know that no claim was made in the previous year. The other two states, 25\% and 40\%, have different behaviour depending on whether or not there was a claim in the previous year.

So we will split each of these into two states: 25+ will denote a 25\% discount with no claim in the previous year, while 25- will denote a 25\% discount with a claim in the previous year. We define the state 40+ and 40- similarly. Then we have a Markov chain, since the current state and the number of claims in the previous year completely defines the distribution on future behaviour.

\end{myanswers}

\textbf{(c)} Draw the transition diagram and write down the transition matrix.

\begin{myanswers}
The transition diagram is as shown below.

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/fortyplus-1} 

}

\caption{Transition diagram for the car insurance Markov chain}\label{fig:fortyplus}
\end{figure}

The transition matrix is given by,
\[ \mathsf P = 
    \begin{matrix}
    & \begin{matrix} 
        0 & 25+ & 25- & 40+ & 40- & 50
      \end{matrix} \\
    \begin{matrix}
      0 \\ 25+ \\ 25- \\ 40+ \\ 40- \\ 50
    \end{matrix}
    &
    \begin{pmatrix}
    0.25 & 0.75 & 0    & 0    & 0   & 0    \\
      0.1  & 0    & 0    & 0.9  & 0   & 0    \\
      0.25 & 0    & 0    & 0.75 & 0   & 0    \\
      0    & 0    & 0.1  & 0    & 0   & 0.9  \\
      0    & 0    & 0.25 & 0    & 0   & 0.75 \\
      0    & 0    & 0    & 0    & 0.1 & 0.9 
    \end{pmatrix}
    \end{matrix}
    \]

\end{myanswers}

\textbf{6.} The credit rating of a company can be modelled as a Markov chain. Assume the rating is assessed once per year at the end of the year and possible ratings are A (good), B (fair) and D (in default). The transition matrix is
\[\mathsf P=\begin{pmatrix} 0.92&0.05&0.03\\
0.05&0.85&0.1\\
0&0&1 \end{pmatrix} . \]

\textbf{(a)} Calculate the two-step transition probabilities, and hence find the expected number of defaults in the next two years from \(100\) companies all rated A at the start of the period.

\begin{myanswers}
The matrix of two-step transition probabilities is given by the matrix square
\[\mathsf P(2) = \mathsf P^2= \begin{pmatrix} 0.8489&0.0885&0.0626\\
0.0885&0.7250&0.1865\\
0&0&1 \end{pmatrix}. \]
The number of defaults in two years from \(100\) A-rated companies is \(100 \times p_{\mathrm{AD}}(2) = 100 \times 0.0626 = 6.26\).

\end{myanswers}

\textbf{(b)} What is the probability that a company rated A will at some point default without ever having been rated B in the meantime?

\begin{myanswers}
Let \(\delta\) be the desired probability that an A-rated company will default without having been rated B. We condition on the first step: with probability \(0.92\) we remain in state A, and by the Markov property the probability of the given event remains at \(\delta\); with probability \(0.05\) we move to state B, and the event fails to occur; and with probability \(0.03\) we move to state D and the event occurs immediately. Therefore, we have
\[ \delta = 0.92\delta + 0.05\times 0 + 0.03 \times 1 = 0.92\delta + 0.03 ,  \]
which has solution \(\delta = 0.03/(1-0.92) = 0.375\).

\end{myanswers}

A corporate bond portfolio manager follows an investment strategy which means that bonds which fall from A-rated to B-rated are sold and replaced with an A-rated bond. The manager believes this will improve the returns on the portfolio because it will reduce the number of defaults experienced.

\textbf{(c)} Calculate the expected number of defaults in the manager's portfolio over the next two years given there are initially 100 A-rated bonds.

\begin{myanswers}
Given that B-rated bonds are replaced by A-rated bonds, we have a new Markov chain with state space \(\{\mathrm{A},\mathrm{D}\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} 0.92+0.05 & 0.03 \\ 0 & 1 \end{pmatrix} =  \begin{pmatrix} 0.97 & 0.03 \\ 0 & 1 \end{pmatrix} .  \]
The two-step transition probability is
\[ \mathsf P(2) = \mathsf P^2 =  \begin{pmatrix} 0.97 & 0.03 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 0.97 & 0.03 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 0.9409 & 0.0591 \\ 0 & 1 \end{pmatrix} .  \]
Thus the number of defaults from \(100\) A-rated bonds in two years is \(100\times p_{\mathrm{AD}}(2) = 100\times 0.0591 = 5.91\). The manager was right: this is slightly less than the \(6.26\) from part (a).

\end{myanswers}

\hypertarget{S07-classes}{%
\section{Class structure}\label{S07-classes}}

\begin{itemize}
\tightlist
\item
  Communicating classes and irreducibility
\item
  Period of a state (and class)
\end{itemize}

\hypertarget{comm-classes}{%
\subsection{Communicating classes}\label{comm-classes}}

If we have a large complicated Markov chain, it can be useful to split the state space up into smaller pieces that can be studied separately. The idea is that states \(i\) and \(j\) should definitely be in the same piece (or "class) if we can get from \(i\) to \(j\) and then back to \(i\) again after some number of steps.

\begin{definition}
\protect\hypertarget{def:comm}{}\label{def:comm}

Consider a Markov chain on a state space \(\mathcal S\) with transition matrix \(\mathsf P\). We say that state \(j\in\mathcal{S}\) is \textbf{accessible} from state \(i\in\mathcal{S}\) and write \(i \to j\) if, for some \(n\), \(p_{ij}(n)>0\).

If \(i \to j\) and \(j \to i\), we say that \(i\) \textbf{communicates with} \(j\) and write \(i \leftrightarrow j\).

\end{definition}

Here, the condition \(p_{ij}(n)>0\) means that, starting from \(i\), there's a positive chance that we'll get to \(j\) at some point in the future -- hence the term ``accessible''.

\begin{theorem}
\protect\hypertarget{thm:equiv-rel}{}\label{thm:equiv-rel}

Consider a Markov chain on a state space \(\mathcal S\) with transition matrix \(\mathsf P\). Then the ``communicates with'' relation \(\leftrightarrow\) is an \href{https://en.wikipedia.org/wiki/Equivalence_relation}{\textbf{equivalence relation}}; that is, it has the following properties:

\begin{itemize}
\tightlist
\item
  \textbf{reflexive}: \(i \leftrightarrow i\) for all \(i\);
\item
  \textbf{symmetric}: if \(i \leftrightarrow j\) then \(j \leftrightarrow i\);
\item
  \textbf{transitive}: if \(i \leftrightarrow j\) and \(j \leftrightarrow k\) then \(i \leftrightarrow k\).
\end{itemize}

\end{theorem}

\begin{proof}

\emph{Reflexivity}: Clearly \(p_{ii}(0) = 1 > 0\), because in ``zero steps'' we stay where we are. So \(i \leftrightarrow i\) for all \(i\).

\emph{Symmetry}: The definition of \(i \leftrightarrow j\) is symmetric under swapping \(i\) and \(j\).

\emph{Transitivity}. If we can get from \(i\) to \(j\) and we can get from \(j\) to \(k\), then we can get from \(i\) to \(k\) by going via \(j\). We just need to write that out formally.

Since \(i \to j\), we have \(p_{ij}(n) > 0\) for some \(n\), and since \(j \to k\), we also have \(p_{jk}(m) > 0\) for some \(m\). Then, by the Chapman--Kolmogorov equations, we have
\[ p_{ik}(n+m) = \sum_{l \in \mathcal S} p_{il}(n) p_{lk}(m) \geq p_{ij}(n) p_{jk}(m) > 0 , \]
from just picking out the \(l=j\) term in the sum. So \(i \to k\) too.

The same argument with \(k\) and \(i\) swapped gives \(k \to i\) also, so \(i \leftrightarrow k\).

\end{proof}

A fact you may remember about equivalence relations is that an equivalence relation, like \(\leftrightarrow\), partitions the space \(\mathcal S\) into \href{https://en.wikipedia.org/wiki/Equivalence_class}{\textbf{equivalence classes}}. This means that each state \(i\) is in exactly one equivalence class, and that class is the set of states \(j\) such that \(i \leftrightarrow j\). In this context, we call these \textbf{communicating classes}.

\begin{example}
\protect\hypertarget{exm:rw-class}{}\label{exm:rw-class}

In the simple random walk, provided \(p\) is not 0 or 1, every state communicates with every other state, because from \(i\) when can get to \(j > i\) by going up \(j - i\) times, and we can get to \(j < i\) by going down \(i - j\) times. Therefore the whole state space \(\mathcal S = \mathbb Z\) is one communicating class.

\end{example}

\begin{example}
\protect\hypertarget{exm:gamblers-class}{}\label{exm:gamblers-class}

Consider the gambler's ruin Markov chain on \(\{0,1,\dots,m\}\). There are three communicating classes. The ruin states \(\{0\}\) and \(\{m\}\) each don't communicate with any other states, so each are a class by themselves. The remaining states \(\{1,2,\dots,m-1\}\) are all in the same class, like the simple random walk.

\end{example}

\begin{example}
\protect\hypertarget{exm:hsd-class}{}\label{exm:hsd-class}

Consider the following simple model for an epidemic. We have three states: healthy (H), sick (S), and dead (D). This transition matrix is
\[ \mathsf P = \begin{pmatrix} p_{\mathrm{HH}} & p_{\mathrm{HS}} & 0 \\
p_{\mathrm{SH}} & p_{\mathrm{SS}} & p_{\mathrm{SD}} \\ 0 & 0 & 1 \end{pmatrix} ,    \]
and the transition diagram is:

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/HSD-1} 

}

\caption{Transition diagram for the healthy--sick--dead chain.}\label{fig:HSD}
\end{figure}

Clearly H and S communicate with each other (you can become infected or recover), while D only communicates with itself (the dead do not recover). Hence, the state space \(\mathcal S = \{\mathrm{H},\mathrm{S},\mathrm{D}\}\) partitions into two communicating classes: \(\{\mathrm{H},\mathrm{S}\}\) and \(\{\mathrm{D}\}\).

\end{example}

A few more definitions that will be important later.

\begin{definition}
\protect\hypertarget{def:irreducible}{}\label{def:irreducible}

If the entire state space \(\mathcal S\) is one communicating class, we say that the Markov chain is \textbf{irreducible}.

We say that a communicating class is \textbf{closed} if no state outside the class is accessible from any state within the class. That is, class \(C \subset \mathcal S\) is closed if whenever there exist \(i \in C\) and \(j \in \mathcal S\) with \(i \to j\), then \(j \in C\) also. If a class is not closed, we say it is \textbf{open}.

If a state \(i\) is in a communicating class \(\{i\}\) by itself and that class is closed, then we say state \(i\) is \textbf{absorbing}.

\end{definition}

In non-maths language:

\begin{itemize}
\tightlist
\item
  An irreducible Markov chain can't be broken down into smaller pieces.
\item
  Once you enter a closed class, you can't leave that class.
\item
  Once you reach an absorbing state, you can't leave that state.
\end{itemize}

How do these work for our earlier examples?

\begin{example}
\protect\hypertarget{exm:ex-irred}{}\label{exm:ex-irred}

Going back to the previous examples:

\begin{itemize}
\tightlist
\item
  In the simple random walk, the whole state space is one communicating class which must therefore be closed. The Markov chain has only one class, so is irreducible.
\item
  In the gambler's ruin, classes \(\{0\}\) and \(\{m\}\) are closed, because the Markov chain stays there forever, and because these closed classes consist of only one state each, \(0\) and \(m\) are absorbing states. The class \(\{1, 2, \dots, m-1\}\) is open, as we can escape the class by going to \(0\) or \(m\). The gambler's ruin chain has multiple classes, so is not irreducible.
\item
  In the ``healthy--sick--dead'' chain, the class \(\{D\}\) is closed, so D is an absorbing state, while the class \(\{H, S\}\) is open, as one can leave it by dying. The Markov chain is not irreducible.
\end{itemize}

\end{example}

\hypertarget{periodicity}{%
\subsection{Periodicity}\label{periodicity}}

When \protect\hyperlink{S02-exact-distribution}{we discussed the simple random walk, we noted} that it alternates between even-numbered and odd-numbered states. This ``periodic'' behaviour is important to understand if we want to know which state we will be in at some point in the future.

The idea is this: List the number of steps for all possible paths starting and ending in the state. Then the period is the \href{https://en.wikipedia.org/wiki/Greatest_common_divisor}{greatest common divisor} (or ``highest common factor'') of the integers in this list.

\begin{definition}
\protect\hypertarget{def:period}{}\label{def:period}

Consider a Markov chain with transition matrix \(\mathsf P\). We say that a state \(i\in\mathcal{S}\) has \textbf{period} \(d_i\), where
\[ d_i=\text{gcd}\big\{n\in\{1,2,\dots,\} : p_{ii}(n) > 0\big\} , \]
where gcd denotes the greatest common divisor.

If \(d_i>1\), then the state \(i\) is called \textbf{periodic}; if \(d_i = 1\), then \(i\) is called \textbf{aperiodic}.

\end{definition}

\begin{example}
\protect\hypertarget{exm:rw-period}{}\label{exm:rw-period}

Consider the simple random walk with \(p \neq 0,1\). We have \(p_{ii}(n) = 0\) for odd \(n\), since we swap from odd to even each step. But \(p_{ii}(2) = 2pq > 0\). Therefore, all states are periodic with period \(\text{gcd}\{2,4,6,\dots\} = 2\).

\end{example}

\begin{example}
\protect\hypertarget{exm:gamblers-period}{}\label{exm:gamblers-period}

For the gambler's ruin, states \(0\) and \(m\) are aperiodic (have period \(1\)), since they are absorbing states. The remaining states states \(1,2,\dots,m-1\) are periodic with period \(2\), because we swap between odd and even states, as in the simple random walk.

\end{example}

\begin{example}
\protect\hypertarget{exm:weird-period}{}\label{exm:weird-period}

Consider the Markov chain with transition diagram as shown:

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/periods-1} 

}

\caption{Transition diagram for an aperiodic irreducible Markov chain.}\label{fig:periods}
\end{figure}

Importantly, we can't return from the triangle side back to the circle side. We thus see there are two communicating classes: \(\{1,2,3,4\}\), which is open, and \(\{5,6,7\}\), which is closed. The Markov chain is not irreducible, and there are no absorbing states.

The circle side swaps between odd and even states (until exiting from \(4\) to \(5\)), so states \(1\),\(2\), \(3\) and \(4\) all have period \(2\). The triangle side cycles around with certainty, meaning that states \(5\), \(6\), and \(7\) all have period \(3\).

\end{example}

You may have noticed in these examples that, within a communicating class, every state has the same period. In fact, it's always the case that states in the same class have the same period.

\begin{theorem}
\protect\hypertarget{thm:class-period}{}\label{thm:class-period}

All states in a communicating class have the same period.

Formally: Consider a Markov chain on a state space \(\mathcal S\) with transition matrix \(\mathsf P\). If \(i,j\in\mathcal S\) are such that \(i \leftrightarrow j\), then \(d_i = d_j\).

\end{theorem}

In particular, in an irreducible Markov chain, all states have the same period \(d\). We say that an irreducible Markov chain is \textbf{periodic} if \(d>1\) and \textbf{aperiodic} if \(d=1\).

\begin{proof}

Let \(i,j\) be such that \(i \leftrightarrow j\). We want to show that \(d_i = d_j\). First we'll show that \(d_i \leq d_j\), and then we'll show that \(d_j \leq d_i\), and thus conclude that they're equal.

Since \(i\leftrightarrow j\), there exist \(n,m\) such that \(p_{ij}(n)>0\) and \(p_{ji}(m)>0\). Then, by the Chapman--Kolmogorov equations,
\[ p_{ii}(n+m) =  \sum_{k \in \mathcal S} p_{ik}(n) p_{ki}(m) \geq p_{ij}(n) p_{ji}(m) > 0 .  \]
So \(d_i\) divides \(n+m\).

Let \(r\) be such that \(p_{jj}(r)>0\). Then, by the same Chapman--Kolmogorov argument,
\[
    p_{ii}(n+m+r)\geq  p_{ij}(n) p_{jj}(r) p_{ji}(m) > 0,
    \]
because we can get from \(i\) to \(i\) by going \(i \to j \to j \to i\).
Hence \(d_i\) divides \(n+m+r\).

But if \(d_i\) divides both \(n+m\) and \(n+m+r\), it must be that \(d_i\) divides \(r\) also. So whenever \(p_{jj}(r)>0\), we have that \(d_i\) divides \(r\). Since \(d_i\) is a common divisor of all the \(r\)s with \(p_{jj}(r)>0\), it can't be any bigger that the \emph{greatest} common divisor of all those \(r\)s. But that greatest common divisor is by definition \(d_j\), the period of \(j\). So \(d_i \leq d_j\).

Repeating the same argument but with \(i\) and \(j\) swapped over, we get \(d_j\leq d_i\) too, and we're done.

\end{proof}

\textbf{In the next section}, we look at two problems to do with ``hitting times'': What is the probability we reach a certain state, and how long on average does it take us to get there?

\hypertarget{S08-hitting-times}{%
\section{Hitting times}\label{S08-hitting-times}}

\begin{itemize}
\tightlist
\item
  Definitions: Hitting probability, expected hitting time, return probability, expected return time
\item
  Finding these by conditioning on the first step
\item
  Return probability and expected return time for the simple random walk
\end{itemize}

\hypertarget{hitting-definitions}{%
\subsection{Hitting probabilities and expected hitting times}\label{hitting-definitions}}

In \protect\hyperlink{S03-gamblers-ruin}{Section 3} and \protect\hyperlink{S04-ldes}{Section 4}, we used conditioning on the first step to find the ruin probability and expected duration for the gambler's ruin problem. Here, we develop those ideas for general Markov chains.

\begin{definition}
\protect\hypertarget{def:hitting-defs}{}\label{def:hitting-defs}

Let \((X_n)\) be a Markov chain on state space \(\mathcal S\). Let \(H_{A}\) be a random variable representing the \textbf{hitting time} to hit the set \(A \subset \mathcal S\), given by
\[ H_{A} = \min \big\{n \in \{0,1,2,\dots\} : X_n \in A  \big\}  . \]
The most common case we'll be interested in will be when \(A = \{j\}\) is just a single state, so
\[ H_{j} = \min \big\{n \in \{0,1,2,\dots\} : X_n = j  \big\}  . \]
(We use the convention that \(H_A = \infty\) if \(X_n \not\in A\) for all \(n\).)

The \textbf{hitting probability} \(h_{iA}\) of the set \(A\) starting from state \(i\) is
\[ h_{iA} = \mathbb P(X_n \in A \text{ for some $n \geq 0$} \mid X_0 = i) = \mathbb P(H_A < \infty \mid X_0 = i) .  \]
Again, the most common case of interest is \(h_{ij}\) the hitting probability of state \(j\) starting from state \(i\).

The \textbf{expected hitting time} \(\eta_{iA}\) of the set \(A\) starting from state \(i\) is
\[ \eta_{iA} = \mathbb E(H_A \mid X_0 = i) .  \]
Clearly \(\eta_{iA}\) can only be finite if \(h_{iA} = 1\).

\end{definition}

The short summary of this is:

\begin{itemize}
\tightlist
\item
  The \textbf{hitting probability} \(h_{ij}\) is the probability we hit state \(j\) starting from state \(i\).
\item
  The \textbf{expected hitting time} \(\eta_{ij}\) is the expected time until we hit state \(j\) starting from state \(i\).
\end{itemize}

The good news us that we already know how to find hitting probabilities and expected hitting times, because we already did it for the \protect\hyperlink{S03-gamblers-ruin}{gambler's ruin problem}! The way we did it then is that we first found equations for hitting probabilities or expected hitting times by conditioning on the first step, and then we solved those equations. We do the same here for other Markov chains.

Let's see an example of how to find a hitting probability.

\begin{example}
\protect\hypertarget{exm:hitting1}{}\label{exm:hitting1}

\emph{Consider a Markov chain with transition matrix}
\[ \begin{pmatrix}
\frac15 & \frac15 & \frac15 & \frac25 \\
0 & 1 & 0 & 0 \\
0 & \frac12 & 0 & \frac12 \\
0 & 0 & 0 & 1
\end{pmatrix} . \]
\emph{Calculate the probability that the chain is absorbed at state \(2\) when started from state \(1\).}

This is asking for the hitting probability \(h_{12}\). We use -- as ever -- a conditioning on the first step argument. Specifically, we have
\begin{align}
  h_{12} &= p_{11} h_{12} + p_{12} h_{22} + p_{13} h_{32} + p_{14} h_{42}  \\
         &= \tfrac 15 h_{12} + \tfrac 15 h_{22} + \tfrac 15 h_{32} + \tfrac 25 h_{42}  \label{eq:hiteq}
\end{align}
This is because, starting from 1, there's a \(p_{11} = \frac15\) probability of staying at 1; then by the Markov property it's like we're starting again from 1, so the hitting probability is still \(h_{12}\). Similarly, there's a \(p_{12} = \frac15\) probability of moving 2; then by the Markov property it's like we're starting again from 2, so the hitting probability is still \(h_{22}\). And so on.

Since \eqref{eq:hiteq} includes not just \(h_{12}\) but also \(h_{22}\), \(h_{32}\) and \(h_{42}\), we need to find equations for those too. Clearly we have \(h_{22} = 1\), as we are ``already there''. Also, \(h_{42} = 0\), since 4 is an absorbing state. For \(h_{32}\), another ``condition on the first step'' argument gives
\[ h_{32} = p_{32}h_{22} + p_{34}h_{42} = \tfrac12 h_{22} + \tfrac12 h_{42} = \tfrac12 ,\]
where we substituted in \(h_{22} = 1\) and \(h_{42} = 0\).

Substituting \(h_{22} = 1\), \(h_{32} = \frac12\) and \(h_{42} = 0\) all into \eqref{eq:hiteq}, we get
\[ h_{12} = \tfrac15 h_{12} + \tfrac15 + \tfrac15 \tfrac12 = \tfrac15 h_{12} + \tfrac{3}{10}. \]
Hence \(\frac45 h_{12} = \frac{3}{10}\), so the answer we were after is \(h_{12} = \frac38\).

\end{example}

So the technique to find hitting probability \(h_{ij}\) from \(i\) to \(j\) is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set up equations for all the hitting probabilities \(h_{kj}\) to \(j\) by conditioning on the first step.
\item
  Solve the resulting simultaneous equations.
\end{enumerate}

It is recommended to derive equations for hitting probabilities from first principles by conditioning on the first step, as we did in the example above. However, we can state what the general formula is: by the same conditioning method, we get
\[ h_{iA} = \begin{cases} \displaystyle\sum_{j \in \mathcal S} p_{ij} h_{jA} & \text{if $i \not\in A$} \\
1 & \text{if $i \in A$.} \end{cases} \]
It can be shown that, if these equations have multiple solutions, then the hitting probabilities are in fact the smallest non-negative solutions.

Now an example for expected hitting times.

\begin{example}
\protect\hypertarget{exm:hitting2}{}\label{exm:hitting2}

\emph{Consider \protect\hyperlink{S06-example1}{the simple no-claims discount chain from Lecture 6}, which had transition matrix}
\[ \mathsf P =\begin{pmatrix}
    \tfrac14 &\tfrac34 & 0\\
    \tfrac14 &0 & \tfrac34\\
    0 &\tfrac14 & \tfrac34\\
    \end{pmatrix} .\]
\emph{Given we start in state 1 (no discount), find the expected amount of time until we reach state 3 (50\% discount).}

This question asks us to find \(\eta_{13}\). We follow the general method, and start by writing down equations for all the \(\eta_{i3}\)s.

Clearly we have \(\eta_{33} = 0\).
For the others, we condition on the first step to get
\begin{align*}
\eta_{13} = 1 + \tfrac14 \eta_{13} + \tfrac34 \eta_{23} \qquad &\Rightarrow \qquad \phantom{-}\tfrac34 \eta_{13} - \tfrac34\eta_{23} = 1 ,\\
\eta_{23} = 1 + \tfrac14 \eta_{13} + \tfrac34 \eta_{33} \qquad &\Rightarrow \qquad  - \tfrac14\eta_{13} + \phantom{\tfrac34}\eta_{23} = 1 .
\end{align*}
This is because the first step takes time \(1\), then we condition on what happens next -- just like we did for the gambler's ruin.

The first equation plus three-quarters times the second gives
\[ \big(\tfrac34 - \tfrac34\tfrac14\big) \eta_{13} = \tfrac{9}{16}\eta_{13} = 1 + \tfrac34 = \tfrac 74 = \tfrac{28}{16} ,\]
which has solution \(\eta_{13} = \tfrac{28}{9} = 3.11\).

\end{example}

Similarly, if we need to, we can give a general formula
\[ \eta_{iA} = \begin{cases} 1 + \displaystyle\sum_{j \in \mathcal S} p_{ij} \eta_{jA} & \text{if $i \not\in A$} \\
0 & \text{if $i \in A$.} \end{cases} \]
Again, if we have multiple solutions, the expected hitting times are the smallest non-negative solutions.

\hypertarget{return-times}{%
\subsection{Return times}\label{return-times}}

Under the definitions above, the hitting probability and expected hitting time to a state from itself are always \(h_{ii} = 1\) and \(\eta_{ii} = 0\), as we're ``already there''.

In this case, it can be interesting to look instead at the random variable representing the \textbf{return time},
\[ M_i = \min \big\{n \in \{1,2,\dots\} : X_n = i  \big\} . \]
Note that this only considers times \(n = 1, 2, \dots\) not including \(n = 0\), so is the \emph{next} time we come back, after \(n = 0\).

We then have the \textbf{return probability} and \textbf{expected return time}
\begin{gather*} m_{i} = \mathbb P(X_n = i  \text{ for some $n \geq 1$} \mid X_0 = i) = \mathbb P(M_i < \infty \mid X_0 = i) ,  \\
 \mu_{i} = \mathbb E(M_i \mid X_0 = i) .  \end{gather*}

Just as before, we can find these by conditioning on the first step. The general equations are
\[
      m_i = \sum_{j \in \mathcal S} p_{ij}h_{ji} , \qquad
      \mu_i = 1 + \sum_{j \in \mathcal S} p_{ij}\eta_{ji},
  \]
where, if necessary, we take the minimal non-negative solution again.

\hypertarget{return-rw}{%
\subsection{Hitting and return times for the simple random walk}\label{return-rw}}

We now turn to hitting and return times for the simple random walk, which goes up with probability \(p\) and down with probability \(q = 1-p\). This material is mandatory and is examinable, but is a bit technical; students who are struggling or have fallen behind might make a tactical decision just to read the two summary theorems below and come back to the details at a later date.

Let's start with hitting probabilities. Without loss of generality, we look at \(h_{i0}\), the probability the random walk hits 0 starting from \(i\).
We will assume that \(i > 0\). For \(i < 0\), we can get the desired result by looking at the random walk ``in the mirror'' -- that is, by swapping the role of \(p\) and \(q\), and treating the positive value \(-i\).

For an initial condition, it's clear we have \(h_{00} = 1\).

For general \(i > 0\), we condition on the first step, to get
\[ h_{i0} = ph_{i+1\, 0} + qh_{i-1\, 0} .  \]
We recognise this equation from the gambler's ruin problem, and recall that we have to treat the cases \(p \neq \frac12\) and \(p = \frac12\) separately.

When \(p \neq \frac12\), the general solution is \(h_{i0} = A + B\rho^i\), where \(\rho = q/p \neq 1\), as before. The initial condition \(h_{00} = 1\) gives \(A = 1-B\), so we have a family of solutions
\[ h_{i0} = (1 - B) + B\rho^i = 1 + B(\rho^i - 1) . \]

In the gambler's ruin problem we had another boundary condition to find \(B\). Here we have no other conditions, but we can use the minimality condition that the hitting probabilities are the smallest non-negative solution to the equation. This minimal non-negative solution will depend on whether \(\rho > 1\) or \(\rho < 1\).

When \(\rho > 1\), so \(p < \frac12\), the term \(\rho^i\) tends to infinity. Thus the minimal non-negative solution will have to take \(B = 0\), because taking \(B < 0\) would eventually give a negative solution, while \(B = 0\) gives the smallest of the non-negative solutions. Hence the solution is \(h_{i0} = 1 + 0(\rho^i-1) = 1\), meaning we hit 0 with certainty. This makes sense, because for \(p < \frac12\) the random walk ``drifts'' to the left, and eventually gets back to 0.

When \(\rho < 1\), so \(p > \frac12\), we have that \(\rho^i - 1\) is negative and tends to \(-1\). So to get a small solution we want \(B\) large and positive, but keeping the solution non-negative limits us to \(B \leq 1\), so the minimality condition is achieved at \(B = 1\). The solution is \(h_{i0} = 1 + 1(\rho^i - 1) = \rho^i\). This is strictly less than 1 as expected, because for \(p > 1/2\), the random walk drifts to the right, so might drift further and further away from 0 and not come back.

For \(p = \frac12\), so \(\rho = 1\), we recall the general solution \(h_{i0} = A + Bi\). The condition \(h_{00} = 1\) gives \(A = 1\), so \(h_{i0} = 1 + Bi\). Because \(i\) is positive, to get the answer to be small we want \(B\) to be small, but non-negativity limits us to \(B\) positive, so the minimal non-negative solution takes \(B = 0\). This gives \(h_{i0} = 1 + 0i = 1\), so we will always get back to 0.

In conclusion, we have the following. (Recall that we get the result for \(i < 0\) by swapping the role of \(p\) and \(q\), and treating the positive value \(-i\).)

\begin{theorem}
\protect\hypertarget{thm:rw-hitting}{}\label{thm:rw-hitting}

Consider a random walk with up probability \(p \neq 0, 1\). Then the hitting probabilities to 0 are givin by, for \(i > 0\),
\[ h_{i0} = \begin{cases} \left(\displaystyle\frac{q}{p}\right)^i < 1 & \text{if $p > \frac12$} \\ \ 1 & \text{if $p \leq \frac12$,} \end{cases} \]
and for \(i < 0\) by
\[ h_{i0} = \begin{cases} \left(\displaystyle\frac{p}{q}\right)^{-i} < 1 & \text{if $p < \frac12$} \\ \ 1 & \text{if $p \geq \frac12$.} \end{cases} \]

\end{theorem}

Now we look at return times. What is the return probability to 0 (or, by symmetry, to any state \(i \in \mathbb Z\))? By conditioning on the first step, \(m_0 = ph_{1\,0} + qh_{-1\,0}\), and we've calculated those hitting times above. We have
\[ m_0 = \begin{cases} p\times 1 \hspace{0.4em}+ q\times 1 \hspace{0.4em}= 1 & \text{if $p = \frac12$} \\
p \times \displaystyle\frac qp + q\times 1 \hspace{0.4em}= 2q < 1 & \text{if $p > \frac12$} \\
p\times 1 \hspace{0.4em} + q \times \displaystyle\frac pq = 2p < 1 & \text{if $p < \frac12$.}\end{cases} \]

So for the simple symmetric random walk (\(p = \frac12\)) we have \(m_i = 1\) and are certain to return to the initial state again and again, while for \(p \neq \frac12\), we have \(m_i < 1\) and we might never return.

What about the expected return times \(\mu_{0}\)? For \(p \neq \frac12\), we have \(\mu_{0} = \infty\), since \(m_0 < 1\) and we might never come back. For \(p = \frac12\), though, \(m_0 = 1\), so we have work to do.

To get the expected return time for \(p = \frac12\), we'll need the expected hitting times for for \(p = \frac12\) too. Conditioning on the first step gives the equation
\[ \eta_{i0} = 1 + \tfrac12\eta_{i+1\,0} + \tfrac12\eta_{i-1\,0} , \]
with initial condition \(\eta_{00} = 0\). We again recognise from the gambler's ruin problem the general solution \(\eta_{i0} = A + Bi -i^2\). The initial condition gives \(A = 0\), so \(\eta_{i0} = Bi - i^2 = i(B-i)\). Then non-negativity demands \(B = \infty\), as any other value would get drowned out by the large negative value \(-i^2\), making the whole expression negative for big enough \(i\). Thus we have \(\eta_{i0} = \infty\).

Then we see easily that the return time is
\[ \mu_0 = 1 + \tfrac12 \eta_{1\,0} + \tfrac12 \eta_{-1\,0} = 1 + \tfrac12 \infty + \tfrac12 \infty = \infty. \]
So for \(p = \frac12\), while the random walk always return, it may take a very long time to do so.

In conclusion, this is what we found out:

\begin{theorem}
\protect\hypertarget{thm:rw-summary}{}\label{thm:rw-summary}

Consider the simple random walk with \(p \neq 0,1\). Then for all states \(i\) we have the following:

\begin{itemize}
\tightlist
\item
  For \(p \neq \frac12\), the return probability \(m_i\) is strictly less than 1, so the expected return time \(\mu_i\) is infinite.
\item
  For the simple symmetric random walk with \(p = \frac12\), the return probability \(m_i\) is equal to 1, but the expected return time \(\mu_i\) is still infinite.
\end{itemize}

\end{theorem}

\textbf{In the next section}, we look at how the return probabilities and expected return times characterise which states continue to be visited by a Markov chain in the long run.

\hypertarget{P04}{%
\section*{Problem sheet 4}\label{P04}}
\addcontentsline{toc}{section}{Problem sheet 4}

\commtrue

You should attempt all these questions and write up your solutions in advance of your workshop in week 5 (Monday 22 or Tuesday 23 February) where the answers will be discussed.

\textbf{1.} Consider the Markov chain with state space \(\mathcal S = \{1,2,3,4\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} 1 & 0 & 0 & 0 \\
1-\alpha & 0 & \alpha & 0 \\
0 & \beta & 0 & 1-\beta \\
0 & 0 & 0 & 1 \end{pmatrix}   \]
where \(0 < \alpha, \beta < 1\).

\textbf{(a)} Draw a transition diagram for this Markov chain.

\begin{myanswers}

\emph{Solution.}

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q41-1} 

}

\caption{Transition diagram for Question 1.}\label{fig:Q41}
\end{figure}

\end{myanswers}

\textbf{(b)} What are the communicating classes for this Markov chain? Is the chain irreducible? Which classes are closed? Which states are absorbing?

\begin{myanswers}
\emph{Solution.} Clearly \(\{1\}\) and \(\{4\}\) are closed communicating classes, so \(1\) and \(4\) are absorbing states. The other class, \(\{2,3\}\) is not closed. Because there are multiple classes, the chain is not irreducible.

\end{myanswers}

\textbf{(c)} Find the hitting probability \(h_{21}\) that, starting from state 2, the chain hits state 1.

\begin{myanswers}
\emph{Solution.} It's clear that \(h_{11} = 1\) and \(h_{41} = 0\). Then, by conditioning on the first step, we have
\begin{gather*}
    h_{21} = \alpha h_{31} + (1-\alpha) h_{11} = \alpha h_{31} + 1 - \alpha \\
    h_{31} = \beta h_{21} + (1-\beta) h_{41} = \beta h_{21} .
    \end{gather*}
Substituting the second equation into the first, we get \(h_{21} = \alpha\beta h_{21} + 1 - \alpha\), so
\[ h_{21} = \frac{1-\alpha}{1-\alpha\beta}  . \]

\end{myanswers}

\textbf{(d)} What is the expected time, starting from state 2, to reach an absorbing state?

\begin{myanswers}
\emph{Solution.} Let's write \(A = \{1,4\}\) for the absorbing states, and \(\eta_{iA}\) for the time to reach an absorbing state starting from state \(i\). Clearly \(\eta_{1A} = \eta_{4A} = 0\). By conditioning on the first step, we have
\begin{gather*}
    \eta_{2A} = 1 + \alpha \eta_{3A} + (1-\alpha) \eta_{1A} = 1 + \alpha \eta_{3A}  \\
    \eta_{3A} = 1 + \beta \eta_{2A} + (1-\beta) \eta_{4A} = 1+\beta \eta_{2A} .
    \end{gather*}
Substituting the second equation into the first gives
\(\eta_{2A}= 1 + \alpha + \alpha\beta \eta_{2A}\), so
\[ \eta_{2A} = \frac{1+\alpha}{1-\alpha\beta}  . \]

\end{myanswers}

\textbf{2.}
Consider the Markov chain with state space \(\mathcal S = \{1,2,3,4,5\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} 0 & \frac13 & \frac23 & 0 & 0 \\
                0 & 0 & 0 & \frac14 & \frac34 \\
                0 & 0 & 0 & \frac14 & \frac34 \\
                1 & 0 & 0 & 0 & 0 \\
                1 & 0 & 0 & 0 & 0 \end{pmatrix} . \]

\textbf{(a)} Draw a transition diagram for this Markov chain.

\begin{myanswers}

\emph{Solution.}

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q42-1} 

}

\caption{Transition diagram for Question 2.}\label{fig:Q42}
\end{figure}

\end{myanswers}

\textbf{(b)} Show that the chain is irreducible.

\begin{myanswers}
\emph{Solution.} We have paths \(1 \to 2 \to 4 \to 1\) and \(1 \to 3 \to 5 \to 1\), so every state communicates with state 1, and by transitivity every state communicates with every other state.

\end{myanswers}

\textbf{(c)} What are the periods of the states?

\begin{myanswers}
\emph{Solution.} Any path from \(1\) to \(1\) goes \(1 \to \{2 \text{ or }3\} \to \{4 \text{ or }5\} \to 1\). So \(p_{11}^{(n)} > 0\) if and only if \(n\) is a multiple of \(3\). So state \(1\) has period \(d_1 = 3\). Because the chain is irreducible, all other states have period \(3\) too.

\end{myanswers}

\textbf{(d)} Find the expected hitting times \(\eta_{i1}\) from each state \(i\) to 1, and the expected return time \(\mu_i\) to 1.

\begin{myanswers}
\emph{Solution.} We could do this through a traditional conditioning on the first step argument. But in fact, the cyclic structure \(1 \to \{2 \text{ or }3\} \to \{4 \text{ or }5\} \to 1\) makes it immediately clear that \(\eta_11 = 0\), \(\eta_{41}=\eta_{51} = 1\), \(\eta_{21} = \eta_{31} = 2\), and \(\mu_1 = 3\).

\end{myanswers}

\textbf{3.}

\textbf{(a)} Show that every Markov chain on a finite state space \(\mathcal S\) has at least one closed communicating class.

\begin{myanswers}
\emph{Solution.} For communicating classes \(C,D\), let's write \(C \to D\) if there is an \(i \in C\) and \(j \in D\) with \(i \to j\). Note that we can't have both \(C \to D\) and \(D \to C\) if \(C\) and \(D\) are distinct classes. This is because there would be \(i_1, j_2 \in C\) and \(j_1, i_2 \in D\) such that \(i_1 \to j_1 \to i_2 \to j_2 \to i_1\), so they would be the same class. Let's also note that there are a finite number of classes \(m\).

Pick a class \(C_1\). If \(C_1\) is closed, we are done; otherwise \(C_1 \to C_2\) for some other class \(C_2\). If \(C_2\) is closed, we are done; otherwise \(C_2 \to C_3\) for some class \(C_3\) different to \(C_1\) and \(C_2\). (It can't be \(C_1\) by the argument above.) We repeat: if \(C_k\) is closed, we are done; otherwise there's a new class \(C_{k+1}\) with \(C_k \to C_{k+1}\). We eventually find a closed class: we either terminate before step \(m\) at a closed class, or otherwise \(C_m\) must be closed, as none of the previous \(m-1\) classes can be accessible from it, by our earlier argument.

\end{myanswers}

\textbf{(b)} Give an example of a Markov chain that has no closed communicating classes.

\begin{myanswers}
\emph{Solution.} By part (a), the state space must be infinite. Here's one example: take \(\mathcal S = \mathbb Z\), and \(X_{n+1} = X_n + 1\) with probability 1, so the Markov chain just marches up and up. There are no states \(i\) and \(j\) such that \(i \leftrightarrow j\) (except for states communicating with themselves) so each state is a separate class. But clearly there is no absorbing state.

\end{myanswers}

\textbf{4.} Prove or give a counterexample: The period of a state \(i\) is the smallest \(d > 0\) such that \(p_{ii}(d) > 0\).

\begin{myanswers}
\emph{Solution.} The statement is not true. Here's one counterexample:

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/counterexample-1} 

}

\caption{Transition diagram for a counterexample for Question 4.}\label{fig:counterexample}
\end{figure}

We see that \(p_{00} = p_{00}(1) = 0\), but \(p_{00}(2) = \frac12 > 0\) (going via A) and \(p_{00}(3) = \frac12 > 0\) (going via B and C). Hence \(d_0 \leq \text{gcd}\{2,3\} = 1\), so \(d_0 = 1\), contradicting the statement in the question.

\end{myanswers}

\textbf{5.} Consider the simple random walk with \(p < \frac12\), and let \(i > 0\). Show that
\(\eta_{i0} = i/(q-p)\).

\begin{myanswers}
\emph{Solution.} By conditioning on the first step, we have
\[ \eta_{i0} = 1 + p\eta_{i+1\,0} + q\eta_{i-1\,0} . \]
Either by solving this linear difference equation directly or by remembering the solution from when we did expected duration of the gambler's ruin, the general solution is
\[ \eta_{i0} = A + B\rho^i + \frac{i}{q-p} , \]
where \(\rho = q/p > 0\).

We have one initial condition \(\eta_{00} = 0\) (since we're ``already there''). This gives \(0 = A + B\), so we have
\[ \eta_{i0} = -B + B\rho^i + \frac{i}{q-p} = B(\rho^i - 1) + \frac{i}{q-p} . \]

We now have to use the principle of the minimum non-negative solution. Since \(\rho > 1\), we also have \(\rho^i - 1 > 0\). Hence \(B\) must be non-negative, to ensure the whole solution is non-negative, but we want \(B\) to be as small as possible, to ensure minimality. Hence we must have \(B = 0\), finally giving the solution
\[ \eta_{i0} = \frac{i}{q-p} \]
as desired.

\end{myanswers}

\textbf{6.} Consider the Markov chain with the following transition matrix and transition diagram:

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q45-1} 

}

\caption{Transition diagram for Question 6.}\label{fig:Q45}
\end{figure}

\[ \mathsf P = \begin{pmatrix} 0 & \frac14 & \frac12 & \frac14 \\
               \frac12 & 0& 0& \frac12 \\
               \frac12 & 0& 0& \frac12 \\
               0 & \frac13 & \frac23 & 0 \end{pmatrix} \]

The Markov chain starts from state 1.

\textbf{(a)} What is the probability that we hit state 2 before we hit state 3?

\begin{myanswers}
\emph{Solution.} Write \(k_i\) for the probability we hit state 2 before state 3 starting from \(i\). Then clearly \(k_2 = 1\) (since we've hit 2 first) and \(k_3 = 0\) (since we've hit 3 first). Further, by conditioning on the first step, \(k_4 = \frac13 k_2 + \frac23 k_3 = \frac13\), and
\[ k_1 = \tfrac14 k_2 + \tfrac12k_3 + \tfrac14 k_4   = \tfrac14 + \tfrac14 k_4 = \tfrac14 + \tfrac14\tfrac13 = \tfrac13 . \]
The desired solution is \(k_1 = \frac13\).

\end{myanswers}

\textbf{(b)} What is the expected time until we hit a state in the set \(\{2,3\}\)?

\begin{myanswers}
\emph{Solution.} Let \(A = \{2,3\}\), and \(\eta_{iA}\) be the expected time until hitting the first of states \(2\) and \(3\) starting from \(i\). Clearly \(\eta_{2A} = \eta_{3A} = 0\). By conditioning on the first step, \(\eta_{4A} = 1+ \frac13 \eta_{2A} + \frac23 \eta_{3A} = 1\), and
\[ \eta_{1A} = 1 + \tfrac14 \eta_{2A} + \tfrac12\eta_{3A} + \tfrac14 \eta_{4A} = 1 + \tfrac14 = \tfrac54 .        \]
The desired solution is \(\eta_{1A} = \tfrac54\).

\end{myanswers}

\hypertarget{S09-recurrence-transience}{%
\section{Recurrence and transience}\label{S09-recurrence-transience}}

\begin{itemize}
\tightlist
\item
  Definition of properties of recurrence and transience
\item
  Recurrence and transience as class properties
\item
  Positive and null recurrence
\end{itemize}

\hypertarget{rec-trans-def}{%
\subsection{Recurrent and transient states}\label{rec-trans-def}}

When thinking about the long-run behaviour of Markov chains, it's useful to classify two different types of states: ``recurrent'' states and ``transient'' states.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.50}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.50}}@{}}
\toprule
Recurrent states & Transient states \\
\midrule
\endhead
If we ever visit \(i\), then we keep returning to \(i\) again and again & We might visit \(i\) a few times, but eventually we leave \(i\) and never come back \\
Starting from \(i\), the expected number of visits to \(i\) is infinite & Starting from \(i\), the expected number of visits to \(i\) is finite \\
Starting from \(i\), the number of visits to \(i\) is certain to be infinite & Starting from \(i\), the number of visits to \(i\) is certain to be finite \\
The return probability \(m_i\) equals 1 & The return probability \(m_i\) is strictly less than 1 \\
\bottomrule
\end{longtable}

We'll take the last one of these, about the return probability, as the definition, then prove that the others follow.

\begin{definition}
\protect\hypertarget{def:unlabeled-div-3}{}\label{def:unlabeled-div-3}

Let \((X_n)\) be a Markov chain on a state space \(\mathcal S\). For \(i \in \mathcal S\), let \(m_i\) be the return probability
\[ m_i = \mathbb P(X_n = i \text{ for some $n \geq 1$} \mid X_0 = i) . \]
If \(m_i = 1\), we say that state \(i\) is \textbf{recurrent}; if \(m_i < 1\), we say that state \(i\) is \textbf{transient}.

\end{definition}

Before stating this theorem, let us note that, from the point we're at state \(i\), the expected number of visits to \(i\) is
\[
\mathbb E(\#\text{ visits to $i$} \mid X_0 = i) = \sum_{n=0}^\infty \mathbb P(X_n = i \mid X_0 = i) = \sum_{n=1}^\infty p_{ii}(n) .
\]

\begin{theorem}
\protect\hypertarget{thm:rectran}{}\label{thm:rectran}

Consider a Markov chain with transition matrix \(\mathsf P\).

\begin{itemize}
\tightlist
\item
  If the state \(i\) is recurrent, then \(\sum_{n=1}^\infty p_{ii}(n) = \infty\), and we return to state \(i\) infinitely many times with probability 1.
\item
  If the state \(i\) is transient, then \(\sum_{n=1}^\infty p_{ii}(n) < \infty\), and we return to state \(i\) infinitely many times with probability 0.
\end{itemize}

\end{theorem}

We'll come to the proof in a moment, but first some examples.

\begin{example}
\protect\hypertarget{exm:rw-rec-trans}{}\label{exm:rw-rec-trans}

Consider the simple random walk. \protect\hyperlink{S08-return-rw}{In the last section} we saw that for the simple symmetric random walk with \(p = \frac12\) we have \(m_i = 1\), so the simple symmetric random walk is recurrent, but for \(p \neq \frac12\) we have \(m_i < 1\), so all the other simple random walks are transient.

\end{example}

\begin{example}
\protect\hypertarget{exm:rec}{}\label{exm:rec}

We saw this chain previously as Example \ref{exm:weird-period}:

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/periods-recap-1} 

}

\caption{Transition diagram from Subsection 7.2.}\label{fig:periods-recap}
\end{figure}

For states 5, 6 and 7, it's clear that the return probability is 1, since the Markov chain cycles around the triangle, so these states are recurrent.

States 1, 2, 3 and 4 are transient. In a moment we'll see a very quick way to show this, but in the meantime we can prove it directly by getting our hands dirty.

From state 4, we might go straight to state 5, in which case we can't come back, so \(m_4 \leq 1 - p_{45} = \frac23\), and state 4 is transient. Similarly, if we move from 1 to 4 to 5, we definitely won't come back to 1, so \(m_1 \leq 1 - p_{14}p_{45} = \frac56\), and state 1 is transient. By the similar arguments, \(m_3 \leq 1 - p_{34}p_{45} = \frac56\), and \(m_2 \leq 1 - p_{21}p_{14}p_{45} = \frac{11}{12}\), so these states are both transient too.

\end{example}

Notice that all states in the communicating class \(\{1,2,3,4\}\) are transient, while all states in the communicating class \(\{5,6,7\}\) are recurrent. We shall return to this point shortly. But first, we've put off the proof for too long.

\begin{proof}[Proof of Theorem \ref{thm:rectran}]

Suppose state \(i\) is recurrent. So starting from \(i\), the probability we return is \(m_i = 1\). After that return, it's as if we restart the chain from \(i\), because of the Markov property -- so the probability we return to \(i\) is again still \(m_i = 1\). Repeating this, we keep on returning, definitely visit infinitely often (with probability 1). In particular, since the number of visits to \(i\) starting from \(i\) is always infinite, its expectation is infinite too, and this expectation is \(\sum_{n=1}^\infty p_{ii}(n) = \infty\).

Suppose, on the other hand, that state \(i\) is transient. So starting from \(i\) the probability we return is \(m_i < 1\). Then the probability we return to \(i\) exactly \(r\) times before never coming back is
\[  \mathbb P \big((\text{number of returns to $i$}) = r\big) = m_i^r(1-m_i) , \]
since we must return on the first \(r\) occasions, but then fail to return on the next occasion.. This is a geometric distribution \(\text{Geom}(1-m_i)\) (the version with support \(\{0,1,2,\dots\}\)). Since the expectation of this type of \(\text{Geom}(p)\) random variable is \((1 - p)/p\), the expected number of returns is
\[ \mathbb E(\text{number of returns to $i$}) = \sum_{n=1}^\infty p_{ii}(n)  = \frac{1 - (1 - m_i)}{1 - m_i} = \frac{m_i}{1 - m_i} . \]
This is finite, since \(m_i <1\). Since the expected number of returns is finite, the probability we return infinitely many times must be 0.

\end{proof}

\hypertarget{rec-tran-classes}{%
\subsection{Recurrent and transient classes}\label{rec-tran-classes}}

We could find whether each state is transient or recurrent by calculating (or bounding) all the return probabilities \(m_i\), using the methods in \protect\hyperlink{S08-hitting-times}{the previous section}. But the following two theorems will give some highly convenient short-cuts.

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-5}{}\label{thm:unlabeled-div-5}

Within a communicating class, either every state is transient or every state is recurrent.

Formally: Let \(i, j \in \mathcal S\) be such that \(i \leftrightarrow j\). If \(i\) is recurrent, then \(j\) is recurrent also; while if \(i\) is transient, then \(j\) transient also.

\end{theorem}

For this reason, we can refer to a communicating class as a ``recurrent class'' or a ``transient class''. If a Markov chain is irreducible, we can refer to it as a ``recurrent Markov chain'' or a ``transient Markov chain''.

\begin{proof}

\emph{First part.} Suppose \(i \leftrightarrow j\) and \(i\) is recurrent. Then, for some \(n\), \(m\) we have \(p_{ij}(n)\), \(p_{ji}(m) > 0\). Then, by the Chapman--Kolmogorov equations,
\begin{align*}
\sum_{r=1}^\infty p_{jj}(n+m+r)
&\geq \sum_{r=1}^\infty p_{ji}(m)p_{ii}(r) p_{ij}(n) \\
&= p_{ji}^{(m)} \left(\sum_{r=1}^\infty p_{ii}(r) \right) p_{ij}(n) .
\end{align*}
If \(i\) is recurrent, then \(\sum_r p_{ii}(r) = \infty\). Then from the above equation, we also have \(\sum_r p_{jj}(n+m+r) = \infty\), meaning \(\sum_s p_{jj}(s) = \infty\), and \(j\) is recurrent.

\emph{Second part.} Suppose \(i\) is transient. Then \(j\) cannot be recurrent, because if it were, the previous argument with \(i\) and \(j\) swapped over would force \(i\) to in fact be recurrent also. So \(j\) must be transient.

\end{proof}

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-7}{}\label{thm:unlabeled-div-7}

~

\begin{itemize}
\tightlist
\item
  Every non-closed communicating class is transient.
\item
  Every finite closed communicating class is recurrent.
\end{itemize}

\end{theorem}

This theorem completely classifies the transience and recurrence of classes, with rare exception of infinite closed classes, which can require further examination.

\begin{proof}

\emph{First part.} Suppose \(i\) is in a non-closed communicating class, so for some \(j\) we have \(i \to j\), meaning \(p_{ij}(n) > 0\) for some \(n\), but \(j \not\to i\), meaning that once we reach \(j\) we cannot return to \(i\). We need to show that \(i\) is transient.

Consider the probability we return to \(i\) after time \(n\). We condition on whether \(X_n = j\) or not. This gives
\begin{align*}
\mathbb P(\text{return} & \text{ to } i \text{ after time $n$} \mid X_0 = i) \\
&= p_{ij}(n)\,\mathbb P(\text{return to $i$ after time $n$} \mid X_n = j, X_0 = i) \\
&\qquad {}+ \big(1 - p_{ij}(n)\big)\,\mathbb P(\text{return to $i$ after time $n$} \mid X_n \neq j, X_0 = i) \\
&\leq \mathbb P(\text{return to $i$ after time $n$} \mid X_n = j, X_0 = i) +  \big(1 - p_{ij}(n)\big) \\
&= 0 + \big(1 - p_{ij}(n)\big) \\
&< 1,
\end{align*}
since we can't get from \(j\) to \(i\), and since \(p_{ij}(n) > 0\). If \(i\) were recurrent we would certainly return infinitely often, and in particular certainly return after time \(n\). So \(i\) must be transient instead.

\emph{Second part.} Suppose the class \(C\) is finite and closed. Then there must be an \(i \in C\) such that, once we visit \(i\), the probability that we return to \(i\) infinitely many times is strictly positive; this is because we are going to stay in the finitely many states of \(C\) for infinitely many time steps. Then that state \(i\) is not transient, so it must be recurrent, which means that the whole class is recurrent.

\end{proof}

Going back to the \protect\hyperlink{exm:rec}{earlier example}, we see that the class \(\{5,6,7\}\) is closed and finite, and therefore recurrent, while class \(\{1,2,3,4\}\) is not closed and therefore transient. This is much less effort than the previous method!

\hypertarget{S09-positive-null}{%
\subsection{Positive and null recurrence}\label{S09-positive-null}}

It will be useful later to further divide recurrent classes, where the return probability \(m_i = 1\), by whether the expected return time \(\mu_i\) is finite or not. (Note that transient states always have \(\mu_i = \infty\).)

\begin{definition}
\protect\hypertarget{def:unlabeled-div-9}{}\label{def:unlabeled-div-9}

Let \((X_n)\) be a Markov chain on a state space \(\mathcal S\). Let \(i \in \mathcal S\) be a recurrent state, so \(m_i = 1\), and let \(\mu_i\) be the expected return time.
If \(\mu_i < \infty\), we say that state \(i\) is \textbf{positive recurrent}; if \(\mu_i = \infty\), we say that state \(i\) is \textbf{null recurrent}.

\end{definition}

The following facts can be proven in a similar way to the previous results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In a recurrent class, either all states are positive recurrent or all states are null recurrent.
\item
  All finite closed classes are positive recurrent.
\end{enumerate}

The first result means we can refer to a ``positive recurrent class'' or a ``null recurrent class'', and an irreducible Markov chain can be a ``positive recurrent Markov chain'' or a ``null recurrent Markov chain''.

Putting everything so far together, we have the following classification:

\begin{itemize}
\tightlist
\item
  non-closed classes are transient;
\item
  finite closed classes are positive recurrent;
\item
  infinite closed classes can be positive recurrent, null recurrent, or transient.
\end{itemize}

We know that the simple symmetric random walk is recurrent. We also saw in \protect\hyperlink{S08-return-rw}{the last section} that \(\mu_i = \infty\), so it is null recurrent.

We can also consider the simple symmetric random walk in \(d\)-dimensions, on \(\mathbb Z^d\). At each step we pick one of the coordinates and increase or decrease it by one; each of the \(2d\) possibilities having probability \(1/(2d)\). We have seen that for \(d=1\) this is null recurrent. A famous result by the Hungarian mathematician George Pólya from 1921 states the simple symmetric random walk is null recurrent for \(d = 1\) and \(d = 2\), but is transient for \(d \geq 3\). (Perhaps this is why cars often crash into each other, but aeroplanes very rarely do?)

\hypertarget{S09-strong-markov}{%
\subsection{Strong Markov property}\label{S09-strong-markov}}

\emph{This subsection is optional and nonexaminable.}

There was a cheat somewhere earlier in this section -- did you notice it?

The Markov property says that, if at some fixed time \(n\) we have \(X_n = i\), then the Markov chain from that point on is just like starting all over again from the state \(i\). When we applied this in the proof of Theorem \ref{thm:rectran}, we were using as \(n\) the first return to state \(i\). But that's not a fixed time -- it's a random time! Did we cheat?

Actually we're fine. The reason is that the first return to \(i\) isn't just any old random time, it's a ``stopping time'', and the Markov property also applies to stopping times too. Roughly speaking, a stopping time is a random time which has the property that ``you know when you get there''.

\begin{definition}
\protect\hypertarget{def:stopping}{}\label{def:stopping}

Let \((X_n)\) be a stochastic process in discrete time, and let \(T\) be a random time. Then \(T\) is a \textbf{stopping time} if for all \(n\), whether or not the event \(\{T = n\}\) occurs is completely determined by the random variables \(X_0, X_1, \dots, X_n\).

\end{definition}

So, for example:

\begin{itemize}
\tightlist
\item
  ``The first visit to state \(i\)'' is stopping time, because as soon as we reach \(i\), we know the value of \(T\).
\item
  ``Three time-steps after the second visit to \(j\)'' is a stopping time, because after our second visit we count on three more steps and have \(T\).
\item
  ``The time-step \emph{before} the first visit to \(i\)'' is not a stopping time, because we still need to go one step further on to know whether we had just been at time \(T\) or not.
\item
  ``The final visit to \(j\)'' is not a stopping time, because at the time of the visit we don't yet know whether we'll come back again or not.
\end{itemize}

There are lots of places in probability theory and finance when something that is true about a fixed time is also true about a random stopping time. When we use the Markov property with a stopping time, we call it the ``strong Markov property''.

\begin{theorem}[Strong Markov property]
\protect\hypertarget{thm:strong-markov}{}\label{thm:strong-markov}

Let \((X_n)\) be a Markov chain on a state space \(\mathcal S\), and let \(T\) be a stopping time that is finite with probability 1. Then all states \(x_0, \dots,x_{T-1}, i, j \in \mathcal S\) we have
\[  \mathbb P(X_{T+1}=j \mid X_T=i, X_{T-1} = x_{T-1} \dots, X_0 = x_0) = p_{ij} . \]

\end{theorem}

\begin{proof}

We have
\begin{align*}
&\mathbb P(X_{T+1}={}x_j \mid X_T=i, X_{T-1} = x_{T-1} \dots, X_0 = x_0) \\
&\qquad{}= \sum_{n=0}^\infty \mathbb P(T = n) \mathbb P(X_{n+1}=j \mid X_n=i, X_{n-1} = x_{n-1} \dots, X_0 = x_0, T = n) \\
&\qquad{}= \sum_{n=0}^\infty \mathbb P(T = n) \mathbb P(X_{n+1}=j \mid X_n=i, X_{n-1} = x_{n-1} \dots, X_0 = x_0) \\
&\qquad{}= \sum_{n=0}^\infty \mathbb P(T = n) \mathbb P(X_{n+1}=j \mid X_n=i) \\
&\qquad{}= \sum_{n=0}^\infty \mathbb P(T = n) p_{ij}\\
&\qquad{}= p_{ij} \sum_{n=0}^\infty \mathbb P(T = n) \\
&\qquad{}= p_{ij} ,
\end{align*}
as desired. The second line was by conditioning on the value of \(T\); in the third line we deleted the superfluous conditioning \(T = n\), because \(T\) is a stopping time, so the event \(T = n\) is entirely decided by \(X_n, X_{n-1}, \dots, X_0\); the fourth line used the (usual non-strong) Markov property; the fifth line is just the definition of \(p_{ij}\); the sixth line took \(p_{ij}\) out of the sum; and the seventh line is because \(T\) is finite with probability 1, so \(\mathbb P(T = n)\) sums to 1.

\end{proof}

\hypertarget{S09-lemma}{%
\subsection{A useful lemma}\label{S09-lemma}}

\emph{This subsection is optional and nonexaminable.}

The following lemma will be used in some later optional and nonexaminable proofs.

\begin{lemma}
\protect\hypertarget{lem:lemma}{}\label{lem:lemma}

Let \((X_n)\) be an irreducible and recurrent Markov chain. Then for any initial distribution and any state \(j\), we will certainly hit \(j\), so the hitting time \(H_j\) is finite with probability 1.

\end{lemma}

\begin{proof}

It suffices to prove the lemma when the initial distribution is ``start at \(i\)''. (We can repeat for all \(i\), then build any initial distribution from a weighted sum of ``start at \(i\)''s.)

Since the chain is irreducible, we have \(j \to i\), so pick \(m\) with \(p_{ji}(m) > 0\). Since the chain is recurrent, we know the return probability from \(j\) to \(j\) is 1, and we return infinitely many times with probability 1. We just need to glue these two facts together.

We have
\begin{align*}
1 &= \mathbb P(X_n = j \text{ for infinitely many $n$} \mid X_0 = j) \\
&= \mathbb P(X_n = j \text{ for some $n > m$} \mid X_0 = j) \\
&= \sum_k \mathbb P(X_m = k \mid X_0 = j) \,\mathbb P(X_n = j \text{ for some $n > m$} \mid X_m = k, X_0 = j) \\
&= \sum_k p_{jk}(m) \,\mathbb P(H_j < \infty \mid X_0 = k) ,
\end{align*}
where the last line used the Markov property to treat the chain as starting over again when it reaches some state \(k\) at time \(m\). Note that \(\sum_k p_{jk}(m) = 1\), since that's the sum of the probabilities of going anywhere in \(m\) steps. This means we must have \(\mathbb P(H_j < \infty \mid X_0 = k)\) whenever \(p_{jk}(m) > 0\), to ensure the final line does indeed sum to 1. But we stated earlier that \(p_{ji}(m) > 0\), so we indeed have \(\mathbb P(H_j < \infty \mid X_0 = i)\), as required.

\end{proof}

\textbf{In the next section}, we look at how positive recurrent Markov chains can settle into a ``stationary distribution'' and experience long-term stability.

\hypertarget{S10-stationary-distributions}{%
\section{Stationary distributions}\label{S10-stationary-distributions}}

\begin{itemize}
\tightlist
\item
  Stationary distributions and how to find them
\item
  Conditions for existence and uniqueness of the stationary distribution
\end{itemize}

\hypertarget{def-stationary-definition}{%
\subsection{Definition of stationary distribution}\label{def-stationary-definition}}

Consider \protect\hyperlink{S05-example}{the two-state ``broken printer'' Markov chain from Lecture 5}.

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/twostate3-1} 

}

\caption{Transition diagram for the two-state broken printer chain.}\label{fig:twostate3}
\end{figure}

Suppose we start the chain from the initial distribution
\[ \lambda_0 = \mathbb P(X_0 = 0) = \frac{\beta}{\alpha+\beta} \qquad \lambda_1 = \mathbb P(X_0 = 1) = \frac{\alpha}{\alpha+\beta} . \]
(You may recognise this from \protect\hyperlink{P03}{Question 3 on Problem Sheet 3 and the associated video}.) What's the distribution after step 1? By conditioning on the initial state, we have
\begin{align*}
  \mathbb P(X_1 = 0) &= \lambda_0 p_{00} + \lambda_1 p_{10} = \frac{\beta}{\alpha+\beta}(1-\alpha) + \frac{\alpha}{\alpha+\beta}\beta = \frac{\beta}{\alpha+\beta} ,\\
  \mathbb P(X_1 = 1) &= \lambda_0 p_{01} + \lambda_1 p_{11} = \frac{\beta}{\alpha+\beta}\alpha + \frac{\alpha}{\alpha+\beta}(1-\beta) = \frac{\alpha}{\alpha+\beta} .
\end{align*}
So we're still in the same distribution we started in. By repeating the same calculation, we're still going to be in this distribution after step 2, and step 3, and forever.

More generally, if we start from a state given by a distribution \(\boldsymbol \pi = (\pi_i)\), then after step 1 the probability we're in state \(j\) is \(\sum_i \pi_i p_{ij}\). So if \(\pi_j = \sum_i \pi_i p_{ij}\), we stay in this distribution forever. We call such a distribution a stationary distribution. We again recodnise this formula as a matrix--vector multiplication, so this is \(\boldsymbol \pi = \boldsymbol \pi\mathsf P\), where \(\boldsymbol \pi\) is a \emph{row} vector.

\begin{definition}
\protect\hypertarget{def:unlabeled-div-12}{}\label{def:unlabeled-div-12}

Let \((X_n)\) be a Markov chain on a state space \(\mathcal S\) with transition matrix \(\mathsf P\).
Let \(\boldsymbol \pi = (\pi_i)\) be a distribution on \(\mathcal S\), in that \(\pi_i \geq 0\) for all \(i \in \mathcal S\) and \(\sum_{i \in \mathcal S} \pi_i = 1\). We call \(\boldsymbol \pi\) a \textbf{stationary distribution} if
\[ \pi_j = \sum_{i\in \mathcal S} \pi_i p_{ij} \quad \text{for all $j \in \mathcal S$,} \]
or, equivalently, if \(\boldsymbol \pi = \boldsymbol \pi\mathsf P\).

\end{definition}

Note that we're saying the \emph{distribution} \(\mathbb P(X_n = i)\) stays the same; the Markov chain \((X_n)\) itself will keep moving. One way to think is that if we started off a thousand Markov chains, choosing each starting position to be \(i\) with probability \(\pi_i\), then (roughly) \(1000 \pi_j\) of them would be in state \(j\) at any time in the future -- but not necessarily the same ones each time.

\hypertarget{find-stationary}{%
\subsection{Finding a stationary distribution}\label{find-stationary}}

Let's try an example. Consider \protect\hyperlink{S06-example1}{the no-claims discount Markov chain from Lecture 6} with state space \(\mathcal S=\{1,2,3\}\) and transition matrix
\[ \mathsf P =\begin{pmatrix}
    \tfrac14 &\tfrac34 & 0\\
    \tfrac14 &0 & \tfrac34\\
    0 &\tfrac14 & \tfrac34\\
    \end{pmatrix} .\]

We want to find a stationary distribution \(\boldsymbol \pi\), which must solve the equation \(\boldsymbol \pi =\boldsymbol \pi\mathsf P\), which is
\[ \begin{pmatrix} \pi_1 & \pi_2 & \pi_3 \end{pmatrix}  = \begin{pmatrix} \pi_1 & \pi_2 & \pi_3 \end{pmatrix}  \begin{pmatrix}
    \tfrac14 &\tfrac34 & 0\\
    \tfrac14 &0 & \tfrac34\\
    0 &\tfrac14 & \tfrac34\\
    \end{pmatrix} .\]

Writing out the equations coordinate at a time, we have
\begin{align*}
    \pi_1 &= \tfrac14\pi_1+\tfrac14\pi_2 , \\
    \pi_2 &= \tfrac34\pi_1+\tfrac14\pi_3 , \\
    \pi_3 &= \tfrac34\pi_2+\tfrac34\pi_3 . 
    \end{align*}
Since \(\boldsymbol\pi\) must be a distribution, we also have the ``normalising condition''
\[ \pi_1+\pi_2+\pi_3=1 . \]

The way to solve these equations is first to solve for all the variables \(\pi_i\) in terms of a convenient \(\pi_{j}\) (called the ``working variable'') and then substitute all of these expressions into the normalising condition to find a value for \(\pi_{j}\).

Let's choose \(\pi_2\) as our working variable. It turns out that \(\boldsymbol\pi = \boldsymbol\pi \mathsf{P}\) always gives one more equation than we actually need, so we can discard one of them for free. Let's get rid of the second equation, and the solve the first and third equations in terms of our working variable \(\pi_2\), to get
\begin{equation}
\pi_1=\tfrac13\pi_2 \qquad \pi_3=3\pi_2 . \label{eq:statt}
\end{equation}

Now let's turn to the normalising condition. That gives
\[ \pi_1+\pi_2+\pi_3 = \tfrac13\pi_2+\pi_2+3\pi_2 = \tfrac{13}{3} \pi_2 = 1 .\]
So the working variable is solved to be \(\pi_2 = \frac{3}{13}\). Substituting this back into \eqref{eq:statt}, we have \(\pi_1=\tfrac13\pi_2 = \frac{1}{13}\) and \(\pi_3=3\pi_2 = \frac{9}{13}\). So the full solution is
\[ \boldsymbol \pi = (\pi_1, \pi_2, \pi_3) = \left(\tfrac{1}{13}, \tfrac{3}{13}, \tfrac{9}{13}\right). \]

The method we used here can be summarised as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write out \(\boldsymbol \pi = \boldsymbol \pi\mathsf P\) coordinate by coordinate. Discard one of the equations.
\item
  Select one of the \(\pi_i\) as a working variable and treat it as a parameter. Solve the equations in terms of the working variable.
\item
  Substitute the solution into the normalising condition to find the working variable, and hence the full solution.
\end{enumerate}

It can be good practice to use the equation discarded earlier to check that the calculated solution is indeed correct.

One extra example for further practice and to show how you should present your solutions to such problems:

\begin{example}
\protect\hypertarget{exm:stationary-1}{}\label{exm:stationary-1}

\emph{Consider a Markov chain on state space \(\mathcal S = \{1,2,3\}\) with transition matrix}
\[ \mathsf P = \begin{pmatrix} \tfrac12 & \tfrac14& \frac14 \\
                   \tfrac14& \frac12& \frac14 \\
                   0       & \frac14 & \frac34 \end{pmatrix} . \]
\emph{Find a stationary distribution for this Markov chain.}

\emph{Step 1.} Writing out \(\boldsymbol \pi = \boldsymbol \pi\mathsf P\) coordinate-wise, we have
\begin{align*}
\pi_1 &= \tfrac12 \pi_1 + \tfrac14\pi_2 \\
\pi_2 &= \tfrac14\pi_1 + \tfrac12\pi_2 + \tfrac14\pi_3 \\
\pi_3 &= \tfrac14\pi_1 + \tfrac14\pi_2 + \tfrac34\pi_3 .
\end{align*}
We choose to discard the third equation.

\emph{Step 2.} We choose \(\pi_1\) as our working variable. From the first equation we get \(\pi_2 = 2\pi_1\). From the second equation we get \(\pi_3 = 2\pi_2 - \pi_1\), and substituting the previous \(\pi_2 = 2\pi_1\) into this, we get \(\pi_3 = 3\pi_1\).

\emph{Step 3.} The normalising condition is
\[ \pi_1 + \pi_2 + \pi_3 = \pi_1 + 2\pi_1 + 3\pi_1 = 6\pi_1 = 1 . \]
Therefore \(\pi_1 = \frac16\). Substituting this into our previous expressions, we get \(\pi_2 = 2\pi_1 = \frac13\) and \(\pi_3 = 3\pi_1 = \frac12\).
Thus the solution is
\(\boldsymbol\pi = ( \tfrac16, \tfrac13, \tfrac12 )\).

We can check our answer with the discarded third equation, just to make sure we didn't make any mistakes. We get
\[ \pi_3 = \tfrac14\pi_1 + \tfrac14\pi_2 + \tfrac34\pi_3 = \tfrac14\tfrac16 + \tfrac14\tfrac13+\tfrac34\tfrac12 = \tfrac1{24} + \tfrac2{24} + \tfrac{9}{24} = \tfrac{1}{2} , \]
which is as it should be.

\end{example}

\hypertarget{exist-unique}{%
\subsection{Existence and uniqueness}\label{exist-unique}}

Given a Markov chain its natural to ask:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Does a stationary distribution exist?
\item
  If a stationary distribution does exists, is there only one, or are there be many stationary distributions?
\end{enumerate}

The answer is given by the following very important theorem.

\begin{theorem}
\protect\hypertarget{thm:statex}{}\label{thm:statex}

Consider an irreducible Markov chain.

\begin{itemize}
\tightlist
\item
  If the Markov chain is positive recurrent, then a stationary distribution \(\boldsymbol \pi\) exists, is unique, and is given by \(\pi_i = 1/\mu_{i}\), where \(\mu_{i}\) is the expected return time to state \(i\).
\item
  If the Markov chain is null recurrent or transient, then no stationary distribution exists.
\end{itemize}

\end{theorem}

We give \protect\hyperlink{stat-proof}{an optional and nonexaminable proof to the first part below}.

In our no-claims discount example, the chain is irreducible and, like all finite state irreducible chains, it is positive recurrent. Thus the stationary distribution \(\boldsymbol\pi = (\tfrac{1}{13}, \tfrac{3}{13}, \tfrac{9}{13})\) we found is the unique stationary distribution for that chain.
Once we have the stationary distribution \(\boldsymbol\pi\), we get the expected return times \(\mu_i = 1/\pi_i\) for free: the expected return times are \(\mu_1 = 13\), \(\mu_2 = \frac{13}{3} = 4.33\), and \(\mu_3 = \frac{13}{9} = 1.44\).

Note the condition in Theorem \ref{thm:statex} that the Markov chain is irreducible. What if the Markov chain is not irreducible, so has more than one communicating class? We can work out what must happen from the theorem:

\begin{itemize}
\tightlist
\item
  If none of the classes are positive recurrent, then no stationary distribution exists.
\item
  If exactly one of the classes is positive recurrent (and therefore closed), then there exists a unique stationary distribution, supported only on that closed class.
\item
  If more the one of the classes are positive recurrent, then many stationary distributions will exist.
\end{itemize}

\begin{example}
\protect\hypertarget{exm:stat-rw}{}\label{exm:stat-rw}

Consider the simple random walk with \(p \neq 0,1\). This Markov chain is irreducible, and is null recurrent for \(p = \frac12\) and transient for \(p \neq \frac12\). Either way, the theorem tells us that no stationary distribution exists.

\end{example}

\begin{example}
\protect\hypertarget{exm:stat-two}{}\label{exm:stat-two}

Consider the Markov chain with transition matrix
\[ \mathsf P = \begin{pmatrix} \frac12 & \frac12 & 0 & 0 \\
\frac12 & \frac12 & 0 & 0 \\
0 & 0 & \frac14 & \frac34\\
0 & 0 & \frac12 & \frac12\end{pmatrix} . \]
This chain has two closed positive recurrent classes, \(\{1,2\}\) and \(\{3,4\}\).

Solving \(\boldsymbol\pi = \boldsymbol\pi\mathsf P\) gives
\begin{align*}
    \pi_1 = \tfrac12 \pi_1 + \tfrac12\pi_2 \qquad &\Rightarrow \qquad \phantom{3}\pi_1 = \pi_2 \\
    \pi_2 = \tfrac12 \pi_1 + \tfrac12\pi_2 \qquad &\Rightarrow \qquad \phantom{3}\pi_1 = \pi_2 \\
    \pi_3 = \tfrac14 \pi_3 + \tfrac12\pi_4 \qquad &\Rightarrow \qquad 3\pi_3 = 2\pi_4 \\
    \pi_4 = \tfrac34 \pi_3 + \tfrac12\pi_4 \qquad &\Rightarrow \qquad 3\pi_3 = 2\pi_4 , 
\end{align*}
giving us the same two constraints twice each. We also have the normalising condition \(\pi_1 + \pi_2+\pi_3+\pi_4 = 1\). If we let \(\pi_1 + \pi_2 = \alpha\) and \(\pi_3 + \pi_4 = 1-\alpha\), we see that
\[ \boldsymbol\pi = \left(\tfrac12\alpha\quad \tfrac12\alpha\quad \tfrac25(1-\alpha)\quad \tfrac35(1-\alpha)\right) \]
is a stationary distribution for any \(0 \leq \alpha \leq 1\), so we have infinitely many stationary distributions.

\end{example}

\hypertarget{stat-proof}{%
\subsection{Proof of existence and uniqueness}\label{stat-proof}}

\emph{This subsection is optional and nonexaminable.}

It's very important to be able to find the stationary distribution(s) of a Markov chain -- you can reasonably expect a question on this to turn up on the exam. You should also know the conditions for existence and uniqueness of the stationary distribution. Being able to \emph{prove} existence and uniqueness is less important, although for completeness we will do so here.

Theorem \ref{thm:statex} had two points. The more important point was that irreducible, positive recurrent Markov chains have a stationary distribution, that it is unique, and that it is given by \(\pi_i = 1/\mu_i\). We give a proof of that below, doing the existence and uniqueness parts separately.
The less important point was that null recurrent and transitive Markov chains do not have a stationary distribution, and this is more fiddly. You can find a proof (usually in multiple parts) in books such as Norris, \href{https://www.statslab.cam.ac.uk/~james/Markov/}{\emph{Markov Chains}}, Section 1.7.

\textbf{Existence:} \emph{Every positive recurrent Markov chain has a stationary distribution.}

Before we start, one last definition. Let us call a vector \(\boldsymbol\nu\) a \textbf{stationary vector} if \(\boldsymbol\nu \mathsf P = \boldsymbol\nu\). This is exactly like a stationary distribution, except without the normalisation condition that it has to sum to 1.

\begin{proof}

Suppose that \((X_n)\) is recurrent (either positive or null, for the moment).

Our first task will be to find a stationary vector. Fix an initial state \(k\), and let \(\nu_i\) be the expected number of visits to \(i\) before we return back to \(k\). That is,
\begin{align*}
\nu_i &= \mathbb E (\# \text{ visits to $i$ before returning to $k$} \mid X_0 = k) \\
      &= \mathbb E \sum_{n=1}^{M_k} \mathbb P(X_n = i \mid X_0 = k) \\
      &= \sum_{n=1}^\infty \mathbb P(X_n = i \text{ and } n \leq M_k \mid X_0 = k)  ,
\end{align*}
where \(M_k\) is \protect\hyperlink{S08-return-times}{the return time, as in Section 8}.
Let us note for later use that, under this definition, \(\nu_k = 1\), because the only visit to \(k\) is the return to \(k\) itself.

Since \(\boldsymbol\nu\) is counting the number of visits to different states in a certain (random) time, it seems plausible that \(\boldsymbol\nu\) suitably normalised could be a stationary distribution, meaning that \(\boldsymbol\nu\) itself could be a stationary vector. Let's check.

We want to show that \(\sum_i \nu_i p_{ij} = \nu_j\). Let's see what we have:
\begin{align*}
\sum_{i \in \mathcal S} \nu_i p_{ij} &= \sum_{i \in \mathcal S} \sum_{n=1}^\infty \mathbb P(X_n = i \text{ and } n \leq M_k \mid X_0 = k) \, p_{ij} \\
&= \sum_{n=1}^\infty \sum_{i \in \mathcal S} \mathbb P(X_n = i \text{ and } X_{n+1} = j \text{ and } n \leq M_k \mid X_0 = k) \\
&= \sum_{n=1}^\infty \mathbb P(X_{n+1} = j \text{ and } n \leq M_k \mid X_0 = k) .
\end{align*}
(Exchanging the order of the sums is legitimate, because recurrence of the chain means that \(M_k\) is finite with probability 1.)
We can now do a cheeky bit of monkeying around with the index \(n\), by swapping out the visit to \(k\) at time \(M_k\) with the visit to \(k\) at time \(0\). This means instead of counting the visits from \(1\) to \(M_k\), we can count the visits from \(0\) to \(M_k - 1\). Shuffling the index about, we get
\begin{align*}
\sum_{i \in \mathcal S} \nu_i p_{ij} &= \sum_{n=0}^\infty \mathbb P(X_{n+1} = j \text{ and } n \leq M_k-1 \mid X_0 = k) \\
&= \sum_{n+1=1}^\infty \mathbb P(X_{n+1} = j \text{ and } n+1 \leq M_k \mid X_0 = k) \\
&= \sum_{n=1}^\infty \mathbb P(X_{n} = j \text{ and } n \leq M_k \mid X_0 = k) \\
&= \nu_j .
\end{align*}
So \(\boldsymbol\nu\) is indeed a stationary vector.

We now want normalise \(\boldsymbol\nu\) into a stationary distribution by dividing through by \(\sum_i \nu_i\). We can do this if \(\sum_i \nu_i\) is finite. But \(\sum_i \nu_i\) is the expected total number of visits to all states before return to \(k\), which is precisely the expected return time \(\mu_k\). Now we use the assumption that \((X_n)\) is \emph{positive} recurrent. This means that \(\mu_k\) is finite, so \(\boldsymbol\pi = (1/\mu_k) \boldsymbol\nu\) is a stationary distribution.

\end{proof}

\textbf{Uniqueness:} \emph{For an irreducible, positive recurrent Markov chain, the stationary distribution is unique and is given by \(\pi_i = 1/\mu_i\).}

I read the following proof in Stirzaker, \href{https://leeds.primo.exlibrisgroup.com/permalink/44LEE_INST/13rlbcs/alma991013131349705181}{\emph{Elementary Probability}}, Section 9.5.

\begin{proof}

Suppose the Markov chain is irreducible and positive recurrent, and suppose \(\boldsymbol\pi\) is a stationary distribution. We want to show that \(\pi_i = 1/\mu_i\) for all \(i\).

The only equation we have for \(\mu_k\) is \protect\hyperlink{S08-return-times}{this one from Section 8}:
\begin{equation}
\mu_k = 1 + \sum_j p_{kj} \eta_{jk} . \label{eq:pf2}
\end{equation}
Since that involves the expected hitting times \(\eta_{ik}\), let's write down \protect\hyperlink{S08-return-times}{the equation for them} too:
\begin{equation}
\eta_{ik} = 1 + \sum_j p_{ij} \eta_{jk} \qquad \text{for all $i \neq k$.} \label{eq:pf1}
\end{equation}

In order to apply the fact that \(\boldsymbol\pi\) is a stationary distribution, we'd like to get these into an equation with \(\sum_i \pi_i p_{ij}\) in it. Here's a way we can do that:
Take \eqref{eq:pf1}, multiply it by \(\pi_i\) and sum over all \(i \neq k\), to get
\begin{equation}
\sum_{i} \pi_i \eta_{ik} = \sum_{i \neq k} \pi_i + \sum_j \sum_{i \neq k} \pi_i p_{ij} \eta_{jk} . \label{eq:pf3}
\end{equation}
(The sum on the left can be over all \(i\), since \(\eta_{kk} = 0\).)
Also, take \eqref{eq:pf2} and multiply it by \(\pi_k\) to get
\begin{equation}
\pi_k \mu_k = \pi_k + \sum_j \pi_k p_{kj} \eta_{jk} \label{eq:pf4}
\end{equation}
Now add \eqref{eq:pf3} and \eqref{eq:pf4} together to get
\[
\sum_{i} \pi_i \eta_{ik} + \pi_k \mu_k = \sum_i \pi_i + \sum_j \sum_i\pi_i p_{ij} \eta_{jk} .
\]

We can now use \(\sum_i\pi_i p_{ij} = \pi_j\), along with \(\sum_i \pi_i = 1\), to get
\[ \sum_{i} \pi_i \eta_{ik} + \pi_k \mu_k = 1 + \sum_j \pi_j \eta_{jk} . \]

But the first term on the left and the last term on the right are equal, and because the Markov chain is irreducible and positive recurrent, they are finite. (That was \protect\hyperlink{S09-lemma}{our lemma in the previous section}.) Thus we're allowed to subtract them, and we get \(\pi_k \mu_k = 1\), which is indeed \(\pi_k = 1/\mu_k\). We can repeat the argument for every choice of \(k\).

\end{proof}

\textbf{In the next section}, we see how the stationary distribution tells us very important things about the long-term behaviour of a Markov chain.

\hypertarget{P05}{%
\section*{Problem sheet 5}\label{P05}}
\addcontentsline{toc}{section}{Problem sheet 5}

\commtrue

You should attempt all these questions and write up your solutions in advance of your workshop in week 6 (Monday 1 or Tuesday 2 March) where the answers will be discussed.

\textbf{1.} Find a stationary distribution for the Markov chain with transition matrix
\[ \mathsf P = \begin{pmatrix} \frac13 & \frac23 & 0 \\
                               \frac16 & \frac13 & \frac12 \\
                               0 & \frac13 & \frac23 \end{pmatrix} . \]

\begin{myanswers}
\emph{Solution.} The equations are
\begin{align*}
\pi_1 &= \tfrac13 \pi_1 + \tfrac16 \pi_2 \\
\pi_2 &= \tfrac23 \pi_1 + \tfrac13 \pi_2 + \tfrac23 \pi_3 \\
\pi_3 &= \phantom{\tfrac23 \pi_1} + \tfrac12 \pi_2 + \tfrac23 \pi_3
\end{align*}
It'll make out lives more pleasant if we pick \(\pi_2\) as the working variable and delete the second equation. The first equation becomes \(\pi_1 = \tfrac14 \pi_2\), and the third equation become \(\pi_3 = \tfrac32 \pi_2\). The normalising condition is
\[ \pi_1 + \pi_2 + \pi_3 = \left( \tfrac14 + 1 + \tfrac32\right) \pi_2 = \tfrac{11}{4} \pi_2 .    \]
So \(\pi_2 = \frac4{11}\), so \(\pi_1 = \frac{1}{11}\) and \(\pi_3 = \frac{6}{11}\). The stationary distribution is \(\boldsymbol\pi = (\frac{1}{11}, \frac{4}{11}, \frac{6}{11})\).

\end{myanswers}

\textbf{2.} Consider a Markov chain with state space \(\mathcal S = \{1,2,3,4\}\) and transition matrix
\[  \mathsf P = \begin{pmatrix} \frac14 & \frac12 &\frac14 & 0 \\
                      \frac14 & \frac14 & \frac12 & 0 \\
                      \frac12 & \frac12 & 0 & 0 \\
                      \frac14 & 0       &\frac14 & \frac 12
                      \end{pmatrix} . \]

\textbf{(a)} Draw a transition diagram for this Markov chain.

\begin{myanswers}

\emph{Solution.}

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q52-1} 

}

\caption{Transition diagram for Question 2.}\label{fig:Q52}
\end{figure}

\end{myanswers}

\textbf{(b)} Identify the communicating classes. State whether each class closed or not. State whether each class is positive recurrent, null recurrent, or transient.

\begin{myanswers}
\emph{Solution.} The class \(\{1,2,3\}\) is closed, so is positive recurrent. The class \(\{4\}\) is not closed, so is transient.

\end{myanswers}

\textbf{(c)} Find a stationary distribution for this Markov chain.

\begin{myanswers}
\emph{Solution.} First, we write out the equations \(\boldsymbol\pi = \boldsymbol\pi\mathsf P\), which are
\begin{align*}
\pi_1 &= \tfrac14\pi_1 + \tfrac14\pi_2 + \tfrac12\pi_3 + \tfrac14\pi_4 \\
\pi_2 &= \tfrac12\pi_1 + \tfrac14\pi_2 + \tfrac12\pi_3 \\
\pi_3 &= \tfrac14\pi_1 + \tfrac12\pi_2 \phantom{{}+\tfrac12\pi_3} + \tfrac14\pi_4 \\
\pi_4 &= \phantom{\tfrac12\pi_1 + \tfrac14\pi_2 + \tfrac12\pi_3+{}}  \tfrac12 \pi_4 .
\end{align*}
From the fourth equation we immediately see that \(\pi_4 = 0\). Second, we rewrite the first two of the other equations with \(\pi_1\) as the working variable, which gives
\begin{align}
3\pi_1 &= \phantom{3}\pi_2 + 2\pi_3 \tag{1} \\
2\pi_1 &= 3\pi_2 - 2\pi_3 \tag{2} .
\end{align}
Adding (1) and (2) gives \(5\pi_1 = 4\pi_2\), so \(\pi_2 = \frac54\pi_1\). Substituting this into (1) and solving gives \(\pi_3 = \frac78 \pi_1\). Third, the normalising condition is
\[ \pi_1 + \pi_2 + \pi_3 + \pi_4 = \pi_1 + \tfrac54\pi_1 + \tfrac78\pi_1 + 0 = \tfrac{25}{8}\pi_1 = 1, \]
so \(\pi_1 = \frac{8}{25}\). Hence, we have a stationary distribution
\[ \boldsymbol\pi = \left(\tfrac{8}{25} \quad \tfrac{10}{25} \quad \tfrac{7}{25} \quad 0 \right) .\]

\end{myanswers}

\textbf{(d)} Is this the only stationary distribution?

\begin{myanswers}
\emph{Solution.} Yes. The Markov chain has one positive recurrent class, so there is a unique stationary distribution, and it is supported only on that class.

\end{myanswers}

\textbf{3.} Consider a Markov chain with state space \(\mathcal S = \{1,2,3,4,5\}\) and transition matrix
\[  \mathsf P = \begin{pmatrix} \frac13 & \frac23 & 0 &0 & 0 \\
                   \frac13 & \frac23 & 0 &0 & 0 \\
                   0 &\frac35 & \frac15 & \frac15 & 0 \\
                   0 & 0 & 0 & \frac14 &\frac34 \\
                   0 & 0 & 0 & \frac12 & \frac12 \end{pmatrix} . \]

\textbf{(a)} Draw a transition diagram for this Markov chain.

\begin{myanswers}

\emph{Solution.}

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q53-1} 

}

\caption{Transition diagram for Question 3.}\label{fig:Q53}
\end{figure}

\end{myanswers}

\textbf{(b)} Identify the communicating classes. State whether each class closed or not. State if each class is positive recurrent, null recurrent, or transient.

\begin{myanswers}
\emph{Solution.} The class \(\{1,2\}\) is closed, so is positive recurrent. The class \(\{3\}\) is not closed, so is transient. The class \(\{4,5\}\) is closed, so is positive recurrent.

\end{myanswers}

\textbf{(c)} Find all of the stationary distributions for this Markov chain.

\begin{myanswers}
\emph{Solution.} We can have a stationary distribution supported on either of the positive recurrent classes, but we will always have \(\pi_3 = 0\), as that state is transient. For the class \(\{1,2\}\) we have
\[ \pi_1 = \tfrac13\pi_1 + \tfrac13\pi_2 \qquad \pi_2 = \tfrac23\pi_1 + \tfrac23\pi_2 , \]
giving \(\pi_2 = 2\pi_1\), and a stationary distribution \((\tfrac 13, \tfrac23, 0, 0, 0)\). For the class \(\{4,5\}\), we have
\[ \pi_4 = \tfrac14\pi_4 + \tfrac12\pi_5 \qquad \pi_5 = \tfrac34\pi_4 + \tfrac12\pi_5 , \]
giving \(3\pi_4 = 2\pi_5\), and a stationary distribution \((0,0,0,\frac25,\frac35)\). Finally, any linear combination of those where the coefficients are positive and add to \(1\) will also be a stationary distribution, so we have a family of stationary distributions
\[ \left( \tfrac13\alpha \quad \tfrac23\alpha \quad 0 \quad \tfrac25(1-\alpha) \quad \tfrac35(1-\alpha) \right)    \]
for \(0 \leq \alpha \leq 1\).

\end{myanswers}

\textbf{4.} Consider the simple random walk \((X_n)\) on \(\mathcal S = \mathbb Z_+ = \{0,1,2,\dots\}\) with up probability \(p\) and down probability \(q = 1-p\), and a mixed barrier at \(0\), where \(p_{01} = p\), \(p_{00} = q\), and \(p_{0i} = 0\) otherwise. We seek a stationary distribution for this Markov chain.

\textbf{(a)} Suppose \(p \neq \frac12\). Show that the general solution to
\[ \pi_j = \sum_i \pi_i p_{ij} = p\pi_{j-1} + q\pi_{j+1} \]
is \(\pi_i = A + B\tau^i\), where \(\tau = p/q\).

\begin{myanswers}
\emph{Solution.} Let's rewrite this as \(q\pi_{j+1} - \pi_j + p\pi_{j-1} = 0\). This is a homogeneous linear difference equation. The characteristic equation is \(p\lambda^2 - \lambda + p = 0\), which factorises as \((q\lambda - p)(\lambda-1) = 0\) (using that \(p + q = 1\)), which has solutions \(\lambda = 1\) and \(\lambda = p/q = \tau\). Since \(p \neq \frac12\), these roots are distinct. So the general solution is \(\pi_i = A + B\tau^i\).

\end{myanswers}

\textbf{(b)} Show that the initial condition \(\pi_0 = q\pi_0 + q\pi_1\) gives \(A = 0\).

\begin{myanswers}
\emph{Solution.} The initial condition gives
\[ A + B = q(A + B) + q(A + B\tau)  ,\]
which after rearranging gives \((1-2q)A = 0\), where we have used \(q + q\tau = q + p = 1\). Since \(1-2q\neq 0\), we must have \(A = 0\).

\end{myanswers}

\textbf{(c)} By considering the normalising condition \(\sum_i \pi_i =1\), work out for what values of \(p \neq \frac12\) there exists a stationary distribution for \((X_n)\). What is the stationary distribution (when it exists)?

\begin{myanswers}
\emph{Solution.} We have \(\pi_i = B\tau^i\), so require \(B \sum_{i=1}^\infty \tau^i = 1\). When \(p > \frac12\), then \(\tau > 1\) and the sum does not converge, and we have no stationary distribution. When \(p < \frac12\), then \(\tau < 1\), so
\[ \sum_{i=1}^\infty \tau^i = \frac{1}{1-\tau} \qquad \Rightarrow \qquad B = \frac{1}{\frac{1}{1-\tau}} = 1 - \tau, \]
so we have a geometric stationary distribution \(\pi_i = (1-\tau)\tau^i\).

\end{myanswers}

\textbf{(d)} Does there exist a stationary distribution when \(p = \frac12\)?

\begin{myanswers}
\emph{Solution.}
Here, the linear difference equation has general solution \(\pi_i = A + Bi\), as we saw with the symmetric gambler's ruin problem. The initial condition gives \(A = \frac12A + \frac12(A + B)\), and so \(B = 0\), giving \(\pi_i = A\). The normalisation condition is \(\infty \times A = 1\), which cannot be fulfilled. Hence no stationary distribution exists.

\end{myanswers}

\textbf{5.} The infinite rooted binary tree is a graph with no cycles. There is one special vertex, the root 0, that has two edges, and every other edge has three edges. A Markov chain \((X_n)\) starts from 0, then at each time step, takes one of the edges coming out of the current vertex and moves along it to the neighbouring vertex. Note that a step can go away from the root or towards the root.

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/tree-1} 

}

\caption{The first three-and-a-bit levels of the rooted binary tree.}\label{fig:tree}
\end{figure}

By considering the distance of \((X_n)\) from the root, or otherwise, show that \((X_n)\) is transient.

\begin{myanswers}
\emph{Solution.} We want to show that \(m_0\), the return probability to the root, is strictly less than 1.

Let \(Y_n\) be the distance of \(X_n\) from the root as suggested in the question. Then \(X_n\) returns to to the root if and only if \(Y_n\) returns to \(0\), so we can look at that instead.

If \((Y_n)\) is the distance from the root, than at each time step \(Y_{n+1} = Y_n + 1\) with probability \(\frac23\), if we take either of the two edges away from root, or \(Y_n = Y_n - 1\) with probability \(\frac13\), if we take the edge back towards the root. If \(Y_n = 0\) is at the root, then \(Y_{n+1} = Y_n+1\). So \((Y_n)\) is a simple random walk with positive drift \(p = \frac23 > \frac12\) and a reflecting barrier at \(0\), which is transient.

To prove it's transient, we can use a conditioning on the first step argument to get \(m_0 = h_{10}\), and
\[ h_{i\,0} = \tfrac23 h_{i+1\,0} + \tfrac23 h_{i-1,\,0} . \]
The general solution is
\[ h_{i0} = A + B\left(\tfrac12\right)^i , \]
since \(\rho = \frac13 / \frac23 = \frac12\). The initial condition \(h_{00} = 1\) gives \(A + B = 1\), so
\[ h_{i0} = 1 - B\left(1 - \left(\tfrac12\right)^i\right) , \]
and non-negative minimlality requires \(B = 1\). Hence
\[ h_{i0} = \left(\tfrac12\right)^i , \]
and \(m_0 = h_{10} = \frac12 < 1\), as required.

\end{myanswers}

\textbf{6.} \emph{``Every Markov chain on a finite state space has at least one stationary distribution.''} Explain carefully why this is true. You may use facts from the notes or previous example sheets, provided that you state them clearly.

\begin{myanswers}
\emph{Solution.} In Problem Sheet 4, Question 3, we showed that every finite-state Markov chain has a closed communicating class. In lectures, we showed the finite closed communicating classes are positive recurrent. If we look at the Markov chain restricted to just such a positive recurrent and closed class, then that restricted Markov chain is irreducible and positive recurrent, so it has a stationary distribution \(\boldsymbol\pi\). Since the class is closed, we have a stationary distribution that is \(\boldsymbol\pi\) on the given closed class and \(0\) elsewhere.

\end{myanswers}

\hypertarget{S11-long-term-chains}{%
\section{Long-term behaviour of Markov chains}\label{S11-long-term-chains}}

\begin{itemize}
\tightlist
\item
  The limit theorem: convergence to the stationary distribution for irreducible, aperiodic, positive recurrent Markov chains
\item
  The ergodic theorem for the long-run proportion of time spent in each state
\end{itemize}

\hypertarget{equilibrium}{%
\subsection{Convergence to equilibrium}\label{equilibrium}}

In this section we're interested in what happens to a Markov chain \((X_n)\) in the long-run -- that is, when \(n\) tends to infinity.

One thing that \emph{could} happen over time is that the distribution \(\mathbb P(X_n = i)\) of the Markov chain could gradually settle down towards some ``equilibrium'' distribution. Further, perhaps that long-term equilibrium might not depend on the initial distribution, but the effects of the initial distribution might eventually almost disappear, exhibiting a ``lack of memory'' of the start of the process.

Just in case that does happen, let's give it a name.

\begin{definition}
\protect\hypertarget{def:eq-dist}{}\label{def:eq-dist}

Let \((X_n)\) be a Markov chain on a state space \(\mathcal S\) with transition matrix \(\mathsf P\). Suppose there exists a distribution \(\mathbf p^* = (p_i^*)\) on \(\mathcal S\) (so \(p_i^* \geq 0\) and \(\sum_i p_i^* = 1\)) such that, whatever the initial distribution \(\boldsymbol\lambda = (\lambda_i)\), we have \(\mathbb P(X_n = j) \to p^*_j\) as \(n \to \infty\) for all \(j \in \mathcal S\). Then we say that \(\mathbf p^*\) is an \textbf{equilibrium distribution}.

\end{definition}

It's clear there can only be at most one equilibrium distribution -- but will there be one at all? The following is the most important result in this course. (Recall that an irreducible Markov chain is aperiodic if it has period 1.)

\begin{theorem}[Limit theorem]
\protect\hypertarget{thm:limit}{}\label{thm:limit}

Let \((X_n)\) be an irreducible and aperiodic Markov chain. Then for any initial distribution \(\boldsymbol\lambda\), we have that \(\mathbb P(X_n = j) \to 1/\mu_j\) as \(n \to \infty\), where \(\mu_j\) is the expected return time to state \(j\).

In particular:

\begin{itemize}
\tightlist
\item
  Suppose \((X_n)\) is positive recurrent. Then the unique stationary distribution \(\boldsymbol\pi\) given by \(\pi_j = 1/\mu_j\) is the equilibrium distribution, so \(\mathbb P(X_n = j) \to \pi_j\) for all \(j\).
\item
  Suppose \((X_n)\) is null recurrent or transient. Then \(\mathbb P(X_n = j) \to 0\) for all \(j\), and there is no equilibrium distribution.
\end{itemize}

\end{theorem}

I particularly like how this one theorem gathers together all the ideas from the course in one result (Markov chains, irreducibility, periodicity, recurrence/transience, positive/null recurrence, return times, stationary distribution\ldots).

Note the three conditions for convergence to an equilibrium distribution: irreducibility, aperiodicity, and positive recurrence.

Consider a irreducible, aperiodic, positive recurrent Markov chain. Taking the initial distribution to be starting in state \(i\) with certainty, the limit theorem tells us that \(p_{ij}(n) \to \pi_j\) for all \(i\) and \(j\). This means that the \(n\)-step transition matrix will have the limiting value
\[ \lim_{n \to \infty} \mathsf P(n) = \begin{pmatrix}
     \pi_1 & \pi_2 & \cdots & \pi_N \\
     \pi_1 & \pi_2 & \cdots & \pi_N \\
     \vdots & \vdots & \vdots & \vdots \\
     \pi_1 & \pi_2 & \cdots & \pi_N \end{pmatrix} , \]
where each row is identical.

We give an \protect\hyperlink{S11-proofs}{full proof of the limit theorem} below (optional and nonexaminable). However, this easier result gets part way there.

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-15}{}\label{thm:unlabeled-div-15}

If an equilibrium distribution \(\mathbf p^*\) does exist, then \(\mathbf p^*\) is a stationary distribution.

\end{theorem}

Given this result it's clear that an irreducible Markov chain cannot have an equilibrium distribution if it is null recurrent or transient, as it doesn't even have a stationary distribution. So the positive recurrent case is the hard (nonexaminable) one.

\begin{proof}

We need to verify that \(\mathbf p^* \mathsf P = \mathbf p^*\). We have
\[ \sum_i p_i^* p_{ij} = \sum_i \left(\lim_{n\to\infty} p_{ki}(n) \right) p_{ij} = \lim_{n\to\infty} \sum_i p_{ki}(n) p_{ij} = \lim_{n\to\infty} p_{kj}(n+1) = p^*_j , \]
as desired.

\end{proof}

(Strictly speaking, swapping the sum and the limit is only formally justified when the state space is finite, although the theorem is true universally.)

\hypertarget{convergence-examples}{%
\subsection{Examples of convergence and non-convergence}\label{convergence-examples}}

\begin{example}
\protect\hypertarget{exm:conv1}{}\label{exm:conv1}

The \protect\hyperlink{S05-example}{two-state ``broken printer'' Markov chain} is irreducible, aperiodic, and positive recurrent, so its stationary distribution is also the equilibrium distribution. We proved this from first principles in \protect\hyperlink{P03}{Question 3 on Problem Sheet 3}.

\end{example}

\begin{example}
\protect\hypertarget{exm:conv2}{}\label{exm:conv2}

Recall \protect\hyperlink{S06-example1}{the simple no-claims discount Markov chain from Lecture 6}, which is irreducible, aperiodic, and positive recurrent. We saw last time that is has the unique stationary distribution
\[ \boldsymbol\pi = \left(\tfrac1{13} \quad \tfrac{3}{13}\quad \tfrac9{13}\right) = (0.0769\quad 0.2308\quad 0.6923) . \]

From the limit theorem, we see that the \(n\)-step transition probability tends to a limit where every row is equal to \(\boldsymbol\pi\). We can check using a computer: for \(n = 12\), say,
\[ \mathsf P(12) = \mathsf P^{12} = \begin{pmatrix} 0.0770 & 0.2308 & 0.6923 \\ 0.0769 & 0.2308 & 0.6923 \\ 0.0769 & 0.2308 & 0.6923 \end{pmatrix}, \]
where \(p_{ij}(12)\) is equal to \(\pi_j\) up to at least 3 decimal places for all \(i,j\). As \(n\) gets bigger, the matrix gets closer and closer to the limiting form.

\end{example}

\begin{example}
\protect\hypertarget{exm:conv-rw}{}\label{exm:conv-rw}

The simple random walk is null recurrent for \(p = \frac12\) and transient otherwise. Either way, we have \(\mathbb P(X_n = i) \to 0\) for all states \(i\), and there is no equilibrium distribution.

\end{example}

\begin{example}
\protect\hypertarget{exm:conv4}{}\label{exm:conv4}

Consider a Markov chain \((X_n)\) on state space \(\mathcal S = \{0,1\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} . \]
So at each stage we swap from state \(0\) to state \(1\) and back again. This chain is irreducible and positive recurrent, so it has a unique stationary distribution, which is clearly \(\boldsymbol\pi = (\frac12\quad\frac12)\).

However, we don't have convergence to equilibrium. If we start from initial distribution \((\lambda_0, \lambda_1)\), then \(\mathbb P(X_n = 0) = \lambda_0\) for even \(n\) and \(\mathbb P(X_n = 0) = \lambda_1\) for odd \(n\). When \(\lambda_0 \neq \frac12\), this does not converge.

The point is that this chain is \emph{not} aperiodic: it has period \(2\), so the limit theorem does not apply.

\end{example}

\begin{example}
\protect\hypertarget{exm:conv5}{}\label{exm:conv5}

Consider a Markov chain with state space \(\mathcal S = \{1,2,3\}\) and transition diagram as shown below.

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/stat-ex-1} 

}

\caption{Transition diagram for a Markov chain with two positive recurrent classes.}\label{fig:stat-ex}
\end{figure}

This chain is not irreducible, but has two aperiodic and positive recurrent communicating classes. In particular, it has many stationary distributions including \((1, 0, 0)\) and \((0, \frac8{17}, \frac9{17})\) (optional exercise for the reader). If we start in state 1, then the limiting distribution is the former, while if we start in states 2 or 3, the limiting distribution is the latter.

In particular, as \(n \to \infty\), we have
\[ \mathsf P^{(n)} \to \begin{pmatrix} 1 & 0 & 0 \\ 0 & \frac8{17} & \frac9{17} \\ 0 & \frac8{17} & \frac9{17} \end{pmatrix}. \]

\end{example}

\hypertarget{S11-ergodic}{%
\subsection{Ergodic theorem}\label{S11-ergodic}}

The limit theorem looked at the limit of \(\mathbb P(X_n = j)\), the probability that the Markov chain is in state \(j\) at some specific point in time \(n\) a long time in the future. We could also look at the \emph{long-run amount of time} spent in state \(j\); that is, averaging the behaviour over a long time period. (The word ``ergodic'' is used in mathematics to refer to concepts to do with the long-term proportion of time.)

Let us write
\[ V_j(N) := \# \big\{ n < N : X_n = j \} \]
for the total number of visits to state \(j\) up to time \(N\). Then we can interpret \(V_j(n)/n\) as the proportion of time up to time \(n\) spent in state \(j\), and its limiting value (if it exists) to be the \textbf{long-run proportion of time} spent in state \(j\).

\begin{theorem}[Ergodic theorem]
\protect\hypertarget{thm:ergodic}{}\label{thm:ergodic}

Let \((X_n)\) be an irreducible Markov chain. Then for any initial distribution \(\boldsymbol\lambda\) we have that \(V_j(n)/n \to 1/\mu_j\) almost surely as \(n \to \infty\), where \(\mu_j\) is the expected return time to state \(j\).

In particular:

\begin{itemize}
\tightlist
\item
  Suppose \((X_n)\) is positive recurrent. Then there is a unique stationary distribution \(\boldsymbol\pi\) given by \(\pi_j = 1/\mu_j\), and \(V_j(n)/n \to \pi_j\) almost surely for all \(j\).
\item
  Suppose \((X_n)\) is null recurrent or transient. Then \(V_j(n)/n \to 0\) almost surely for all \(j\).
\end{itemize}

\end{theorem}

For completeness, we should note that ``almost sure'' convergence means that \(\mathbb P(V_j(n)/n \to 1/\mu_j) = 1\), although the precise definition is not important for us in this module.

Note that, because we are averaging over a long-time period, we no longer need the condition that the Markov chain is aperiodic; for convergence of the long-term proportion of time to the stationary distribution we just need irreducibility and positive recurrence.

Again, we give an optional and nonexaminable proof \protect\hyperlink{S11-proofs}{below}.

\begin{example}
\protect\hypertarget{exm:ergodic-ex}{}\label{exm:ergodic-ex}

Recall \protect\hyperlink{S06-example1}{the simple no-claims discount Markov chain}. Since this chain is irreducible and positive recurrent, we now see that the long-term proportion of time spent in each state corresponds to the stationary distribution \(\boldsymbol\pi = (\frac1{13} \quad \frac{3}{13}\quad \frac9{13})\). Therefore, over the lifetime of an insurance policy held for a long period of time, the average discount is approximately
\[ \tfrac{1}{13}(0\%) + \tfrac{3}{13}(25\%) + \tfrac{9}{13}(50\%) = \tfrac{21}{52} = 40.4\% . \]

\end{example}

\begin{example}
\protect\hypertarget{exm:ergodic-ex2}{}\label{exm:ergodic-ex2}

The two-state ``swapping'' chain we saw earlier did have a unique stationary distribution \((\frac12, \frac12)\), but did not have an equilibrium distribution, because it was periodic. But it is true that \(V_0(n)/n \to \pi_0 = \frac12\) and \(V_1(n)/n \to \pi_1 = \frac12\), due to the ergodic theorem. So although where we are at some specific point in the future depends on where we started from, in the long run we always spend half our time in each state.

\end{example}

\hypertarget{S11-proofs}{%
\subsection{Proofs of the limit and ergodic theorems}\label{S11-proofs}}

\emph{This subsection is optional and nonexaminable.}

Again, it's important to be able to use the limit and ergodic theorems, but less important to be able to prove them.

First, the limit theorem. The only bit left is the first part: that for an irreducible, aperiodic, positive recurrent Markov chain, the stationary distribution \(\boldsymbol\pi\) is an equilibrium distribution.

This cunning proof uses a technique called ``coupling''. When looking at two different random objects \(X\) and \(Y\) (like random variables or stochastic processes), it seems natural to prefer \(X\) and \(Y\) to be independent. However, \textbf{coupling} is the idea that it can sometimes be beneficial to let \(X\) and \(Y\) actually be dependent on each other.

\begin{proof}[Proof of Theorem \ref{thm:limit}]

Let \((X_n)\) be our irreducible, aperiodic, positive recurrent Markov chain with transition matrix \(\mathsf P\) and initial distribution \(\boldsymbol\lambda\). Let \((Y_n)\) be a Markov chain also with transition matrix \(\mathsf P\) but ``in equilibrium'' -- that is, started from the stationary distribution \(\boldsymbol\pi\), and thus staying in that distribution for ever.

Pick a state \(s \in \mathcal S\), and let \(T\) be the first time the \(X_n = Y_n = s\) (or \(T = \infty\), if that never happens). Now here's the coupling: after \(T\), when \((X_n)\) and \((Y_n)\) collide at \(s\), then make \((X_n)\) stick to \((Y_n)\), so \(X_n = Y_n\) for \(n \geq T\). Since a Markov chain has no memory, \((X_{T+n}) = (Y_{T+n})\) is still just a Markov chain with the same transition probabilities from that point on. (Readers of \protect\hyperlink{S09-strong-markov}{a previous optional subsection} will recognise \(T\) as a stopping time and will notice we're using the strong Markov property.) Most importantly, thanks to the coupling, from the time \(T\) onwards, \((X_n)\) will also always have distribution \(\boldsymbol\pi\), a fact that will obviously be very useful in this proof.

It will be important that \(T\) is finite with probability 1. Define \((\mathbf Z_n)\) by \(\mathbf Z_n = (X_n, Y_n)\). So \((\mathbf Z_n)\) is a Markov chain on \(\mathcal S \times \mathcal S\), and \(T\) is the expected hitting time of \((\mathbf Z_n)\) to the state \((s, s) \in \mathcal S \times \mathcal S\). The transition probabilities for \((\mathbf Z_n)\) are \(\mathsf{\tilde P} = (\tilde p_{(i,k)(j,l)})\) where
\[ \tilde p_{(i,k)(j,l)} = p_{ij}p_{kl} . \]
This is the probability that the joint chain goes from \(\mathbf Z_n = (X_n, Y_n) = (i, k)\) to \(\mathbf Z_{n+1} = (X_{n+1}, Y_{n+1}) = (j, l)\).

Since the original Markov chain is irreducible and aperiodic, this means that \(p_{ij}(n), p_{kl}(n) > 0\) for all \(n\) sufficiently large, so \(\tilde p_{(i,k)(j,l)}(n) > 0\) for all \(n\) sufficiently large also, meaning that \((\mathbf Z_n)\) is irreducible (and, although this isn't required, aperiodic). Further, \((\mathbf Z_n)\) has a stationary distribution \(\mathbf{\tilde \pi} = (\tilde \pi_{(i,k)})\) where
\[ \tilde \pi_{(i,k)} = \pi_i \pi_k , \]
which means that \((\mathbf Z_n)\) is positive recurrent. Thus \(T\) is finite with probability 1.

So we can finally prove the limit theorem. We want to show that \(\mathbb P(X_n = i)\) tends to \(\pi_i\). The difference between them is
\begin{align*}
\big|\mathbb P(X_i = i) - \pi_i\big|
&= \mathbb P(n \leq T)\times\big|\mathbb P(X_i = i \mid n \leq T) - \pi_i\big|  + P(n > T) \times |\pi_i - \pi_i| \\
&= \mathbb P(n \leq T)\times\big|\mathbb P(X_i = i \mid n \leq T) \\
&\leq \mathbb P(n \leq T) .
\end{align*}
Here, the equality on the first line is because \((X_n)\) follows the stationary distribution exactly once it sticks to \((Y_n)\) after time \(T\), and the inequality on the third line is because the absolute difference between two probabilities is between 0 and 1. But we've already shown that \(T\) is finite with probability 1, so \(\mathbb P(n \leq T) = \mathbb P(T \geq n) \to 0\), and we're done.

\end{proof}

The proof of the ergodic theorem uses the law of large numbers. Recall that the law of large numbers states that if \(Y_1, Y_2, \dots\) are IID random variables with mean \(\mu\), then
\[ \frac{Y_1 + Y_2 + \cdots + Y_n}{n} \to \mu \qquad \text{as $n \to \infty$}. \]
This means it's also true that for any sequence \((a_n)\) with \(a_n \to \infty\), we also have
\[ \frac{Y_1 + Y_2 + \cdots + Y_{a_n}}{a_n} \to \mu \qquad \text{as $n \to \infty$}. \]

\begin{proof}[Proof of Theorem \ref{thm:ergodic}]

If \((X_n)\) is transient, then the number of visits to state \(i\) is finite with probability 1, so \(V_i(n)/n \to 0\), as required.

Suppose instead that \((X_n)\) is recurrent. By our \protect\hyperlink{S09-lemma}{useful lemma} we know we will hit \(i\) in finite time, so we can ignore that negligible ``burn-in'' period, and (by the strong Markov property) assume we start from \(i\). Let \(M_{i}^{(r)}\) be the time between the \(r\)th and \((r+1)\)th visits to \(i\). Note that the \(M_{i}^{(r)}\) are IID with mean \(\mu_i\).

The time of the last visit to \(i\) before time \(n\) is
\[ M_{i}^{(1)} + M_{i}^{(2)} + \cdots + M_{i}^{(V_i(n)-1)} < n ,\]
and the time of the first visit to \(i\) after time \(n\) is
\[ M_{i}^{(1)} + M_{i}^{(2)} + \cdots + M_{i}^{(V_i(n))} \geq n .\]
Hence
\begin{equation}
\frac{M_{i}^{(1)} + M_{i}^{(2)} + \cdots + M_{i}^{(V_i(n)-1)}}{V_i(n)} < \frac{n}{V_i(n)} \leq \frac{M_{i}^{(1)} + M_{i}^{(2)} + \cdots + M_{i}^{(V_i(n))}}{V_i(n)} . \label{eq:erg}
\end{equation}

Because \((X_n)\) is recurrent, we keep returning to \(i\), so \(V_i(n) \to \infty\) with probability 1. Hence, by the law of large numbers, both the left- and right-hand sides of \eqref{eq:erg} tend to \(\mathbb E M_{i}^{(r)} = \mu_i\). So \(n/V_i(n)\) is sandwiched between them, and tends to \(\mu_i\) too. Finally \(n/V_i(n) \to \mu_i\) is equivalent to \(V_i(n)/n \to 1/\mu_i\), so we are done.

\end{proof}

This completes the material on discrete time Markov chains. \textbf{In the next section}, we recap what we have learned, and have a little time for some revision.

\hypertarget{S12-revision-i}{%
\section{End of of Part I: Discrete time Markov chains}\label{S12-revision-i}}

\begin{itemize}
\tightlist
\item
  No new material in this section, but a half-week break to catch up and take stock on what we've learned
\end{itemize}

\hypertarget{todo-revision}{%
\subsection{Things to do}\label{todo-revision}}

We've now finished the material of the Part 1 of the module, on discrete time Markov chains. So this is a good time to take stock, revise what we've learned, and make sure we're completely up to date before starting Part 2 of the module on continuous time processes.

Some things you may want to do in lieu of reading a section of notes:

\begin{itemize}
\tightlist
\item
  Make sure you've completed \textbf{Problem Sheets 1 to 6}, and go back to any questions that stumped you before.
\item
  Start working on \textbf{\protect\hyperlink{computing}{Computational Worksheet 2}} (which doubles as \textbf{Assessment 2}). There are optional computer drop-in sessions in Week 7, and the work is due on Thursday 18 March 1400 (week 8).
\item
  Start working on \textbf{\protect\hyperlink{A3}{Assessment 3}} which is due on Thursday 25 March 1400 (week 9).
\item
  Re-read any sections of notes you struggled with earlier.
\item
  Take the opportunity to look at the optional nonexaminable subsections, if you opted out the first time around. (\protect\hyperlink{S09-strong-markov}{Section 9}, \protect\hyperlink{S10-proof}{Section 10}, \protect\hyperlink{S11-proofs}{Section 11})
\item
  \href{mailto:m.aldridge@leeds.ac.uk}{Let me know} if there's anything from this part of the course you'd like me to go through in next week's lecture.
\end{itemize}

\hypertarget{summary-i}{%
\subsection{Summary of Part 1}\label{summary-i}}

The following list is not exhaustive, but if you can do most of the things on this list, you're well placed for the exam.

\begin{itemize}
\tightlist
\item
  Define the simple random walk and other random walks.
\item
  Perform elementary calculations for the simple random walk, including by referring to the exact binomial distribution.
\item
  Calculate the expectation and variance of general random walks.
\item
  Define the gambler's ruin Markov chain.
\item
  Find the ruin probability and expected duration for the gambler's ruin by (i) setting up equations by conditioning on the first step and (ii) solving the resulting linear difference equation.
\item
  Draw a transition diagram of a Markov chain given the transition matrix.
\item
  Calculate \(n\)-step transitions probabilities by (a) summing the probabilities over all relevant paths or (b) calculating the matrix power.
\item
  Find the communicating classes in a Markov chain.
\item
  Calculate the period of communicating class.
\item
  Calculate hitting probabilities and expected hitting times by (i) setting up equations by conditioning on the first step and (ii) solving the resulting simultaneous equations.
\item
  Define positive recurrence, null recurrence and transience, and explain their properties.
\item
  Find the positive recurrence, null recurrence or transience of communicating classes.
\item
  Find the stationary distribution of Markov chain.
\item
  Give conditions for a stationary distribution to exist and be unique.
\item
  Give conditions for convergence to an equilibrium distribution.
\item
  Calculate long-term proportions of time using the ergodic theorem.
\end{itemize}

\textbf{In the next section}, we begin our study of continuous time Markov processes by looking at the most important example: the Poisson process.

\hypertarget{P06}{%
\section*{Problem sheet 6}\label{P06}}
\addcontentsline{toc}{section}{Problem sheet 6}

\commtrue

You should attempt all these questions and write up your solutions in advance of your workshop in week 7 (Monday 8 or Tuesday 9 March) where the answers will be discussed.

Remember that the \textbf{\href{https://forms.office.com/Pages/ResponsePage.aspx?id=qO3qvR3IzkWGPlIypTW3ywARQdZlKXRHsLcXi_ngX8NUM0NLOFFaQURFVExJMklIRjdVM04wQk8xTy4u}{mid-semester survey}} is still open.

\textbf{1.} Consider a Markov chain \((X_n)\) with state space \(\mathcal S =\{1,2,3\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{pmatrix}.   \]

\textbf{(a)} Draw a transition diagram for this Markov chain. Is it irreducible? Is each state periodic or aperiodic?

\begin{myanswers}

\emph{Solution.}

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q61-1} 

}

\caption{Transition diagram for Question 1.}\label{fig:Q61}
\end{figure}

The Markov chain is irreducible, since we have \(1 \to 2 \to 3 \to 1\). Since we continually cycle around the triangle, it's clear that the period is \(d = 3\).

\end{myanswers}

\textbf{(b)} What is \(m_i\), the return probability, for each state? What is \(\mu_i\), the expected return time for each state.

\begin{myanswers}
\emph{Solution.}
Again, since we continually cycle around the triangle, we always return in \(3\) steps, so \(m_i = 1\) and \(\mu_i =3\) for all \(i\).

\end{myanswers}

\textbf{(c)} By solving \(\boldsymbol \pi = \boldsymbol\pi \mathsf P\), find the stationary distribution. Use this to confirm the values of \(\mu_i\).

\begin{myanswers}
\emph{Solution.}
The equations give \(\pi_1 = \pi_3\), \(\pi_2 = \pi_1\) and \(\pi_3 = \pi_2\), meaning they're all equal. The normalising condition gives \(\boldsymbol\pi = (\frac13, \frac13, \frac13)\). The expected return times are \(\mu_i = 1/\pi_i = 3\), as predicted.

\end{myanswers}

\textbf{(d)} For what initial distributions \(\boldsymbol\lambda\) do the limits \(\lim_{n \to \infty} \mathbb P(X_n = i)\) exist?

\begin{myanswers}
\emph{Solution.}
For a given initial condition \((\lambda_1, \lambda_2, \lambda_3)\) it's clear we cycle through the initial condition, \((\lambda_2, \lambda_3, \lambda_1)\), and \((\lambda_3, \lambda_1, \lambda_2)\). Hence the limits only exist if \(\lambda_1 = \lambda_2 = \lambda_3 = \frac13\).

\end{myanswers}

\textbf{(e)} What is the long-run proportion of time spent in each state?

\begin{myanswers}
\emph{Solution.}
Since the Markov chain is irreducible and positive recurrent (like every finite irreducible chain), the ergodic theorem tells us that the long-run proportion of time spent in each state \(i\) is \(\pi_i = \frac13\).

\end{myanswers}

\textbf{2.} Every person has two chromosomes; each chromosome is a copy of a chromosome from one of the person's parents. There are two types of chromosome, which are conventionally labelled X and Y. A child born with a Y chromosome is male, while a child with two X chromosomes is female.

Haemophilia is a blood-clotting disorder caused by a defective X chromosome (we will label this as \(\text X^*\)). Females with the defective chromosome (\(\text X^*\text X\)) will not typically show symptoms of the disease but can pass it on to children -- they are ``carriers''. Males with the defective chromosome (\(\text X^* \text Y\)) have the disease and its symptoms.

A medical statistician is studying the progress of the disease through first-born children, starting with a female carrier. The statistician makes the following assumptions: First, each parent has an equal probability of passing either of their chromosomes to their children. Second, the partner of each person in the study does not have a defective X chromosome. Third, no new genetic disorders occur.

\textbf{(a)} Show that we can use a Markov chain to model the progress of the disease under the above assumptions. What is the state space? Draw a transition diagram.

\begin{myanswers}

\emph{Solution.}
Consider a stochastic process on the state space \(\mathcal S = \{\text{F}, \text{M}, \text{m}, \text{f}\}\), where F means the first-born child is a female carrier, M means the child is a male haemophiliac, f means the child is a female non-carrier, and m means the child is a male without the disease. (This is not the only way to set up the Markov chain.) If we let \((X_n)\) be the status of the first-born child at the \(n\)th generation, then it is clear that \(X_{n+1}\), the status of the \((n+1)\)st individual, will depend on the status of their parent \(X_n\), but, given that, will not depend further on the history of the process. So we have the Markov property.

The transition probabilities are: From a female carrier, \(p_{\mathrm{FF}} = \frac14\) or \(p_{\mathrm{FM}} = \frac14\) if she passes on the \(\text X^*\), or \(p_{\mathrm{Ff}} = p_{\mathrm{Fm}} = \frac14\) if not. From a male haemophiliac, \(p_{\mathrm{MF}} = \frac12\) if he passes on the \(\text X^*\), or \(p_{\mathrm{Mm}} = \frac12\) if not. From those without the \(\text X^*\), we have \(p_{\mathrm{ff}} = p_{\mathrm{fm}} = p_{\mathrm{mf}} =p_{\mathrm{mm}} = \frac12\).

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q62-1} 

}

\caption{Transition diagram for Question 2.}\label{fig:Q62}
\end{figure}

\end{myanswers}

\textbf{(b)} What are the communicating classes in the chain? Is each class positive recurrent, null recurrent, or transient?

\begin{myanswers}
\emph{Solution.}
Note we can move from F or M to f or m but not back again. So the class \(\{\text F, \text M\}\) is non-closed and thus transient, while the class \(\{\text f, \text m\}\) is closed and thus positive recurrent.

\end{myanswers}

\textbf{(c)} Calculate a stationary distribution. Is this the only stationary distribution?

\begin{myanswers}
\emph{Solution.}
We have a stationary distribution is \(\pi_{\mathrm M} = \pi_{\mathrm{F}} = 0\), \(\pi_{\mathrm m} = \pi_{\mathrm f} = \frac12\). One way to see this is to solve \(\boldsymbol\pi = \boldsymbol\pi\mathsf P\). Another way is the following: Since F and M are transient we must have \(\pi_{\mathrm M} = \pi_{\mathrm{F}} = 0\). But, within the recurrent class, m and f are symmetrical, so we must have \(\pi_{\mathrm m} = \pi_{\mathrm f}\). The result follows.

Since there is exactly one positive recurrent class, the stationary distribution is unique.

\end{myanswers}

\textbf{(d)} Under this model, what is the limiting probability that, in many generations' time, a child has haemophilia?

\begin{myanswers}
\emph{Solution.}
Since M and F are transient states, we have \(\lim \mathbb P(X_n = \mathrm F) = \lim \mathbb P(X_n = \mathrm M) = 0\), so the limiting probability is \(0\).

\end{myanswers}

\textbf{3.}
An airline operates a frequent flyer scheme with four classes of membership; Ordinary, Bronze, Silver and Gold. Scheme members get benefits according to their membership class.
Changing membership class operates as follows:

\begin{itemize}
\tightlist
\item
  If a member books two or more flights in a given year, they are moved up a class of membership for the next year (or remain at Gold).
\item
  If a member books a single flight, they remain in their current class in the following year.
\item
  If a member books no flights, they move down a class (or remain at Ordinary).
\end{itemize}

The airline's research has shown that in a given year 40\% of members book no flights, 40\% book exactly one flight and the remaining 20\% book two or more flights, independent of their history. Moreover, the cost of running the scheme per member is estimated as £0 for Ordinary members, £10 for Bronze members, £20 for Silver members, and £30 for Gold members.

\textbf{(a)} Show that this system can be modelled using a Markov chain. Write down the transition probabilities and draw a transition diagram.

\begin{myanswers}

\emph{Solution.}
We write \(\mathcal S = \{\text{O},\text{B},\text{S},\text{G}\}\) for the set of states, and let \(X_n\) be the membership level in year \(n\). Next year's membership level depends on this year's, but not, given this year's, on the previous history, so we have the Markov property.

With probability \(40\%\), a member books no flights, giving
\[ p_{\mathrm{BO}} = p_{\mathrm{SB}} = p_{\mathrm{GG}} = 0.4 . \]
With probability \(40\%\), a member books one flights, giving
\[ p_{\mathrm{BB}} = p_{\mathrm{SS}} = 0.4  \]
and \(p_{\mathrm{OO}} = 0.4+0.4 = 0.8\).
With probability \(20\%\), a member books two or more flights, giving
\[ p_{\mathrm{OB}} = p_{\mathrm{BS}} = p_{\mathrm{SG}} = 0.2  \]
and \(p_{\mathrm{GG}} = 0.4+0.2 = 0.6\).

\begin{figure}

{\centering \includegraphics{math2750_files/figure-latex/Q63-1} 

}

\caption{Transition diagram for Question 3.}\label{fig:Q63}
\end{figure}

\end{myanswers}

\textbf{(b)} Explain why a unique stationary distribution exists and calculate it.

\begin{myanswers}
\emph{Solution.}
The Markov chain is irreducible and positive recurrent, so has a unique stationary distribution.

The equations from \(\boldsymbol\pi = \boldsymbol\pi \mathsf P\) are
\begin{align}
    \pi_{\mathrm O} &= 0.8 \pi_{\mathrm O} + 0.4 \pi_{\mathrm B } \\
    \pi_{\mathrm B} &= 0.2\pi_{\mathrm O} + 0.4\pi_{\mathrm B} + 0.4\pi_{\mathrm S} \\
    \pi_{\mathrm S} &= \phantom{0.2 \pi_{\mathrm S} +} 0.2 \pi_{\mathrm B} + 0.4 \pi_{\mathrm S} + 0.4 \pi_{\mathrm G} \\
    \pi_{\mathrm G} &= \phantom{0.2 \pi_{\mathrm S} + 0.2 \pi_{\mathrm B} +} 0.2 \pi_{\mathrm S} + 0.6 \pi_{\mathrm G} .
    \end{align}
We will choose \(\pi_{\mathrm B}\) as the working variable and discard the last equation. Rearranging the other three equations gives
\begin{align*}
\pi_{\mathrm O} \phantom{{}+ 2\pi_{\mathrm S} - 2\pi_{\mathrm G}} &= 2\pi_{\mathrm B} \\
\pi_{\mathrm O} + 2\pi_{\mathrm S} \phantom{{}- 2\pi_{\mathrm G}} &= 3\pi_{\mathrm B} \\
3\pi_{\mathrm S} - 2\pi_{\mathrm G} &= \pi_{\mathrm B} 
\end{align*}
Substituting the first of these into the second and rearranging gives \(\pi_{\mathrm S} = \frac12 \pi_{\mathrm B}\). Substituting this into the third and rearranging gives \(\pi_{\mathrm S} = \frac14 \pi_{\mathrm B}\)

The normalising condition is
\[ \pi_{\mathrm O} + \pi_{\mathrm B} + \pi_{\mathrm S} + \pi_{\mathrm G} = \left(2 + 1 + \tfrac12 + \tfrac14 \right)\pi_{\mathrm B} = \tfrac{15}{4} \pi_{\mathrm B} = 1 . \]
Hence \(\pi_{\mathrm B} = \frac{4}{15}\). Back-solving, we get the solution
\[ \left(\pi_{\mathrm O},\pi_{\mathrm B},\pi_{\mathrm S},\pi_{\mathrm G}\right) = \left(\tfrac8{15},\tfrac{4}{15},\tfrac{2}{15},\tfrac{1}{15} \right) . \]

\end{myanswers}

\textbf{(c)} The airline makes a profit of £10 per passenger per flight, before the cost of the frequent flyer scheme. In the long run, does the airline expect to remain in profit after the cost of the scheme?

\begin{myanswers}
\emph{Solution.}
By the ergodic theorem, the long run time spent in state \(x\) is \(\pi_x\). So the long-run cost per member is
\[ 0\pi_{\mathrm O} + 10\pi_{\mathrm B} + 20 \pi_{\mathrm S} + 30\pi_{\mathrm G}
    = 0 \cdot\tfrac8{15} + 10\cdot\tfrac{4}{15} + 20\cdot\tfrac{2}{15} + 30 \cdot \tfrac{1}{15} = \tfrac{110}{15} = \tfrac{22}{3} , \]
for a cost of \pounds 7.33.
The average number of flights taken per member is at least
\[ 0.4\times 0 + 0.4 \times 1 + 0.2\times 2 = 0.8 , \]
for a profit of at least \(10\times0.8 = \text{\pounds}8\). (``At least'' because the probability \(0.2\) refers to ``two or more'' flights.) Since \(8 >7.33\), the airline will make a profit in the long run.

\end{myanswers}

\textbf{4.} We have \(N\) balls, each of which is placed into one of two urns. At each time step, a ball is chosen uniformly at random and moved to the other urn.

\textbf{(a)} Show that the stationary probability the first urn contains \(i\) balls is
\[ \frac{1}{2^N} \binom{N}{i} . \]

\begin{myanswers}
\emph{Solution.} One way to solve this is to let \(X_n\) be the number of balls in the first urn after having moved \(n\) balls. This has transition probabilities
\[ p_{i,i+1} = 1 - \frac{i}{N} \qquad p_{i,i-1} = \frac{i}{N} . \]
The equations for the stationary distribution are
\[ \pi_i = \left(1 - \frac{i-1}{N}\right)\pi_{i-1} + \frac{i+1}N \pi_{i+1} . \]
One can check that \(\pi_i = C \binom{N}{i}\) satisfies this for any constant \(C\) by using the combinatorial identities
\begin{align*}
\left(1 - \frac{i-1}{N}\right)\binom{N}{i-1} &= \binom{N-1}{i-1} \\
\frac{i+1}N \binom{N}{i+1} &= \binom{N-1}{i} \\
\binom{N-1}{i-1} + \binom{N-1}{i} &= \binom Ni ,
\end{align*}
and check that the normalising condition demands \(C = 1/2^N\) because of
\[ \sum_{i=0}^N \binom Ni = 2^N . \]

The following is perhaps a better way. Let \(Y_n^j = 1\) denote that ball \(j\) is in the first urn at time \(n\), and \(Y_n^j = 2\) denote that it is in the second urn. Let \(\mathbf Y_n = (Y_n^1, Y_n^2, \dots, Y_n^N)\). Then \((\mathbf Y_n)\) is a Markov chain on the state space \(\mathcal S = \{1,2\}^N\). The transition probabilities are that \(p_{\mathbf{yz}} = 1/N\) for any \(\mathbf y, \mathbf z \in \mathcal S\) that differ in exactly one of the \(N\) coordinates. Because of the symmetry, it's clear that we have a stationary distribution \(\boldsymbol\phi = (\phi_{\mathbf y})\) where \(\phi_{\mathbf y} = 1/|\mathcal S| = 1/2^N\) for all \(\mathbf y \in \mathcal S\). Since the Markov chain is irreducible and positive recurrent, this is the only stationary distribution. Hence the stationary probability the first urn contains \(i\) balls is
\[ \sum_{\mathbf y \in \mathcal S(i)} \phi_{\mathbf y}  = \big|\mathcal S(i)\big|\, \frac{1}{2^N} = \binom{N}{i} \frac 1{2^N} , \]
where \(\mathcal S(i)\) is the set of \(\mathbf y \in \mathcal S\) consisting of \(i\) \(1\)s and \(N-i\) \(2\)s.

\end{myanswers}

\textbf{(b)} Is this an equilibrium distribution?

\begin{myanswers}
\emph{Solution.} No.~The number of balls in the left-hand urn switches between odd and even each turn, so the chain is periodic with period 2, and does not have an equilibrium distribution.

\end{myanswers}

\textbf{(c)}
In the long run, for what proportion of time are all of the balls in the same urn?

\begin{myanswers}
\emph{Solution.} By the ergodic theorem, the long run proportion of time that all the balls are in the same urn is
\[ \pi_0 + \pi_N = \phi_{(1,1,\dots,1)} + \phi_{(2,2,\dots,2)} = \frac{1}{2^N} + \frac{1}{2^N} = \frac{1}{2^{N-1}} . \]

\end{myanswers}

\hypertarget{A3}{%
\section*{Assessment 3}\label{A3}}
\addcontentsline{toc}{section}{Assessment 3}

This assessment counts as 4\% of your final module grade. You should attempt both questions. You must show your working, and there are marks for the quality of your mathematical writing.

The deadline for submission is \textbf{Thursday 25 March at 2pm}. Submission will be to Gradescope via Minerva, from Monday 21 March. It would be helpful to start your solution to Question 2 on a new page. If you hand-write your solutions and scan them using your phone, please convert to PDF using a scanning app (I like Microsoft Office Lens or Adobe Scan) rather than submit images.

Late submissions up to Wednesday 31 March at 2pm will still be marked, but the total mark will be reduced by 10\% per day or part-day for which the work is late. Submissions are not permitted after Wednesday 31 March.

Your solutions to this assessment should be your own work. Copying, collaboration or plagiarism are not permitted. Asking others to do your work, including via the internet, is not permitted. Transgressions are considered to be a very serious matter, and will be dealt with according to the University's disciplinary procedures.

\textbf{1.}
A firm rents out cars and operates from three locations -- the Airport, the Beach and the
City. Customers may return vehicles to any of the three locations.
The company estimates that the probability of a car being returned to each location is
as follows:

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{assessments/car-hire} 

}

\caption{Table of car-hire data.}\label{fig:car-table}
\end{figure}

\textbf{(a)} A car is currently parked at the beach. What is the probability that, after being hired twice, it ends up at the airport? {{[}2 marks{]}}

\textbf{(b)} A car is parked in the city. On average, how many times will it need to be hired until it is left at the beach? {{[}2{]}}

\textbf{(c)} The firm has been running for many years with a fleet of 25 cars, of which 7 are currently on hire. How many of the other cars would you expect to be parked at each of the three locations? Explain your answer clearly. {{[}4{]}}

\textbf{2.} Consider a Markov chain \((X_n)\) with state space \(\mathcal S = \{1,2,3,4,5\}\) and transition matrix
\[ \mathsf P = \begin{pmatrix} 0.2 & 0.8 & 0   & 0   & 0   \\
                               0.4 & 0.6 & 0   & 0   & 0   \\
                               0.2 & 0.1 & 0.2 & 0.5 & 0   \\
                               0   & 0   & 0   & 0.4 & 0.6 \\
                               0   & 0   & 0   & 0.8 & 0.2 \end{pmatrix}. \]

\textbf{(a)} What are the communicating classes in the Markov chain? State, giving brief reasons, whether each class is positive recurrent, null recurrent, or transient. {{[}3{]}}

\textbf{(b)} Calculate the hitting probability \(h_{34}\). {{[}2{]}}

\textbf{(c)} Find two different stationary distributions for this Markov chain. {{[}3{]}}

\textbf{(d)} What is the limit as \(n \to \infty\) of the \(n\)-step transition matrix \(\mathsf P(n)\)? Explain your answer. {{[}4{]}}

\hypertarget{part-part-ii-continuous-time-markov-jump-processes}{%
\part*{Part II: Continuous time Markov jump processes}\label{part-part-ii-continuous-time-markov-jump-processes}}
\addcontentsline{toc}{part}{Part II: Continuous time Markov jump processes}

\hypertarget{S13-poisson-poisson}{%
\section{Poisson process with Poisson increments}\label{S13-poisson-poisson}}

\begin{itemize}
\tightlist
\item
  Reminder: the Poisson distribution
\item
  The Poisson process has independent Poisson increments
\item
  Summed and marked Poisson processes
\end{itemize}

\newcommand{\Po}{\operatorname{Po}}
\newcommand{\ee}{\mathrm{e}}

\hypertarget{poisson-dist}{%
\subsection{Poisson distribution}\label{poisson-dist}}

In the next three sections we'll be considering the Poisson process, a continuous time discrete space process with the Markov property. Given its name, it's not surprising to hear the Poisson \emph{process} is related to the Poisson \emph{distribution}. Let's start with a reminder of that.

Recall that a discrete random variable \(X\) has a \textbf{Poisson distribution} with rate \(\lambda\), written \(X \sim \operatorname{Po}(\lambda)\), if its probability mass function is
\[ \mathbb P(X = n) = \mathrm{e}^{-\lambda} \frac{\lambda^n}{n!} \qquad n = 0,1,2,\dots. \]

The Poisson distribution is often used to model the number of ``arrivals'' in a fixed amount of time -- for example, the number of calls to a call centre in one hour, the number of claims to an insurance company in one year, or the number of particles decaying from a large amount of radioactive material in one second.

The Poisson distribution is named after the French mathematician and physicist Siméon Denis Poisson, who studied it in 1837, although it was used by another French mathematician, Abraham de Moivre, more than 100 years earlier.

Recall the following facts about of the Poisson distribution:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Its expectation is \(\mathbb EX = \lambda\) and the variance is \(\operatorname{Var}(X) = \lambda\).
\item
  If \(X \sim \operatorname{Po}(\lambda)\) and \(Y \sim \operatorname{Po}(\mu)\) are independent, then \(X + Y \sim \operatorname{Po}(\lambda+\mu)\).
\item
  Let \(X \sim \operatorname{Po}(\lambda)\) represent some arrivals, and independently ``mark'' each arrival with probability \(p\). Then the number of marked arrivals \(Y\) has distribution \(Y = \operatorname{Po}(p\lambda)\), the number of unmarked arrivals \(Z\) has distribution \(Z \sim \operatorname{Po}((1-p)\lambda)\), and \(Y\) and \(Z\) are independent.
\end{enumerate}

(I say ``recall'', but it's OK if the third one is new to you.)

\hypertarget{poisson-def-poisson}{%
\subsection{Definition 1: Poisson increments}\label{poisson-def-poisson}}

Suppose that, instead of just modelling the number of arrivals in a \emph{fixed} amount of time, we want to continually model the total number of arrivals as it changes over time. This will be a stochastic process with discrete state space \(\mathcal S = \mathbb Z_+ = \{0,1,2,\dots\}\) and continuous time \(\mathbb R_+ = [0,\infty)\). In continuous time, we will normally write stochastic processes as \((X(t))\), with the time variable being a \(t\) in brackets, rather than a subscript \(n\) as we had in discrete time.

Suppose calls arrive at a call centre at a rate of \(\lambda = 100\) an hour. The following assumptions seem reasonable:

\begin{itemize}
\tightlist
\item
  We begin counting with \(X(0) = 0\) calls.
\item
  The number of calls in the first hour is \(X(1) \sim \operatorname{Po}(100)\). The number of calls in the second hour \(X(2) - X(1)\) will also be \(\operatorname{Po}(100)\), and will be independent of the number of calls in the first hour.
\item
  The number of calls in a two hour period will be \(X(t+2) - X(t) \sim \operatorname{Po}(200)\), while the number of calls in a half-hour period will be \(X(t+\frac12) - X(t) \sim \operatorname{Po}(50)\).
\end{itemize}

These properties will define the Poisson process.

\begin{definition}
\protect\hypertarget{def:poisson-def-1}{}\label{def:poisson-def-1}

The \textbf{Poisson process} with rate \(\lambda\) is defined as followed. It is a stochastic process \((X(t))\) with continuous time \(t \in [0,\infty)\) and discrete state space \(\mathcal S = \mathbb Z_+\) with the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(X(0) = 0\);
\item
  Poisson increments: \(X(t+s) - X(t) \sim \operatorname{Po}(\lambda s)\) for all \(s,t>0\);
\item
  independent increments: \(X(t_2) - X(t_1)\) and \(X(t_4) - X(t_3)\) are independent for all \(t_1 \leq t_2 \leq t_3 \leq t_4\).
\end{enumerate}

\end{definition}

Note that the condition \(t_1 \leq t_2 \leq t_3 \leq t_4\) means that the time interval from \(t_1\) to \(t_2\) and the time interval from \(t_3\) to \(t_4\) don't overlap. (Overlapping time intervals will not have independent increments, as arrivals in the overlap will count for both.)

The Poisson process was discovered in the first decade 20th century, and the process was named after the distribution. Many people were working on similar things, so it's difficult to say who discovered it first, but important work was done by the Swedish actuary and mathematician Filip Lundberg and the Danish engineer and mathematician AK Erlang.

\begin{example}
\protect\hypertarget{exm:poisson-ex-1}{}\label{exm:poisson-ex-1}

\emph{Claims arrive at insurance company at a rate of \(\lambda = 8\) per hour, modelled as a Poisson process. What is the probability there are no claims in a given \(15\) minute period?}

By property 2, the number of claims in \(15\) minutes is a Poisson distribution with mean \(\frac{15}{60}\lambda = 2\). The probability there are no claims is
\[ \mathrm{e}^{-2} \frac{2^0}{0!} = \mathrm{e}^{-2} = 0.135 . \]

\end{example}

\begin{example}
\protect\hypertarget{exm:poisson-ex-2}{}\label{exm:poisson-ex-2}

\emph{A professor receives visitors to her office at a rate of \(\lambda = 2.5\) per day, modelled as a Poisson process. What is the probability she gets at least one visitor every day this (5-day) week?}

The probability she gets at least one visitor on any given day is
\[ 1 - \mathrm{e}^{-2.5} \frac{2.5^0}{0!} = 1 - \mathrm{e}^{-2.5} = 0.918.   \]
By property 3, the numbers of visitors on different days are independent, so the probability of getting at least one visitor each day this week is
\(0.918^5 = 0.652\).

\end{example}

\hypertarget{summed-marked}{%
\subsection{Summed and marked Poisson processes}\label{summed-marked}}

The following theorem shows that the sum of two Poisson processes is itself a Poisson process.

\begin{theorem}
\protect\hypertarget{thm:summed}{}\label{thm:summed}

Let \((X(t))\) and \((Y(t))\) be independent Poisson processes with rates \(\lambda\) and \(\mu\) respectively. Then the process \((Z(t))\) given by \(Z(t) = X(t) + Y(t)\) is a Poisson process with rate \(\lambda+\mu\).

\end{theorem}

\begin{proof}

The proof of this is a question on \protect\hyperlink{P07}{Problem Sheet 7}.

\end{proof}

\begin{example}
\protect\hypertarget{exm:poisson-ex-3}{}\label{exm:poisson-ex-3}

\emph{A student receives email to her university mail address at a rate of \(\lambda = 4\) emails per hour, and to her personal email address at a rate of \(\mu = 2\) per hour. Using a Poisson process model, what is the probability the student receives 3 or fewer emails in a 30 minute period?}

The total number of emails is a sum of Poisson processes with rate \(\lambda + \mu = 6\). The total number of emails received in half an hour is Poisson with rate \((\lambda + \mu)/2 = 3\). Thus the probability that 3 or fewer emails are received is
\[ \mathrm{e}^{-3} \frac{3^0}{0!} + \mathrm{e}^{-3} \frac{3^1}{1!} + \mathrm{e}^{-3} \frac{3^2}{2!} + \mathrm{e}^{-3} \frac{3^3}{3!} = 13\mathrm{e}^{-3} = 0.647 . \]

\end{example}

We also have the \textbf{marked Poisson process}, which can be thought of as the opposite to the summed process: the summed process combines two processes together, while the marked process splits one process into two.

\begin{theorem}
\protect\hypertarget{thm:marked}{}\label{thm:marked}

Let \((X(t))\) be a Poisson process with rate \(\lambda\). Each arrival is independently marked with probability \(p\). Then the marked process \((Y(t))\) is a Poisson process with rate \(p\lambda\), the unmarked process \((Z(t))\) is a Poisson process with rate \((1-p)\lambda\), and \((Y(t))\) and \((Z(t))\) are independent.

\end{theorem}

\begin{proof}

Given the third fact that you were ``reminded'' of, it's easy to check the necessary properties.

\end{proof}

\begin{example}
\protect\hypertarget{exm:pois-ex-4}{}\label{exm:pois-ex-4}

\emph{In the 2019/20 English Premier League football season, an average of \(\lambda = 2.72\) goals were scored per game, with a proportion \(p = 0.56\) of them scored by the home team. If we model this as a Poisson process, what is the probability a match ends in a 1--1 draw?}

The number of home goals scored is Poisson with rate \(p\lambda = 0.56\times 2.72 = 1.52\), and the number of away goals scored is Poisson with rate \((1-p)\lambda = 1.20\). Under the Poisson process assumption, these are independent, so the probability the home and the away team both score \(1\) goal is
\[ \mathrm{e}^{-1.52} \frac{1.52^1}{1!} \times  \mathrm{e}^{-1.20} \frac{1.20^1}{1!} = 1.52\mathrm{e}^{-1.52} \times 1.20 \mathrm{e}^{-1.20} =  0.12, \]
or 12\%.

\end{example}

\textbf{In the next section}, we look at the Poisson process in a different way: the times in between arrivals have an exponential distribution.

\hypertarget{S14-poisson-exponential}{%
\section{Poisson process with exponential holding times}\label{S14-poisson-exponential}}

\begin{itemize}
\tightlist
\item
  Reminder: the exponential distribution
\item
  The Poisson process has exponential holding times
\item
  The Markov property in continuous time
\end{itemize}

\newcommand{\Exp}{\operatorname{Exp}}

\hypertarget{exponential}{%
\subsection{Exponential distribution}\label{exponential}}

Last time, we introduced the Poisson process by looking at the random number of arrivals in fixed amount of time, which follows a Poisson distribution. Another way of looking at the Poisson process is to look at the random amount of of time required for a fixed number of arrivals. For this, we'll need the exponential distribution.

We start by recalling the exponential distribution.
Recall that we say a continuous random variable \(T\) has the \textbf{exponential distribution} with rate \(\lambda\), and write \(T \sim \operatorname{Exp}(\lambda)\), if it has the probability distribution function \(f(t) = \lambda \mathrm{e}^{-\lambda t}\) for \(t \geq 0\).

Exponential distributions are often used to model ``waiting times'' -- for example, the amount of time until a light bulb breaks, the times between buses arriving, or the time between withdrawals from a bank account.

You are reminded of the following facts about the exponential distribution:

\begin{itemize}
\tightlist
\item
  The cumulative distribution function is \(F(t) = \mathbb P(T \leq t) = 1 - \mathrm{e}^{-\lambda t}\); although it is usually more convenient to deal with the tail probability \(\mathbb P(T > t) = 1 - F(t) = \mathrm{e}^{-\lambda t}\).
\item
  The expectation is \(\mathbb E T = 1/\lambda\) and the variance is \(\operatorname{Var}(T) = 1/\lambda^2\).
\end{itemize}

The following memoryless property will of course be important in a course about memoryless Markov processes.

\begin{theorem}
\protect\hypertarget{thm:memoryless-thm}{}\label{thm:memoryless-thm}

Let \(T \sim \operatorname{Exp}(\lambda)\). Then, for any \(s,t\geq0\),
\[ \mathbb P(T > t + s \mid T > t) = \mathbb P(T > s) . \]

\end{theorem}

Suppose we are waiting an exponentially distributed time for an alarm to go off. No matter how long we've been waiting for, the remaining time to wait is still exponentially distributed, with the same parameter -- hence ``memoryless''.

\begin{proof}

By standard use of conditional probability, we have
\[ \mathbb P(T > t + s \mid T > t) = \frac{\mathbb P(T > t + s \text{ and } T > t)}{\mathbb P(T > t)} =  \frac{\mathbb P(T > t + s)}{\mathbb P(T > t)} = \frac{\mathrm{e}^{-\lambda(t+s)}}{\mathrm{e}^{-\lambda t}} = \mathrm{e}^{-\lambda s} , \]
which is still the tail probability of the exponential distribution.

\end{proof}

The following property will be important later on in the course.

\begin{theorem}
\protect\hypertarget{thm:exponential-thm}{}\label{thm:exponential-thm}

Let \(T_1 \sim \operatorname{Exp}(\lambda_1)\), \(T_2 \sim \operatorname{Exp}(\lambda_2)\), \(\dots\), \(T_n \sim \operatorname{Exp}(\lambda_n)\) be independent exponential distributions, and let \(T\) be the minimum of the \(T_i\)s, so \(T = \min\{T_1, T_2, \dots, T_n \}\). Then
\[ T \sim \operatorname{Exp}(\lambda_1 + \lambda_2 + \cdots + \lambda_n) . \]
Further, the probability that \(T_j\) is the smallest of all the \(T_i\)s is
\[ \mathbb P(T = T_j) = \frac{\lambda_j}{\lambda_1 + \lambda_2 + \cdots + \lambda_n} . \]

\end{theorem}

\begin{proof}

The proof of this is an question on \protect\hyperlink{P07}{Problem Sheet 7}.

\end{proof}

\hypertarget{definition-2-exponential-holding-times}{%
\subsection{Definition 2: exponential holding times}\label{definition-2-exponential-holding-times}}

We mentioned before that exponential distributions are often used to model ``waiting times''.
When modelling a process \((X(t))\) counting many arrivals at rate \(\lambda\), we might model the process like this: after waiting an \(\operatorname{Exp}(\lambda)\) amount of time, an arrival appears. After another \(\operatorname{Exp}(\lambda)\) amount of time, another arrival appears. And so on. We often use the term ``holding time'' to refer to the time between consecutive arrivals.

This suggests a process with the following properties:

\begin{itemize}
\tightlist
\item
  We start with \(X(0) = 0\).
\item
  Let \(T_1, T_2, \dots \sim \operatorname{Exp}(\lambda)\) be the holding times, all independent. Then
  \[ X(t) = \begin{cases} 0 & \text{for $0 \leq t < T_1$} \\
    1 & \text{for $T_1 \leq t < T_1 + T_2$} \\
    2 & \text{for $T_1+T_2 \leq t < T_1 + T_2 + T_3$} \\
    \text{and so on.} & \end{cases} \]
\end{itemize}

A process described like this is also the Poisson process!

\begin{figure}
\centering
\includegraphics{sections/pois-exp-pic.png}
\caption{\label{fig:pois-exp-pic}A Poisson process with exponentially distributed holding times.}
\end{figure}

\begin{theorem}
\protect\hypertarget{thm:pois-exp-equiv}{}\label{thm:pois-exp-equiv}

Let \((X(t))\) be a Poisson process with rate \(\lambda\), as defined by its independent Poisson increments (see Subsection \ref{poisson-def-poisson}). Then \((X(t))\) has the exponential holding times structure described above.

\end{theorem}

\begin{proof}

Let \((X(t))\) be a Poisson process with rate \(\lambda\). We seek the distribution of the first arrival time \(T_1\). We have
\[ \mathbb P(T_1 > t_1) = \mathbb P\big(X(t_1) - X(0) = 0\big) = \mathrm{e}^{-\lambda t_1} \frac{(\lambda t_1)^0}{0!} = \mathrm{e}^{-\lambda t_1} , \]
since the arrival comes after \(t_1\) if there are no arrivals in the interval \([0,t_1]\).
But this is precisely the tail probability of an exponential distribution with rate \(\lambda\).

Now consider the second holding time. We have
\[ \mathbb P(T_2 > t_2 \mid T_1 = t_1) = \mathbb P\big(X(t_1 + t_2) - X(t_1) = 0\big) = \mathrm{e}^{-\lambda ((t_1 + t_2) - t_1)}  =  \mathrm{e}^{-\lambda t_2} \]
by the same argument as before. This is the tail distribution for another \(\operatorname{Exp}(\lambda)\) distribution, independent of \(T_1\).

Repeating this argument for all \(n\) gives the desired exponential holding time structure.

\end{proof}

\begin{example}
\protect\hypertarget{exm:bookshop}{}\label{exm:bookshop}

\emph{Customers visit a second-hand bookshop at a rate of \(\lambda = 5\) per hour. Each customer a book with probability \(p = 0.4\). What is the expected time to make ten sales, and what is the standard deviation?}

The count of books sold marked Poisson process, which we saw last time is itself a Poisson process with rate \(p\lambda = 2\).

The expected time of the tenth sale is
\[ \mathbb E(T_1 + \dots + T_{10}) = 10\,\mathbb ET_1 = 10 \times \tfrac{1}{2} = 5 \text{ hours} .\]
The variance is
\[ \operatorname{Var}(T_1 + \dots + T_{10}) = 10\,\operatorname{Var}(T_1) = 10 \times \frac{1}{2^2} = 2.5 , \]
where we used that the holding times are independent and identically distributed, so the standard deviation is \(\sqrt{2.5} = 1.58\) hours.

\emph{What is the probability it takes more than an hour to sell the first book?}

This is the probability the first holding time is longer than an hour, which is the tail of the exponentital distribution,
\[ \mathbb P(T_1 > 1) = \mathrm{e}^{-2\times 1} = \mathrm{e}^{-2} = 0.135. \]

An alternative way to solve this would be to say that the first book is is sold later than 1 hour in if \(X(1) = 0\). So using the Poisson increments definition from last time, we have
\[ \mathbb P\big(X(1) = 0\big) = \mathrm{e}^{-2\times 1} \frac{(2\times 1)^0}{0!}  = \mathrm{e}^{-2} = 0.135. \]
We get the same answer either way.

\end{example}

\hypertarget{cont-markov}{%
\subsection{Markov property in continuous time}\label{cont-markov}}

We previously saw the Markov ``memoryless'' property in discrete time. The equivalent definition in continuous time is the following.

\begin{definition}
\protect\hypertarget{def:pp}{}\label{def:pp}

Let \((X(t))\) be a stochastic process on a discrete state space \(\mathcal S\) and continuous time \(t \in [0,\infty)\). We say that \((X(t))\) has the \textbf{Markov property}
if
\begin{multline*}
\mathbb P\big( X(t_{n+1}) = x_{n+1} \mid X(t_n) = x_n, \dots, X(t_1) = x_1, X(t_0) = x_0 \big) \\
= \mathbb P\big( X(t_{n+1}) = x_{n+1} \mid X(t_n) = x_{n}\big)
\end{multline*}
for all times \(t_0 < t_1 < \cdots < t_n < t_{n+1}\) and all states \(x_0, x_1, \dots, x_n, x_{n+1} \in \mathcal S\).

\end{definition}

In other words, the state of the process at some point \(t_{n+1}\) in the future depends on where we are now \(t_n\), but, given that, does not depend on any collection of previous times \(t_0, t_1, \dots, t_{n-1}\).

The Poisson process does indeed have the Markov property, since, by the property of independent increments, we have that
\[ \mathbb P\big( X(t_{n+1}) = x_{n+1} \mid X(t_n) = x_n, \dots, X(t_0) = x_0 \big) = \mathbb P\big( X(t_{n+1}) -X(t_n) = x\big) , \]
where \(x = x_{n+1}-x_n\). By Poisson increments, this has probability
\[ \mathrm{e}^{-\lambda(t_{n+1}-t_n)} \frac{\big(\lambda(t_{n+1}-t_n)\big)^x}{x!} . \]

Alternatively, by the memoryless property of the exponential distribution, we see that we can restart the holding time from \(t_n\) and still have this and all future holding times exponentially distributed.

\textbf{In the next section}, we look at the Poisson process in a third way, by looking at what happens in a very small time period.

\hypertarget{P07}{%
\section*{Problem sheet 7}\label{P07}}
\addcontentsline{toc}{section}{Problem sheet 7}

\commfalse

You should attempt all these questions and write up your solutions in advance of your workshop in week 8 (Monday 15 or Tuesday 16 March) where the answers will be discussed.

\textbf{1.} Let \((X(t))\) be a Poisson process with rate \(\lambda = 5\). Calculate:

\textbf{(a)} \(\mathbb P\big(X(0.4) \leq 2\big)\);

\begin{myanswers}
\emph{Solution.}
We have that \(X(0.4) \sim \text{Po}(2)\), so
\[ \mathbb P\big(X(0.4) \leq 2\big) = \mathrm{e}^{-2} + 2\mathrm{e}^{-2} + \frac{2^2}{2} \mathrm{e}^{-2} = 5 \mathrm{e}^{-2} = 0.677 .\]

\end{myanswers}

\textbf{(b)} \(\mathbb EX(6.4)\);

\begin{myanswers}
\emph{Solution.} We have that \(X(6.4) \sim \text{Po}(32)\), so \(\mathbb E X(6.4) = 32\).

\end{myanswers}

\textbf{(c)} \(\mathbb P\big(X(0.5) = 0 \text{ and } X(1) = 1\big)\).

\begin{myanswers}
\emph{Solution.}
This happens if we have \(X(0.5) - X(0) = 0\) and also the independent event \(X(1) - X(0.5) = 1\). Both increments are \(\text{Po}(2.5)\), so
\[\mathbb P\big(X(0.5) = 0 \text{ and } X(1) = 1\big) = \mathrm{e}^{-2.5} \times 2.5\mathrm{e}^{-2.5} = 0.0168.  \]

\end{myanswers}

Let \(T_n\) be the \(n\)th holding time, and let \(J_n = T_1 + \cdots + T_n\) be the \(n\)th arrival time. Calculate:

\textbf{(d)} \(\mathbb P(0.1 \leq T_2 < 0.3)\);

\begin{myanswers}
\emph{Solution.} We have \(T_2 \sim \text{Exp}(5)\), so
\begin{multline*} \mathbb P(0.1 \leq T_2 < 0.3) = \mathbb P(T_2 > 0.1) - \mathbb P(T_2 > 0.3) \\= \mathrm{e}^{-5\times0.1} - \mathrm{e}^{-5\times0.3} = \mathrm{e}^{-0.5} - \mathrm{e}^{-1.5} = 0.383.       \end{multline*}

\end{myanswers}

\textbf{(e)} \(\mathbb E J_{100}\);

\begin{myanswers}
\emph{Solution.}
\[ \mathbb E J_{100} = \mathbb E (T_1 + \dots + T_{100}) = 100 \, \mathbb ET_1 = 100 \times \tfrac15 = 20.     \]

\end{myanswers}

\textbf{(f)} \(\operatorname{Var}(J_{100})\).

\begin{myanswers}
\emph{Solution.}
Using independence,
\[ \mathbb \operatorname{Var}( J_{100}) = \operatorname{Var}E (T_1 + \dots + T_{100}) = 100 \, \operatorname{Var}(T_1) = 100 \times \tfrac1{5^2} = 4.     \]

\end{myanswers}

\textbf{(g)} Using a normal approximation, approximate \(\mathbb P(18 \leq J_{100} \leq 22)\).

\begin{myanswers}
\emph{Solution.}
The normal approximation is \(J_{100} \sim \mathrm N(20,4)\). So, letting \(Z \sim \text{N}(0,1)\), we have
\begin{multline*}  \mathbb P(18 \leq J_{100} \leq 22) = \mathbb P\left(\frac{18-20}{\sqrt4} \leq Z \leq \frac{18-20}{\sqrt4}\right) \\ = \mathbb P(-1 \leq Z \leq 1) = 2\Phi(1) - 1 = 0.683.  \end{multline*}

\end{myanswers}

\textbf{2.} Suppose that telephone calls arrive at a call centre according to a Poisson process with rate \(\lambda = 100\) per hour, and are answered with probability \(0.6\).

\textbf{(a)} What is the probability that there are no answered calls in the next minute?

\begin{myanswers}
\emph{Solution.}
The answered calls form a Poisson process with rate \(100 \times 0.6 = 60\) per hour, or \(1\) per minute. The probability there are no unanswered calls in one minute are \(\mathrm{e}^{-1} = 0.368\).

\end{myanswers}

\textbf{(b)} Use a suitable normal approximation, with a continuity correction, to find the probability that there will be at least \(25\) answered calls in the next \(30\) minutes.

\begin{myanswers}
\emph{Solution.}
The number of calls in \(30\) minutes is Poisson of rate \(30\), which has expectation \(30\) and variance \(30\). So, using a continuity correction, we have
\begin{multline} \mathbb P \big(X(30) \geq 24.5 \big) = \mathbb P \left( Z \geq \frac{24.5 - 30}{\sqrt{30}} \right) \\= \mathbb P(Z \geq -1.00 ) = \Phi(1.00) = 0.842 . \end{multline}

\end{myanswers}

\textbf{3.}

\textbf{(a)} Let \(X \sim \text{Po}(\lambda)\) and \(Y \sim \text{Po}(\mu)\) be two independent Poisson distributions. Show that \(X + Y \sim \text{Po}(\lambda + \mu)\). One way to start would be to write
\[ \mathbb P(X+Y = z) = \sum_{x=0}^z \mathbb P(X = x) \, \mathbb P(Y = z-x) . \]

\begin{myanswers}
\emph{Solution.} We have
\begin{align*}
\mathbb P(X+Y = z) &= \sum_{x=0}^z \mathbb P(X = x) \, \mathbb P(Y = z-x) \\
  &= \sum_{x=0}^z \mathrm{e}^{-\lambda} \frac{\lambda^x}{x!} \mathrm{e}^{-\mu} \frac{\mu^{z-x}}{(z-x)!} \\
  &= \mathrm{e}^{-(\lambda + \mu)} \sum_{x=0}^z \frac{1}{x!(z-x)!} \lambda^x \mu^{z-x} \\
  &= \mathrm{e}^{-(\lambda + \mu)} \frac{1}{z!} \sum_{x=0}^z \frac{z!}{x!(z-x)!} \lambda^x \mu^{z-x} \\
  &= \mathrm{e}^{-(\lambda + \mu)} \frac{1}{z!} \sum_{x=0}^z \binom{z}{x} \lambda^x \mu^{z-x} .
\end{align*}
But the sum here is precisely
\[ (\lambda + \mu)^z = \sum_{x=0}^z \binom{z}{x} \lambda^x \mu^{z-x} ,  \]
so we have
\[ \mathbb P(X+Y = z) = \mathrm{e}^{-(\lambda + \mu)} \frac{(\lambda+\mu)^z}{z!}    , \]
as desired.

\end{myanswers}

\textbf{(b)} Let \((X(t))\) and \((Y(t))\) be independent Poisson processes with rate \(\lambda\) and \(\mu\) respectively. Use part (a) to show that \((X(t) + Y(t))\) is a Poisson process with rate \(\lambda + \mu\).

\begin{myanswers}
\emph{Solution.}
First, that \(Z(0) = X(0) + Y(0) = 0 + 0 = 0\) is immediate. Second, the previous result shows that \((Z(t))\) has independent Poisson increments with rate \(\lambda + \mu\). Third, since \((X(t))\) and \((Y(t))\) each have independent increments and are independent of each other, it follows that \((Z(t))\) has independent increments also.

\end{myanswers}

\textbf{(c)} Number 1 buses arrive at a bus stop at a rate of \(\lambda_1 = 4\) per hour, and Number 6 buses arrive at the rate \(\lambda_6 = 2\) per hour. I've been waiting at the bus stop for 5 minutes for either bus to arrive; how much longer do I have to wait, on average?

\begin{myanswers}
\emph{Solution.}
By the above, the arrivals of buses form a Poisson process of rate \(\lambda_1 + \lambda_6 = 6\). By the memoryless property of the exponential distribution, the fact we have already been waiting for 5 minutes is irrelevant. The waiting time is \(\text{Exp}(6)\), with mean \(\frac16\) of an hour, or 10 minutes.

\end{myanswers}

\textbf{4.}
Let \(T_1 \sim \operatorname{Exp}(\lambda_1), T_2 \sim \operatorname{Exp}(\lambda_2), \dots, T_n \sim \operatorname{Exp}(\lambda_n)\) be independent exponential distributions, and let \(T\) be the minimum \(T = \min \{T_1, T_2 \dots, T_n\}\).

\textbf{(a)} Show that \(T \sim \operatorname{Exp}(\lambda_1 + \lambda_2 + \cdots + \lambda_n)\). (You may use the fact that
\[ \mathbb P(T > t) = \mathbb P(T_1 > t) \, \mathbb P(T_2 > t) \cdots \mathbb P(T_n > t) , \]
provided you explain why it's true.)

\begin{myanswers}
\emph{Solution.}
For the minimum to be bigger than \(t\), we need all \(n\) of the times to be bigger than \(t\). Hence
\begin{multline*} \mathbb P(T > t) = \mathbb P(T_1 > t)  \, \mathbb P(T_2 > t)\cdots   \mathbb P(T_n > t)\\ = \mathrm{e}^{-\lambda_1 t} \mathrm{e}^{-\lambda_2 t} \cdots \mathrm{e}^{-\lambda_n t} = \mathrm{e}^{-(\lambda_1 + \lambda_2 + \cdots + \lambda_n) t} . \end{multline*}
But this is precisely the tail probability of an \(\operatorname{Exp}(\lambda_1 + \lambda_2 + \cdots + \lambda_n)\) distribution.

\end{myanswers}

\textbf{(b)} Show that the probability that the minimum is \(T_j\) is given by
\[ \mathbb P(T_j = T) = \frac{\lambda_j}{\lambda_1 + \lambda_2 + \cdots + \lambda_n} . \]
(You could choose to begin by proving the \(n = 2\) case, if you want.)

\begin{myanswers}
\emph{Solution.}
By conditioning on the value of \(T_j\) and arguing as above, we have
\[ \mathbb P(T_j = T) = \int_0^\infty f_{T_j}(t) \, \mathbb P(\text{all other $T_k$s} \geq t) \, \mathrm dt .  \]
Using the result above and substituting in the PDF of an exponential distribution, we get
\begin{align*} \mathbb P(T_j = T) &= \int_0^\infty \lambda_j \mathrm{e}^{-\lambda_j t}\, \mathrm{e}^{-(\lambda_1 + \cdots + \lambda_{j-1} + \lambda_{j+1} + \cdots + \lambda_n)t} \, \mathrm dt \\         
&= \lambda_j \int_0^\infty \mathrm{e}^{-(\lambda_1 + \lambda_2 + \cdots + \lambda_n) t} \, \mathrm dt \\
&= \lambda_j \left[ -\frac{1}{\lambda_1 + \lambda_2 + \cdots + \lambda_n} \mathrm{e}^{-(\lambda_1 + \lambda_2 + \cdots + \lambda_n) t} \right]_0^\infty \\
&= \lambda_j \left(-0 - \left(- \frac{1}{\lambda_1 + \lambda_2 + \cdots + \lambda_n}\right)\right) \\
&= \frac{\lambda_j}{\lambda_1 + \lambda_2 + \cdots + \lambda_n} . \end{align*}

\end{myanswers}

\textbf{5.}
Let \((X(t))\) be a Poisson process with rate \(\lambda\). Conditional on there being exactly 1 arrival before time \(t\), find the distribution of the time of that arrival.

\begin{myanswers}
\emph{Solution.}
Clearly the arrival is in the interval \([0,t]\). The conditional CDF is
\begin{align*}
  \mathbb P\big(T_1 \leq s \mid X(t) = 1\big)
    &= \frac{\mathbb P\big( T_1 \leq s \text{ and } X(t) = 1 \big)}{\mathbb P\big(X(t) = 1 \big)} %\\
%    &= \frac{\mathbb P( T_1 \leq s \text{ and } T_2 > t-s )}{\mathbb P\big(X(t) = 1 \big)} \\
%    &= \frac{ (1 - \ee^{-\lambda s} ) \ee^{-\lambda(t-s)}{\ee^{-\lambda t}} \\
%    &= \frac{ \ee^{-\lambda(t-s)} 
\end{align*}
The denominator is \(\lambda t \mathrm{e}^{-\lambda t}\). The numerator is
\begin{align*}
\mathbb P\big( T_1 \leq s \text{ and } X(t) = 1 \big)
  &= \mathbb P\big(X(s) = 1 \text{ and } X(t) = 1 \big) \\
  &= \mathbb P \big(X(s)-X(0) = 1\big) \, \mathbb P\big(X(t) - X(s) = 0\big) \\
  &= \lambda s \mathrm{e}^{-\lambda s} \mathrm{e}^{\lambda(t-s)} \\
  &= \lambda s \mathrm{e}^{-\lambda t} .
\end{align*}
Putting this together, the conditional CDF is
\[ \mathbb P\big(T_1 \leq s \mid X(t) = 1\big) = \frac{\lambda s \mathrm{e}^{-\lambda t}}{\lambda t \mathrm{e}^{-\lambda t}} = \frac{s}{t} .   \]
This is precisely the CDF for the uniform distribution on \([0,t]\).

\end{myanswers}

\hypertarget{computing}{%
\section*{Computational worksheets}\label{computing}}
\addcontentsline{toc}{section}{Computational worksheets}

\begin{itemize}
\tightlist
\item
  \textbf{Computational Worksheet 1:} \href{computing/C1.html}{{[}HTML format{]}} \href{computing/C1.Rmd}{{[}Rmd format{]}}

  \begin{itemize}
  \tightlist
  \item
    Example report: \href{computing/C1-example.html}{{[}HTML format{]}} \href{computing/C1-example.Rmd}{{[}Rmd format{]}}
  \end{itemize}
\item
  \textbf{Computational Worksheet 2 (Assessment 2):} \href{computing/C2.html}{{[}HTML format{]}} \href{computing/C2.Rmd}{{[}Rmd format{]}} This worksheet is assessed, and is due on \emph{Thursday 18 March} at 2pm.
\end{itemize}

\hypertarget{C-about}{%
\subsection*{About the computational worksheets}\label{C-about}}
\addcontentsline{toc}{subsection}{About the computational worksheets}

These computational worksheets are an opportunity to learn more about Markov chains through simulation using R.

This first worksheet is not an assessed part of the course, but it is for you to learn and practice. The second worksheet \emph{is} an assessed part of the course, and counts for 3\% of your grade on this course. A report on Worksheet 2 will be due on \textbf{Thursday 18 March} (week 8) at 2pm. The material in these worksheets is examinable.

I recommend working on Computational Worksheet 1 in weeks 3 or 4, and on Computational Worksheet 2 in weeks 6 or 7. I estimate that each worksheet may take about 2 hours to work through.

You will have two computational drop-in sessions available with Muyang Zhang. These drop-in sessions are optional opportunities for you to come along to ask for help if you are stuck or want to know more. These sessions will happen on Teams. The sessions may appear in your timetable as ``Practicals''. It is important that you work through most of the worksheet \emph{before} your drop-in session, as this will be your main opportunity to ask for help. (You can also use the module discussion board on Teams.) The dates of the drop-in sessions are:

\begin{itemize}
\tightlist
\item
  Computational Worksheet 1: Monday 15 -- Wednesday 17 February (week 4)
\item
  Computational Worksheet 2: Monday 8 -- Wednesday 10 March (week 7)
\end{itemize}

Note that the Worksheet 2 practical sessions are the week before the deadline, so it's in your benefit to start working on that worksheet early.

The computational worksheets are available in two formats:

\begin{itemize}
\tightlist
\item
  First, as an easy-to-read \textbf{HTML file}. You should open this in a web browser.
\item
  Second, as a file with the suffix \textbf{.Rmd}. This can be read as a plain text file. However, I recommend downloading this file and opening it in RStudio. This will make it easy to run the R code included in the file, by clicking the green ``play'' button by each chunk of R code. (These files is written in a language called ``\href{https://www.stat.cmu.edu/~cshalizi/rmarkdown/}{R Markdown}'', which you could choose to use for writing your report.)
\end{itemize}

\hypertarget{R-access}{%
\subsection*{How to access R}\label{R-access}}
\addcontentsline{toc}{subsection}{How to access R}

These worksheets use the statistical programming language \textbf{R}. Use of R is mandatory. I recommend interacting with R via the program \textbf{RStudio}, although this is optional. There are various ways to access R and RStudio.

\begin{itemize}
\tightlist
\item
  You may already have R and RStudio installed on your own computer.
\item
  You can install R and RStudio on your own computer now if you haven't previously. You should first \href{https://cran.rstudio.com/}{download R from the Comprehensive R Archive Network}, and then \href{https://rstudio.com/products/rstudio/download/\#download}{download ``RStudio Desktop'' from rstudio.com}. Remember to download and install R first, and only then to download and install RStudio.
\item
  The \href{https://rstudio.cloud/}{RStudio Cloud} is like ``Google Docs for R''. You can get 15 hours a month for free, which should be more than enough for these worksheets. Because this doesn't require installation, it's good for Chromebooks or computers where you don't have full installation rights.
\item
  If you are in Leeds, all the university computers have R and RStudio installed. However, at the time of writing (5 February 2021), all IT clusters on campus are closed. You can see \href{https://leeds.service-now.com/it?id=clusters}{the latest news on cluster availability here}.
\item
  You can access R and RStudio via the university's \href{https://it.leeds.ac.uk/it?id=kb_article\&sysparm_article=KB0014548}{virtual Windows desktop} or (for those who have a Windows computer) via the university's \href{https://it.leeds.ac.uk/it?id=kb_article\&sysparm_article=KB0014827}{AppsAnywhere} system.
\end{itemize}

\hypertarget{R-background}{%
\subsection*{R background}\label{R-background}}
\addcontentsline{toc}{subsection}{R background}

These worksheets are mostly self-explanatory. However, they do assume a little background. For example, I assume you know how to operate R (for example by opening RStudio and typing commands in the ``console''), and that you know that \texttt{x\ \textless{}-\ c(1,\ 2,\ 3)} creates a vector called \texttt{x} and that \texttt{mean(x)} calculates the mean of the entries of \texttt{x}.

Most students on this course will know R from MATH1710 and MATH1712 or other courses. If you are new to R, I recommend Dr Jochen Voss's ``A Short Introduction to R'' or Dr Arief Gusnanto's ``Introduction to R'', both available from \href{http://www1.maths.leeds.ac.uk/~arief/MATH1712/index.html}{Dr Gusnanto's MATH1712 homepage}.

\end{document}
